commit b7ac758969b1350ba0f534bcf6a7c1544d64b064
Author: Anthony Romano <anthony.romano@coreos.com>
Date:   Mon Apr 25 12:32:58 2016 -0700

    *: rename storage package to mvcc

diff --git a/Documentation/api_v3.md b/Documentation/api_v3.md
index 7b01fd6..9fa6cb8 100644
--- a/Documentation/api_v3.md
+++ b/Documentation/api_v3.md
@@ -141,6 +141,6 @@ etcd ensures linearizability for all other operations by default. Linearizabilit
 [strict_consistency]: https://en.wikipedia.org/wiki/Consistency_model#Strict_consistency
 [serializable_isolation]: https://en.wikipedia.org/wiki/Isolation_(database_systems)#Serializable
 [Linearizability]: #Linearizability
-[kv-proto]: https://github.com/coreos/etcd/blob/master/storage/storagepb/kv.proto
+[kv-proto]: https://github.com/coreos/etcd/blob/master/mvcc/mvccpb/kv.proto
 [kv-service]: https://github.com/coreos/etcd/blob/master/etcdserver/etcdserverpb/rpc.proto
 [response_header]: https://github.com/coreos/etcd/blob/master/etcdserver/etcdserverpb/rpc.proto
diff --git a/Documentation/dev-guide/api_reference_v3.md b/Documentation/dev-guide/api_reference_v3.md
index 239cbfc..79722c0 100644
--- a/Documentation/dev-guide/api_reference_v3.md
+++ b/Documentation/dev-guide/api_reference_v3.md
@@ -567,7 +567,7 @@ Empty field.
 | Field | Description | Type |
 | ----- | ----------- | ---- |
 | header |  | ResponseHeader |
-| kvs | kvs is the list of key-value pairs matched by the range request. | (slice of) storagepb.KeyValue |
+| kvs | kvs is the list of key-value pairs matched by the range request. | (slice of) mvccpb.KeyValue |
 | more | more indicates if there are more keys to return in the requested range. | bool |
 
 
@@ -700,11 +700,11 @@ From google paxosdb paper: Our implementation hinges around a powerful primitive
 | created | created is set to true if the response is for a create watch request. The client should record the watch_id and expect to receive events for the created watcher from the same stream. All events sent to the created watcher will attach with the same watch_id. | bool |
 | canceled | canceled is set to true if the response is for a cancel watch request. No further events will be sent to the canceled watcher. | bool |
 | compact_revision | compact_revision is set to the minimum index if a watcher tries to watch at a compacted index.  This happens when creating a watcher at a compacted revision or the watcher cannot catch up with the progress of the key-value store.  The client should treat the watcher as canceled and should not try to create any watcher with the same start_revision again. | int64 |
-| events |  | (slice of) storagepb.Event |
+| events |  | (slice of) mvccpb.Event |
 
 
 
-##### message `Event` (storage/storagepb/kv.proto)
+##### message `Event` (mvcc/mvccpb/kv.proto)
 
 | Field | Description | Type |
 | ----- | ----------- | ---- |
@@ -713,7 +713,7 @@ From google paxosdb paper: Our implementation hinges around a powerful primitive
 
 
 
-##### message `KeyValue` (storage/storagepb/kv.proto)
+##### message `KeyValue` (mvcc/mvccpb/kv.proto)
 
 | Field | Description | Type |
 | ----- | ----------- | ---- |
diff --git a/Documentation/dev-guide/interacting_v3.md b/Documentation/dev-guide/interacting_v3.md
index 62b3849..553d3a6 100644
--- a/Documentation/dev-guide/interacting_v3.md
+++ b/Documentation/dev-guide/interacting_v3.md
@@ -179,7 +179,7 @@ compacted revision 5
 
 # any revisions before the compacted one are not accessible
 $ etcdctl get --rev=4 foo
-Error:  rpc error: code = 11 desc = etcdserver: storage: required revision has been compacted
+Error:  rpc error: code = 11 desc = etcdserver: mvcc: required revision has been compacted
 ```
 
 ## Grant leases
@@ -240,4 +240,4 @@ lease 32695410dcc0ca0 keepalived with TTL(100)
 lease 32695410dcc0ca0 keepalived with TTL(100)
 lease 32695410dcc0ca0 keepalived with TTL(100)
 ...
-```
\ No newline at end of file
+```
diff --git a/Documentation/op-guide/maintenance.md b/Documentation/op-guide/maintenance.md
index e416ff0..b368d37 100644
--- a/Documentation/op-guide/maintenance.md
+++ b/Documentation/op-guide/maintenance.md
@@ -31,7 +31,7 @@ Revisions prior to the compaction revision become inaccessible:
 
 ```sh
 $ etcdctl get --rev=2 somekey
-Error:  rpc error: code = 11 desc = etcdserver: storage: required revision has been compacted
+Error:  rpc error: code = 11 desc = etcdserver: mvcc: required revision has been compacted
 ```
 
 ## Defragmentation
@@ -64,7 +64,7 @@ The space quota can be triggered with a loop:
 # fill keyspace
 $ while [ 1 ]; do dd if=/dev/urandom bs=1024 count=1024  | etcdctl put key  || break; done
 ...
-Error:  rpc error: code = 8 desc = etcdserver: storage: database space exceeded
+Error:  rpc error: code = 8 desc = etcdserver: mvcc: database space exceeded
 # confirm quota space is exceeded
 $ etcdctl endpoint status
 +----------------+------------------+-----------+---------+-----------+-----------+------------+
diff --git a/Documentation/rfc/v3api.md b/Documentation/rfc/v3api.md
index cab075e..1371347 100644
--- a/Documentation/rfc/v3api.md
+++ b/Documentation/rfc/v3api.md
@@ -208,4 +208,4 @@ WatchResponse {
 ```
 
 [api-protobuf]: https://github.com/coreos/etcd/blob/master/etcdserver/etcdserverpb/rpc.proto
-[kv-protobuf]: https://github.com/coreos/etcd/blob/master/storage/storagepb/kv.proto
+[kv-protobuf]: https://github.com/coreos/etcd/blob/master/mvcc/mvccpb/kv.proto
diff --git a/alarm/alarms.go b/alarm/alarms.go
index 5d5a1bc..464a89d 100644
--- a/alarm/alarms.go
+++ b/alarm/alarms.go
@@ -19,8 +19,8 @@ import (
 	"sync"
 
 	pb "github.com/coreos/etcd/etcdserver/etcdserverpb"
+	"github.com/coreos/etcd/mvcc/backend"
 	"github.com/coreos/etcd/pkg/types"
-	"github.com/coreos/etcd/storage/backend"
 	"github.com/coreos/pkg/capnslog"
 )
 
diff --git a/auth/authpb/auth.pb.go b/auth/authpb/auth.pb.go
index 7d40104..3acf355 100644
--- a/auth/authpb/auth.pb.go
+++ b/auth/authpb/auth.pb.go
@@ -756,21 +756,20 @@ var (
 )
 
 var fileDescriptorAuth = []byte{
-	// 254 bytes of a gzipped FileDescriptorProto
+	// 236 bytes of a gzipped FileDescriptorProto
 	0x1f, 0x8b, 0x08, 0x00, 0x00, 0x09, 0x6e, 0x88, 0x02, 0xff, 0xe2, 0xe2, 0x4a, 0x2c, 0x2d, 0xc9,
 	0xd0, 0x2b, 0x28, 0xca, 0x2f, 0xc9, 0x17, 0x62, 0x03, 0xb1, 0x0b, 0x92, 0xa4, 0x44, 0xd2, 0xf3,
-	0xd3, 0xf3, 0xc1, 0x42, 0xfa, 0x20, 0x16, 0x44, 0x56, 0xc9, 0x87, 0x8b, 0x25, 0xb4, 0x38, 0xb5,
-	0x48, 0x48, 0x88, 0x8b, 0x25, 0x2f, 0x31, 0x37, 0x55, 0x82, 0x51, 0x81, 0x51, 0x83, 0x27, 0x08,
-	0xcc, 0x16, 0x92, 0xe2, 0xe2, 0x28, 0x48, 0x2c, 0x2e, 0x2e, 0xcf, 0x2f, 0x4a, 0x91, 0x60, 0x02,
-	0x8b, 0xc3, 0xf9, 0x42, 0x22, 0x5c, 0xac, 0x45, 0xf9, 0x39, 0xa9, 0xc5, 0x12, 0xcc, 0x0a, 0xcc,
-	0x1a, 0x9c, 0x41, 0x10, 0x8e, 0x52, 0x3d, 0x17, 0x57, 0x40, 0x6a, 0x51, 0x6e, 0x66, 0x71, 0x71,
-	0x66, 0x7e, 0x9e, 0x90, 0x00, 0x17, 0x73, 0x76, 0x6a, 0x25, 0xd4, 0x48, 0x10, 0x53, 0xc8, 0x18,
-	0x68, 0x22, 0x50, 0x3e, 0xa4, 0xb2, 0x20, 0x15, 0x6c, 0x22, 0x9f, 0x91, 0xb8, 0x1e, 0xc4, 0x79,
-	0x7a, 0x08, 0x7d, 0x7a, 0x20, 0xe9, 0x20, 0xb8, 0x42, 0x25, 0x2d, 0x2e, 0x16, 0x10, 0x2d, 0xc4,
-	0xc1, 0xc5, 0x12, 0xe4, 0xea, 0xe8, 0x22, 0xc0, 0x20, 0xc4, 0xc9, 0xc5, 0x1a, 0x1e, 0xe4, 0x19,
-	0xe2, 0x2a, 0xc0, 0x28, 0xc4, 0xcb, 0xc5, 0x09, 0x12, 0x84, 0x70, 0x99, 0x94, 0x42, 0x80, 0x6a,
-	0x80, 0x2e, 0xc1, 0xea, 0x1d, 0x0b, 0x2e, 0x5e, 0xa0, 0x1b, 0x10, 0xf6, 0x00, 0x5d, 0xc0, 0xac,
-	0xc1, 0x6d, 0x24, 0x84, 0xe9, 0x82, 0x20, 0x54, 0x85, 0x4e, 0x22, 0x27, 0x1e, 0xca, 0x31, 0x5c,
-	0x00, 0xe2, 0x13, 0x8f, 0xe4, 0x18, 0x2f, 0x00, 0xf1, 0x03, 0x20, 0x4e, 0x62, 0x03, 0x87, 0xa0,
-	0x31, 0x20, 0x00, 0x00, 0xff, 0xff, 0x92, 0x06, 0xa1, 0xed, 0x6d, 0x01, 0x00, 0x00,
+	0xd3, 0xf3, 0xc1, 0x42, 0xfa, 0x20, 0x16, 0x44, 0x56, 0xc9, 0x94, 0x8b, 0x25, 0xb4, 0x38, 0xb5,
+	0x48, 0x88, 0x87, 0x8b, 0x25, 0x2f, 0x31, 0x37, 0x55, 0x82, 0x51, 0x81, 0x51, 0x83, 0x47, 0x48,
+	0x80, 0x8b, 0xa3, 0x20, 0xb1, 0xb8, 0xb8, 0x3c, 0xbf, 0x28, 0x45, 0x82, 0x09, 0x2c, 0xc2, 0xcb,
+	0xc5, 0x5a, 0x94, 0x9f, 0x93, 0x5a, 0x2c, 0xc1, 0xac, 0xc0, 0xac, 0xc1, 0xa9, 0x54, 0xc0, 0xc5,
+	0x15, 0x90, 0x5a, 0x94, 0x9b, 0x59, 0x5c, 0x9c, 0x99, 0x9f, 0x27, 0xc4, 0xcd, 0xc5, 0x9c, 0x9d,
+	0x5a, 0x09, 0xd5, 0xab, 0x09, 0xd4, 0x0b, 0x94, 0x0a, 0xa9, 0x2c, 0x48, 0x05, 0xeb, 0xe5, 0x33,
+	0x12, 0xd7, 0x83, 0x38, 0x41, 0x0f, 0xa1, 0x45, 0x0f, 0x24, 0xad, 0xa4, 0xc5, 0xc5, 0x02, 0xa2,
+	0x85, 0x38, 0xb8, 0x58, 0x82, 0x5c, 0x1d, 0x5d, 0x04, 0x18, 0x84, 0x38, 0xb9, 0x58, 0xc3, 0x83,
+	0x3c, 0x43, 0x5c, 0x05, 0x18, 0x81, 0x36, 0x72, 0x82, 0x04, 0x21, 0x5c, 0x26, 0x25, 0x7b, 0xa0,
+	0x1a, 0xa0, 0x03, 0xd0, 0x1c, 0xaa, 0xc9, 0xc5, 0x0b, 0xb4, 0x19, 0x61, 0x2e, 0xd0, 0x46, 0x66,
+	0x0d, 0x6e, 0x23, 0x21, 0x4c, 0x1b, 0x9d, 0x44, 0x4e, 0x3c, 0x94, 0x63, 0xb8, 0x00, 0xc4, 0x27,
+	0x1e, 0xc9, 0x31, 0x5e, 0x00, 0xe2, 0x07, 0x40, 0x9c, 0xc4, 0x06, 0x0e, 0x06, 0x63, 0x40, 0x00,
+	0x00, 0x00, 0xff, 0xff, 0x8d, 0x09, 0x06, 0x6e, 0x32, 0x01, 0x00, 0x00,
 }
diff --git a/auth/store.go b/auth/store.go
index a0d0597..5598024 100644
--- a/auth/store.go
+++ b/auth/store.go
@@ -22,7 +22,7 @@ import (
 
 	"github.com/coreos/etcd/auth/authpb"
 	pb "github.com/coreos/etcd/etcdserver/etcdserverpb"
-	"github.com/coreos/etcd/storage/backend"
+	"github.com/coreos/etcd/mvcc/backend"
 	"github.com/coreos/pkg/capnslog"
 	"golang.org/x/crypto/bcrypt"
 )
diff --git a/clientv3/concurrency/election.go b/clientv3/concurrency/election.go
index 991bd26..aad942a 100644
--- a/clientv3/concurrency/election.go
+++ b/clientv3/concurrency/election.go
@@ -18,7 +18,7 @@ import (
 	"errors"
 
 	v3 "github.com/coreos/etcd/clientv3"
-	"github.com/coreos/etcd/storage/storagepb"
+	"github.com/coreos/etcd/mvcc/mvccpb"
 	"golang.org/x/net/context"
 )
 
@@ -128,7 +128,7 @@ func (e *Election) observe(ctx context.Context, ch chan<- v3.GetResponse) {
 			return
 		}
 
-		var kv *storagepb.KeyValue
+		var kv *mvccpb.KeyValue
 
 		cctx, cancel := context.WithCancel(ctx)
 		if len(resp.Kvs) == 0 {
@@ -144,7 +144,7 @@ func (e *Election) observe(ctx context.Context, ch chan<- v3.GetResponse) {
 				}
 				// only accept PUTs; a DELETE will make observe() spin
 				for _, ev := range wr.Events {
-					if ev.Type == storagepb.PUT {
+					if ev.Type == mvccpb.PUT {
 						kv = ev.Kv
 						break
 					}
@@ -162,12 +162,12 @@ func (e *Election) observe(ctx context.Context, ch chan<- v3.GetResponse) {
 				return
 			}
 			for _, ev := range wr.Events {
-				if ev.Type == storagepb.DELETE {
+				if ev.Type == mvccpb.DELETE {
 					keyDeleted = true
 					break
 				}
 				resp.Header = &wr.Header
-				resp.Kvs = []*storagepb.KeyValue{ev.Kv}
+				resp.Kvs = []*mvccpb.KeyValue{ev.Kv}
 				select {
 				case ch <- *resp:
 				case <-cctx.Done():
diff --git a/clientv3/concurrency/key.go b/clientv3/concurrency/key.go
index 43290a2..aa2ac3e 100644
--- a/clientv3/concurrency/key.go
+++ b/clientv3/concurrency/key.go
@@ -20,7 +20,7 @@ import (
 	"time"
 
 	v3 "github.com/coreos/etcd/clientv3"
-	"github.com/coreos/etcd/storage/storagepb"
+	"github.com/coreos/etcd/mvcc/mvccpb"
 	"golang.org/x/net/context"
 )
 
@@ -51,7 +51,7 @@ func waitDelete(ctx context.Context, client *v3.Client, key string, rev int64) e
 	wch := client.Watch(cctx, key, v3.WithRev(rev))
 	for wr := range wch {
 		for _, ev := range wr.Events {
-			if ev.Type == storagepb.DELETE {
+			if ev.Type == mvccpb.DELETE {
 				return nil
 			}
 		}
diff --git a/clientv3/integration/kv_test.go b/clientv3/integration/kv_test.go
index 053eb0d..006623c 100644
--- a/clientv3/integration/kv_test.go
+++ b/clientv3/integration/kv_test.go
@@ -23,8 +23,8 @@ import (
 	"github.com/coreos/etcd/clientv3"
 	"github.com/coreos/etcd/etcdserver/api/v3rpc/rpctypes"
 	"github.com/coreos/etcd/integration"
+	"github.com/coreos/etcd/mvcc/mvccpb"
 	"github.com/coreos/etcd/pkg/testutil"
-	"github.com/coreos/etcd/storage/storagepb"
 	"golang.org/x/net/context"
 )
 
@@ -99,7 +99,7 @@ func TestKVRange(t *testing.T) {
 		rev        int64
 		opts       []clientv3.OpOption
 
-		wantSet []*storagepb.KeyValue
+		wantSet []*mvccpb.KeyValue
 	}{
 		// range first two
 		{
@@ -107,7 +107,7 @@ func TestKVRange(t *testing.T) {
 			0,
 			nil,
 
-			[]*storagepb.KeyValue{
+			[]*mvccpb.KeyValue{
 				{Key: []byte("a"), Value: nil, CreateRevision: 2, ModRevision: 2, Version: 1},
 				{Key: []byte("b"), Value: nil, CreateRevision: 3, ModRevision: 3, Version: 1},
 			},
@@ -118,7 +118,7 @@ func TestKVRange(t *testing.T) {
 			0,
 			[]clientv3.OpOption{clientv3.WithSerializable()},
 
-			[]*storagepb.KeyValue{
+			[]*mvccpb.KeyValue{
 				{Key: []byte("a"), Value: nil, CreateRevision: 2, ModRevision: 2, Version: 1},
 				{Key: []byte("b"), Value: nil, CreateRevision: 3, ModRevision: 3, Version: 1},
 			},
@@ -129,7 +129,7 @@ func TestKVRange(t *testing.T) {
 			2,
 			nil,
 
-			[]*storagepb.KeyValue{
+			[]*mvccpb.KeyValue{
 				{Key: []byte("a"), Value: nil, CreateRevision: 2, ModRevision: 2, Version: 1},
 			},
 		},
@@ -139,7 +139,7 @@ func TestKVRange(t *testing.T) {
 			0,
 			[]clientv3.OpOption{clientv3.WithSort(clientv3.SortByKey, clientv3.SortAscend)},
 
-			[]*storagepb.KeyValue{
+			[]*mvccpb.KeyValue{
 				{Key: []byte("a"), Value: nil, CreateRevision: 2, ModRevision: 2, Version: 1},
 				{Key: []byte("b"), Value: nil, CreateRevision: 3, ModRevision: 3, Version: 1},
 				{Key: []byte("c"), Value: nil, CreateRevision: 4, ModRevision: 6, Version: 3},
@@ -154,7 +154,7 @@ func TestKVRange(t *testing.T) {
 			0,
 			[]clientv3.OpOption{clientv3.WithSort(clientv3.SortByCreateRevision, clientv3.SortDescend)},
 
-			[]*storagepb.KeyValue{
+			[]*mvccpb.KeyValue{
 				{Key: []byte("fop"), Value: nil, CreateRevision: 9, ModRevision: 9, Version: 1},
 				{Key: []byte("foo/abc"), Value: nil, CreateRevision: 8, ModRevision: 8, Version: 1},
 				{Key: []byte("foo"), Value: nil, CreateRevision: 7, ModRevision: 7, Version: 1},
@@ -169,7 +169,7 @@ func TestKVRange(t *testing.T) {
 			0,
 			[]clientv3.OpOption{clientv3.WithSort(clientv3.SortByModRevision, clientv3.SortDescend)},
 
-			[]*storagepb.KeyValue{
+			[]*mvccpb.KeyValue{
 				{Key: []byte("fop"), Value: nil, CreateRevision: 9, ModRevision: 9, Version: 1},
 				{Key: []byte("foo/abc"), Value: nil, CreateRevision: 8, ModRevision: 8, Version: 1},
 				{Key: []byte("foo"), Value: nil, CreateRevision: 7, ModRevision: 7, Version: 1},
@@ -184,7 +184,7 @@ func TestKVRange(t *testing.T) {
 			0,
 			[]clientv3.OpOption{clientv3.WithPrefix()},
 
-			[]*storagepb.KeyValue{
+			[]*mvccpb.KeyValue{
 				{Key: []byte("foo"), Value: nil, CreateRevision: 7, ModRevision: 7, Version: 1},
 				{Key: []byte("foo/abc"), Value: nil, CreateRevision: 8, ModRevision: 8, Version: 1},
 			},
@@ -195,7 +195,7 @@ func TestKVRange(t *testing.T) {
 			0,
 			[]clientv3.OpOption{clientv3.WithFromKey()},
 
-			[]*storagepb.KeyValue{
+			[]*mvccpb.KeyValue{
 				{Key: []byte("foo"), Value: nil, CreateRevision: 7, ModRevision: 7, Version: 1},
 				{Key: []byte("foo/abc"), Value: nil, CreateRevision: 8, ModRevision: 8, Version: 1},
 				{Key: []byte("fop"), Value: nil, CreateRevision: 9, ModRevision: 9, Version: 1},
@@ -392,7 +392,7 @@ func TestKVGetRetry(t *testing.T) {
 		if gerr != nil {
 			t.Fatal(gerr)
 		}
-		wkvs := []*storagepb.KeyValue{
+		wkvs := []*mvccpb.KeyValue{
 			{
 				Key:            []byte("foo"),
 				Value:          []byte("bar"),
diff --git a/clientv3/integration/mirror_test.go b/clientv3/integration/mirror_test.go
index f928f78..de2696f 100644
--- a/clientv3/integration/mirror_test.go
+++ b/clientv3/integration/mirror_test.go
@@ -21,8 +21,8 @@ import (
 
 	"github.com/coreos/etcd/clientv3/mirror"
 	"github.com/coreos/etcd/integration"
+	"github.com/coreos/etcd/mvcc/mvccpb"
 	"github.com/coreos/etcd/pkg/testutil"
-	"github.com/coreos/etcd/storage/storagepb"
 	"golang.org/x/net/context"
 )
 
@@ -40,7 +40,7 @@ func TestMirrorSync(t *testing.T) {
 
 	syncer := mirror.NewSyncer(c, "", 0)
 	gch, ech := syncer.SyncBase(context.TODO())
-	wkvs := []*storagepb.KeyValue{{Key: []byte("foo"), Value: []byte("bar"), CreateRevision: 2, ModRevision: 2, Version: 1}}
+	wkvs := []*mvccpb.KeyValue{{Key: []byte("foo"), Value: []byte("bar"), CreateRevision: 2, ModRevision: 2, Version: 1}}
 
 	for g := range gch {
 		if !reflect.DeepEqual(g.Kvs, wkvs) {
@@ -61,7 +61,7 @@ func TestMirrorSync(t *testing.T) {
 
 	select {
 	case r := <-wch:
-		wkv := &storagepb.KeyValue{Key: []byte("foo"), Value: []byte("bar"), CreateRevision: 2, ModRevision: 3, Version: 2}
+		wkv := &mvccpb.KeyValue{Key: []byte("foo"), Value: []byte("bar"), CreateRevision: 2, ModRevision: 3, Version: 2}
 		if !reflect.DeepEqual(r.Events[0].Kv, wkv) {
 			t.Fatalf("kv = %v, want %v", r.Events[0].Kv, wkv)
 		}
diff --git a/clientv3/integration/watch_test.go b/clientv3/integration/watch_test.go
index 437ee77..f11a654 100644
--- a/clientv3/integration/watch_test.go
+++ b/clientv3/integration/watch_test.go
@@ -25,8 +25,8 @@ import (
 	"github.com/coreos/etcd/etcdserver/api/v3rpc"
 	"github.com/coreos/etcd/etcdserver/api/v3rpc/rpctypes"
 	"github.com/coreos/etcd/integration"
+	mvccpb "github.com/coreos/etcd/mvcc/mvccpb"
 	"github.com/coreos/etcd/pkg/testutil"
-	storagepb "github.com/coreos/etcd/storage/storagepb"
 	"golang.org/x/net/context"
 )
 
@@ -419,7 +419,7 @@ func testWatchWithProgressNotify(t *testing.T, watchOnPut bool) {
 		}
 		if watchOnPut { // wait for put if watch on the put key
 			ev := []*clientv3.Event{{Type: clientv3.EventTypePut,
-				Kv: &storagepb.KeyValue{Key: []byte("foox"), Value: []byte("bar"), CreateRevision: 2, ModRevision: 2, Version: 1}}}
+				Kv: &mvccpb.KeyValue{Key: []byte("foox"), Value: []byte("bar"), CreateRevision: 2, ModRevision: 2, Version: 1}}}
 			if !reflect.DeepEqual(ev, resp.Events) {
 				t.Fatalf("expected %+v, got %+v", ev, resp.Events)
 			}
@@ -457,7 +457,7 @@ func TestWatchEventType(t *testing.T) {
 	}
 
 	tests := []struct {
-		et       storagepb.Event_EventType
+		et       mvccpb.Event_EventType
 		isCreate bool
 		isModify bool
 	}{{
diff --git a/clientv3/watch.go b/clientv3/watch.go
index f9bdaff..b41168d 100644
--- a/clientv3/watch.go
+++ b/clientv3/watch.go
@@ -20,17 +20,17 @@ import (
 
 	v3rpc "github.com/coreos/etcd/etcdserver/api/v3rpc/rpctypes"
 	pb "github.com/coreos/etcd/etcdserver/etcdserverpb"
-	storagepb "github.com/coreos/etcd/storage/storagepb"
+	mvccpb "github.com/coreos/etcd/mvcc/mvccpb"
 	"golang.org/x/net/context"
 	"google.golang.org/grpc"
 )
 
 const (
-	EventTypeDelete = storagepb.DELETE
-	EventTypePut    = storagepb.PUT
+	EventTypeDelete = mvccpb.DELETE
+	EventTypePut    = mvccpb.PUT
 )
 
-type Event storagepb.Event
+type Event mvccpb.Event
 
 type WatchChan <-chan WatchResponse
 
diff --git a/clientv3/watch_test.go b/clientv3/watch_test.go
index 2220770..318d45c 100644
--- a/clientv3/watch_test.go
+++ b/clientv3/watch_test.go
@@ -17,7 +17,7 @@ package clientv3
 import (
 	"testing"
 
-	"github.com/coreos/etcd/storage/storagepb"
+	"github.com/coreos/etcd/mvcc/mvccpb"
 )
 
 func TestEvent(t *testing.T) {
@@ -28,7 +28,7 @@ func TestEvent(t *testing.T) {
 	}{{
 		ev: &Event{
 			Type: EventTypePut,
-			Kv: &storagepb.KeyValue{
+			Kv: &mvccpb.KeyValue{
 				CreateRevision: 3,
 				ModRevision:    3,
 			},
@@ -37,7 +37,7 @@ func TestEvent(t *testing.T) {
 	}, {
 		ev: &Event{
 			Type: EventTypePut,
-			Kv: &storagepb.KeyValue{
+			Kv: &mvccpb.KeyValue{
 				CreateRevision: 3,
 				ModRevision:    4,
 			},
diff --git a/compactor/compactor.go b/compactor/compactor.go
index 7519b53..9155290 100644
--- a/compactor/compactor.go
+++ b/compactor/compactor.go
@@ -19,7 +19,7 @@ import (
 	"time"
 
 	pb "github.com/coreos/etcd/etcdserver/etcdserverpb"
-	"github.com/coreos/etcd/storage"
+	"github.com/coreos/etcd/mvcc"
 	"github.com/coreos/pkg/capnslog"
 	"github.com/jonboulle/clockwork"
 	"golang.org/x/net/context"
@@ -96,7 +96,7 @@ func (t *Periodic) Run() {
 
 			plog.Noticef("Starting auto-compaction at revision %d", rev)
 			_, err := t.c.Compact(t.ctx, &pb.CompactionRequest{Revision: rev})
-			if err == nil || err == storage.ErrCompacted {
+			if err == nil || err == mvcc.ErrCompacted {
 				t.revs = make([]int64, 0)
 				last = clock.Now()
 				plog.Noticef("Finished auto-compaction at revision %d", rev)
diff --git a/contrib/recipes/barrier.go b/contrib/recipes/barrier.go
index f2b2bc5..fba12b9 100644
--- a/contrib/recipes/barrier.go
+++ b/contrib/recipes/barrier.go
@@ -16,7 +16,7 @@ package recipe
 
 import (
 	v3 "github.com/coreos/etcd/clientv3"
-	"github.com/coreos/etcd/storage/storagepb"
+	"github.com/coreos/etcd/mvcc/mvccpb"
 	"golang.org/x/net/context"
 )
 
@@ -60,6 +60,6 @@ func (b *Barrier) Wait() error {
 		b.client,
 		b.key,
 		resp.Header.Revision,
-		[]storagepb.Event_EventType{storagepb.PUT, storagepb.DELETE})
+		[]mvccpb.Event_EventType{mvccpb.PUT, mvccpb.DELETE})
 	return err
 }
diff --git a/contrib/recipes/client.go b/contrib/recipes/client.go
index 9e283d4..1feb210 100644
--- a/contrib/recipes/client.go
+++ b/contrib/recipes/client.go
@@ -18,7 +18,7 @@ import (
 	"errors"
 
 	v3 "github.com/coreos/etcd/clientv3"
-	spb "github.com/coreos/etcd/storage/storagepb"
+	spb "github.com/coreos/etcd/mvcc/mvccpb"
 	"golang.org/x/net/context"
 )
 
diff --git a/contrib/recipes/double_barrier.go b/contrib/recipes/double_barrier.go
index a95a826..deae34d 100644
--- a/contrib/recipes/double_barrier.go
+++ b/contrib/recipes/double_barrier.go
@@ -16,7 +16,7 @@ package recipe
 
 import (
 	"github.com/coreos/etcd/clientv3"
-	"github.com/coreos/etcd/storage/storagepb"
+	"github.com/coreos/etcd/mvcc/mvccpb"
 	"golang.org/x/net/context"
 )
 
@@ -67,7 +67,7 @@ func (b *DoubleBarrier) Enter() error {
 		b.client,
 		b.key+"/ready",
 		ek.Revision(),
-		[]storagepb.Event_EventType{storagepb.PUT})
+		[]mvccpb.Event_EventType{mvccpb.PUT})
 	return err
 }
 
@@ -109,7 +109,7 @@ func (b *DoubleBarrier) Leave() error {
 			b.client,
 			string(highest.Key),
 			highest.ModRevision,
-			[]storagepb.Event_EventType{storagepb.DELETE})
+			[]mvccpb.Event_EventType{mvccpb.DELETE})
 		if err != nil {
 			return err
 		}
@@ -126,7 +126,7 @@ func (b *DoubleBarrier) Leave() error {
 		b.client,
 		key,
 		lowest.ModRevision,
-		[]storagepb.Event_EventType{storagepb.DELETE})
+		[]mvccpb.Event_EventType{mvccpb.DELETE})
 	if err != nil {
 		return err
 	}
diff --git a/contrib/recipes/priority_queue.go b/contrib/recipes/priority_queue.go
index 9da49d3..64d634c 100644
--- a/contrib/recipes/priority_queue.go
+++ b/contrib/recipes/priority_queue.go
@@ -18,7 +18,7 @@ import (
 	"fmt"
 
 	v3 "github.com/coreos/etcd/clientv3"
-	"github.com/coreos/etcd/storage/storagepb"
+	"github.com/coreos/etcd/mvcc/mvccpb"
 	"golang.org/x/net/context"
 )
 
@@ -65,7 +65,7 @@ func (q *PriorityQueue) Dequeue() (string, error) {
 		q.client,
 		q.key,
 		resp.Header.Revision,
-		[]storagepb.Event_EventType{storagepb.PUT})
+		[]mvccpb.Event_EventType{mvccpb.PUT})
 	if err != nil {
 		return "", err
 	}
diff --git a/contrib/recipes/queue.go b/contrib/recipes/queue.go
index ab5c9a4..edcb03f 100644
--- a/contrib/recipes/queue.go
+++ b/contrib/recipes/queue.go
@@ -16,7 +16,7 @@ package recipe
 
 import (
 	v3 "github.com/coreos/etcd/clientv3"
-	"github.com/coreos/etcd/storage/storagepb"
+	"github.com/coreos/etcd/mvcc/mvccpb"
 	"golang.org/x/net/context"
 )
 
@@ -61,7 +61,7 @@ func (q *Queue) Dequeue() (string, error) {
 		q.client,
 		q.keyPrefix,
 		resp.Header.Revision,
-		[]storagepb.Event_EventType{storagepb.PUT})
+		[]mvccpb.Event_EventType{mvccpb.PUT})
 	if err != nil {
 		return "", err
 	}
diff --git a/contrib/recipes/rwmutex.go b/contrib/recipes/rwmutex.go
index be51dd4..d39b363 100644
--- a/contrib/recipes/rwmutex.go
+++ b/contrib/recipes/rwmutex.go
@@ -16,7 +16,7 @@ package recipe
 
 import (
 	v3 "github.com/coreos/etcd/clientv3"
-	"github.com/coreos/etcd/storage/storagepb"
+	"github.com/coreos/etcd/mvcc/mvccpb"
 	"golang.org/x/net/context"
 )
 
@@ -91,7 +91,7 @@ func (rwm *RWMutex) waitOnLowest() error {
 		rwm.client,
 		string(lastKey.Kvs[0].Key),
 		rwm.myKey.Revision(),
-		[]storagepb.Event_EventType{storagepb.DELETE})
+		[]mvccpb.Event_EventType{mvccpb.DELETE})
 	return err
 }
 
diff --git a/contrib/recipes/watch.go b/contrib/recipes/watch.go
index 5f4ab2b..759c95e 100644
--- a/contrib/recipes/watch.go
+++ b/contrib/recipes/watch.go
@@ -16,12 +16,12 @@ package recipe
 
 import (
 	"github.com/coreos/etcd/clientv3"
-	"github.com/coreos/etcd/storage/storagepb"
+	"github.com/coreos/etcd/mvcc/mvccpb"
 	"golang.org/x/net/context"
 )
 
 // WaitEvents waits on a key until it observes the given events and returns the final one.
-func WaitEvents(c *clientv3.Client, key string, rev int64, evs []storagepb.Event_EventType) (*clientv3.Event, error) {
+func WaitEvents(c *clientv3.Client, key string, rev int64, evs []mvccpb.Event_EventType) (*clientv3.Event, error) {
 	wc := c.Watch(context.Background(), key, clientv3.WithRev(rev))
 	if wc == nil {
 		return nil, ErrNoWatcher
@@ -29,7 +29,7 @@ func WaitEvents(c *clientv3.Client, key string, rev int64, evs []storagepb.Event
 	return waitEvents(wc, evs), nil
 }
 
-func WaitPrefixEvents(c *clientv3.Client, prefix string, rev int64, evs []storagepb.Event_EventType) (*clientv3.Event, error) {
+func WaitPrefixEvents(c *clientv3.Client, prefix string, rev int64, evs []mvccpb.Event_EventType) (*clientv3.Event, error) {
 	wc := c.Watch(context.Background(), prefix, clientv3.WithPrefix(), clientv3.WithRev(rev))
 	if wc == nil {
 		return nil, ErrNoWatcher
@@ -37,7 +37,7 @@ func WaitPrefixEvents(c *clientv3.Client, prefix string, rev int64, evs []storag
 	return waitEvents(wc, evs), nil
 }
 
-func waitEvents(wc clientv3.WatchChan, evs []storagepb.Event_EventType) *clientv3.Event {
+func waitEvents(wc clientv3.WatchChan, evs []mvccpb.Event_EventType) *clientv3.Event {
 	i := 0
 	for wresp := range wc {
 		for _, ev := range wresp.Events {
diff --git a/e2e/ctl_v3_alarm_test.go b/e2e/ctl_v3_alarm_test.go
index 20fb997..a72e7e4 100644
--- a/e2e/ctl_v3_alarm_test.go
+++ b/e2e/ctl_v3_alarm_test.go
@@ -31,7 +31,7 @@ func alarmTest(cx ctlCtx) {
 	// test big put (to be rejected, and trigger quota alarm)
 	bigbuf := strings.Repeat("a", int(cx.quotaBackendBytes))
 	if err := ctlV3Put(cx, "abc", bigbuf, ""); err != nil {
-		if !strings.Contains(err.Error(), "etcdserver: storage: database space exceeded") {
+		if !strings.Contains(err.Error(), "etcdserver: mvcc: database space exceeded") {
 			cx.t.Fatal(err)
 		}
 	}
@@ -41,7 +41,7 @@ func alarmTest(cx ctlCtx) {
 
 	// alarm is on rejecting Puts and Txns
 	if err := ctlV3Put(cx, "def", smallbuf, ""); err != nil {
-		if !strings.Contains(err.Error(), "etcdserver: storage: database space exceeded") {
+		if !strings.Contains(err.Error(), "etcdserver: mvcc: database space exceeded") {
 			cx.t.Fatal(err)
 		}
 	}
diff --git a/etcdctl/READMEv3.md b/etcdctl/READMEv3.md
index c6f9783..2ea3799 100644
--- a/etcdctl/READMEv3.md
+++ b/etcdctl/READMEv3.md
@@ -723,7 +723,7 @@ On success, prints a line of JSON encoding the database hash, revision, total ke
 
 
 [etcdrpc]: ../etcdserver/etcdserverpb/rpc.proto
-[storagerpc]: ../storage/storagepb/kv.proto
+[storagerpc]: ../mvcc/mvccpb/kv.proto
 
 ## Compatibility Support
 
diff --git a/etcdctl/ctlv3/command/make_mirror_command.go b/etcdctl/ctlv3/command/make_mirror_command.go
index c59b8ad..4b97e91 100644
--- a/etcdctl/ctlv3/command/make_mirror_command.go
+++ b/etcdctl/ctlv3/command/make_mirror_command.go
@@ -23,7 +23,7 @@ import (
 	"github.com/coreos/etcd/clientv3"
 	"github.com/coreos/etcd/clientv3/mirror"
 	"github.com/coreos/etcd/etcdserver/api/v3rpc/rpctypes"
-	"github.com/coreos/etcd/storage/storagepb"
+	"github.com/coreos/etcd/mvcc/mvccpb"
 	"github.com/spf13/cobra"
 	"golang.org/x/net/context"
 )
@@ -125,10 +125,10 @@ func makeMirror(ctx context.Context, c *clientv3.Client, dc *clientv3.Client) er
 				ops = []clientv3.Op{}
 			}
 			switch ev.Type {
-			case storagepb.PUT:
+			case mvccpb.PUT:
 				ops = append(ops, clientv3.OpPut(string(ev.Kv.Key), string(ev.Kv.Value)))
 				atomic.AddInt64(&total, 1)
-			case storagepb.DELETE, storagepb.EXPIRE:
+			case mvccpb.DELETE, mvccpb.EXPIRE:
 				ops = append(ops, clientv3.OpDelete(string(ev.Kv.Key)))
 				atomic.AddInt64(&total, 1)
 			default:
diff --git a/etcdctl/ctlv3/command/printer.go b/etcdctl/ctlv3/command/printer.go
index 5a04cff..f177a9c 100644
--- a/etcdctl/ctlv3/command/printer.go
+++ b/etcdctl/ctlv3/command/printer.go
@@ -23,7 +23,7 @@ import (
 
 	v3 "github.com/coreos/etcd/clientv3"
 	pb "github.com/coreos/etcd/etcdserver/etcdserverpb"
-	spb "github.com/coreos/etcd/storage/storagepb"
+	spb "github.com/coreos/etcd/mvcc/mvccpb"
 	"github.com/dustin/go-humanize"
 	"github.com/olekukonko/tablewriter"
 )
diff --git a/etcdctl/ctlv3/command/snapshot_command.go b/etcdctl/ctlv3/command/snapshot_command.go
index 63cdf84..67503fd 100644
--- a/etcdctl/ctlv3/command/snapshot_command.go
+++ b/etcdctl/ctlv3/command/snapshot_command.go
@@ -28,12 +28,12 @@ import (
 	"github.com/coreos/etcd/etcdserver"
 	"github.com/coreos/etcd/etcdserver/etcdserverpb"
 	"github.com/coreos/etcd/etcdserver/membership"
+	"github.com/coreos/etcd/mvcc"
+	"github.com/coreos/etcd/mvcc/backend"
 	"github.com/coreos/etcd/pkg/fileutil"
 	"github.com/coreos/etcd/pkg/types"
 	"github.com/coreos/etcd/raft"
 	"github.com/coreos/etcd/raft/raftpb"
-	"github.com/coreos/etcd/storage"
-	"github.com/coreos/etcd/storage/backend"
 	"github.com/coreos/etcd/wal"
 	"github.com/spf13/cobra"
 	"golang.org/x/net/context"
@@ -275,7 +275,7 @@ func makeDB(snapdir, dbfile string) {
 	// update consistentIndex so applies go through on etcdserver despite
 	// having a new raft instance
 	be := backend.NewDefaultBackend(dbpath)
-	s := storage.NewStore(be, nil, &initIndex{})
+	s := mvcc.NewStore(be, nil, &initIndex{})
 	id := s.TxnBegin()
 	btx := be.BatchTx()
 	del := func(k, v []byte) error {
diff --git a/etcdctl/ctlv3/command/util.go b/etcdctl/ctlv3/command/util.go
index 1d4e0ae..126b9ed 100644
--- a/etcdctl/ctlv3/command/util.go
+++ b/etcdctl/ctlv3/command/util.go
@@ -19,7 +19,7 @@ import (
 	"fmt"
 	"regexp"
 
-	pb "github.com/coreos/etcd/storage/storagepb"
+	pb "github.com/coreos/etcd/mvcc/mvccpb"
 	"github.com/spf13/cobra"
 	"golang.org/x/net/context"
 )
diff --git a/etcdserver/api/v3rpc/maintenance.go b/etcdserver/api/v3rpc/maintenance.go
index 6de1c66..552398b 100644
--- a/etcdserver/api/v3rpc/maintenance.go
+++ b/etcdserver/api/v3rpc/maintenance.go
@@ -19,8 +19,8 @@ import (
 
 	"github.com/coreos/etcd/etcdserver"
 	pb "github.com/coreos/etcd/etcdserver/etcdserverpb"
+	"github.com/coreos/etcd/mvcc/backend"
 	"github.com/coreos/etcd/pkg/types"
-	"github.com/coreos/etcd/storage/backend"
 	"github.com/coreos/etcd/version"
 	"golang.org/x/net/context"
 )
diff --git a/etcdserver/api/v3rpc/rpctypes/error.go b/etcdserver/api/v3rpc/rpctypes/error.go
index 4c89820..7fc3678 100644
--- a/etcdserver/api/v3rpc/rpctypes/error.go
+++ b/etcdserver/api/v3rpc/rpctypes/error.go
@@ -23,9 +23,9 @@ var (
 	ErrEmptyKey     = grpc.Errorf(codes.InvalidArgument, "etcdserver: key is not provided")
 	ErrTooManyOps   = grpc.Errorf(codes.InvalidArgument, "etcdserver: too many operations in txn request")
 	ErrDuplicateKey = grpc.Errorf(codes.InvalidArgument, "etcdserver: duplicate key given in txn request")
-	ErrCompacted    = grpc.Errorf(codes.OutOfRange, "etcdserver: storage: required revision has been compacted")
-	ErrFutureRev    = grpc.Errorf(codes.OutOfRange, "etcdserver: storage: required revision is a future revision")
-	ErrNoSpace      = grpc.Errorf(codes.ResourceExhausted, "etcdserver: storage: database space exceeded")
+	ErrCompacted    = grpc.Errorf(codes.OutOfRange, "etcdserver: mvcc: required revision has been compacted")
+	ErrFutureRev    = grpc.Errorf(codes.OutOfRange, "etcdserver: mvcc: required revision is a future revision")
+	ErrNoSpace      = grpc.Errorf(codes.ResourceExhausted, "etcdserver: mvcc: database space exceeded")
 
 	ErrLeaseNotFound = grpc.Errorf(codes.NotFound, "etcdserver: requested lease not found")
 	ErrLeaseExist    = grpc.Errorf(codes.FailedPrecondition, "etcdserver: lease already exists")
diff --git a/etcdserver/api/v3rpc/util.go b/etcdserver/api/v3rpc/util.go
index 4b2d1a8..0f36057 100644
--- a/etcdserver/api/v3rpc/util.go
+++ b/etcdserver/api/v3rpc/util.go
@@ -19,16 +19,16 @@ import (
 	"github.com/coreos/etcd/etcdserver"
 	"github.com/coreos/etcd/etcdserver/api/v3rpc/rpctypes"
 	"github.com/coreos/etcd/lease"
-	"github.com/coreos/etcd/storage"
+	"github.com/coreos/etcd/mvcc"
 	"google.golang.org/grpc"
 	"google.golang.org/grpc/codes"
 )
 
 func togRPCError(err error) error {
 	switch err {
-	case storage.ErrCompacted:
+	case mvcc.ErrCompacted:
 		return rpctypes.ErrCompacted
-	case storage.ErrFutureRev:
+	case mvcc.ErrFutureRev:
 		return rpctypes.ErrFutureRev
 	case lease.ErrLeaseNotFound:
 		return rpctypes.ErrLeaseNotFound
diff --git a/etcdserver/api/v3rpc/watch.go b/etcdserver/api/v3rpc/watch.go
index 2f380a5..418e69a 100644
--- a/etcdserver/api/v3rpc/watch.go
+++ b/etcdserver/api/v3rpc/watch.go
@@ -21,15 +21,15 @@ import (
 
 	"github.com/coreos/etcd/etcdserver"
 	pb "github.com/coreos/etcd/etcdserver/etcdserverpb"
-	"github.com/coreos/etcd/storage"
-	"github.com/coreos/etcd/storage/storagepb"
+	"github.com/coreos/etcd/mvcc"
+	"github.com/coreos/etcd/mvcc/mvccpb"
 )
 
 type watchServer struct {
 	clusterID int64
 	memberID  int64
 	raftTimer etcdserver.RaftTimer
-	watchable storage.Watchable
+	watchable mvcc.Watchable
 }
 
 func NewWatchServer(s *etcdserver.EtcdServer) pb.WatchServer {
@@ -71,7 +71,7 @@ const (
 )
 
 // serverWatchStream is an etcd server side stream. It receives requests
-// from client side gRPC stream. It receives watch events from storage.WatchStream,
+// from client side gRPC stream. It receives watch events from mvcc.WatchStream,
 // and creates responses that forwarded to gRPC stream.
 // It also forwards control message like watch created and canceled.
 type serverWatchStream struct {
@@ -80,12 +80,12 @@ type serverWatchStream struct {
 	raftTimer etcdserver.RaftTimer
 
 	gRPCStream  pb.Watch_WatchServer
-	watchStream storage.WatchStream
+	watchStream mvcc.WatchStream
 	ctrlStream  chan *pb.WatchResponse
 
 	// progress tracks the watchID that stream might need to send
 	// progress to.
-	progress map[storage.WatchID]bool
+	progress map[mvcc.WatchID]bool
 	// mu protects progress
 	mu sync.Mutex
 
@@ -102,7 +102,7 @@ func (ws *watchServer) Watch(stream pb.Watch_WatchServer) error {
 		watchStream: ws.watchable.NewWatchStream(),
 		// chan for sending control response like watcher created and canceled.
 		ctrlStream: make(chan *pb.WatchResponse, ctrlStreamBufLen),
-		progress:   make(map[storage.WatchID]bool),
+		progress:   make(map[mvcc.WatchID]bool),
 		closec:     make(chan struct{}),
 	}
 	defer sws.close()
@@ -154,7 +154,7 @@ func (sws *serverWatchStream) recvLoop() error {
 		case *pb.WatchRequest_CancelRequest:
 			if uv.CancelRequest != nil {
 				id := uv.CancelRequest.WatchId
-				err := sws.watchStream.Cancel(storage.WatchID(id))
+				err := sws.watchStream.Cancel(mvcc.WatchID(id))
 				if err == nil {
 					sws.ctrlStream <- &pb.WatchResponse{
 						Header:   sws.newResponseHeader(sws.watchStream.Rev()),
@@ -162,7 +162,7 @@ func (sws *serverWatchStream) recvLoop() error {
 						Canceled: true,
 					}
 					sws.mu.Lock()
-					delete(sws.progress, storage.WatchID(id))
+					delete(sws.progress, mvcc.WatchID(id))
 					sws.mu.Unlock()
 				}
 			}
@@ -175,9 +175,9 @@ func (sws *serverWatchStream) recvLoop() error {
 
 func (sws *serverWatchStream) sendLoop() {
 	// watch ids that are currently active
-	ids := make(map[storage.WatchID]struct{})
+	ids := make(map[mvcc.WatchID]struct{})
 	// watch responses pending on a watch id creation message
-	pending := make(map[storage.WatchID][]*pb.WatchResponse)
+	pending := make(map[mvcc.WatchID][]*pb.WatchResponse)
 
 	interval := GetProgressReportInterval()
 	progressTicker := time.NewTicker(interval)
@@ -190,11 +190,11 @@ func (sws *serverWatchStream) sendLoop() {
 				return
 			}
 
-			// TODO: evs is []storagepb.Event type
-			// either return []*storagepb.Event from storage package
-			// or define protocol buffer with []storagepb.Event.
+			// TODO: evs is []mvccpb.Event type
+			// either return []*mvccpb.Event from the mvcc package
+			// or define protocol buffer with []mvccpb.Event.
 			evs := wresp.Events
-			events := make([]*storagepb.Event, len(evs))
+			events := make([]*mvccpb.Event, len(evs))
 			for i := range evs {
 				events[i] = &evs[i]
 			}
@@ -213,7 +213,7 @@ func (sws *serverWatchStream) sendLoop() {
 				continue
 			}
 
-			storage.ReportEventReceived()
+			mvcc.ReportEventReceived()
 			if err := sws.gRPCStream.Send(wr); err != nil {
 				return
 			}
@@ -234,7 +234,7 @@ func (sws *serverWatchStream) sendLoop() {
 			}
 
 			// track id creation
-			wid := storage.WatchID(c.WatchId)
+			wid := mvcc.WatchID(c.WatchId)
 			if c.Canceled {
 				delete(ids, wid)
 				continue
@@ -243,7 +243,7 @@ func (sws *serverWatchStream) sendLoop() {
 				// flush buffered events
 				ids[wid] = struct{}{}
 				for _, v := range pending[wid] {
-					storage.ReportEventReceived()
+					mvcc.ReportEventReceived()
 					if err := sws.gRPCStream.Send(v); err != nil {
 						return
 					}
@@ -260,11 +260,11 @@ func (sws *serverWatchStream) sendLoop() {
 		case <-sws.closec:
 			// drain the chan to clean up pending events
 			for range sws.watchStream.Chan() {
-				storage.ReportEventReceived()
+				mvcc.ReportEventReceived()
 			}
 			for _, wrs := range pending {
 				for range wrs {
-					storage.ReportEventReceived()
+					mvcc.ReportEventReceived()
 				}
 			}
 		}
diff --git a/etcdserver/apply.go b/etcdserver/apply.go
index a55d4d7..0eed6f2 100644
--- a/etcdserver/apply.go
+++ b/etcdserver/apply.go
@@ -21,9 +21,9 @@ import (
 
 	pb "github.com/coreos/etcd/etcdserver/etcdserverpb"
 	"github.com/coreos/etcd/lease"
+	"github.com/coreos/etcd/mvcc"
+	"github.com/coreos/etcd/mvcc/mvccpb"
 	"github.com/coreos/etcd/pkg/types"
-	dstorage "github.com/coreos/etcd/storage"
-	"github.com/coreos/etcd/storage/storagepb"
 	"github.com/gogo/protobuf/proto"
 )
 
@@ -166,7 +166,7 @@ func (a *applierV3backend) Range(txnID int64, r *pb.RangeRequest) (*pb.RangeResp
 	resp.Header = &pb.ResponseHeader{}
 
 	var (
-		kvs []storagepb.KeyValue
+		kvs []mvccpb.KeyValue
 		rev int64
 		err error
 	)
@@ -292,12 +292,12 @@ func (a *applierV3backend) Txn(rt *pb.TxnRequest) (*pb.TxnResponse, error) {
 func (a *applierV3backend) applyCompare(c *pb.Compare) (int64, bool) {
 	ckvs, rev, err := a.s.KV().Range(c.Key, nil, 1, 0)
 	if err != nil {
-		if err == dstorage.ErrTxnIDMismatch {
+		if err == mvcc.ErrTxnIDMismatch {
 			panic("unexpected txn ID mismatch error")
 		}
 		return rev, false
 	}
-	var ckv storagepb.KeyValue
+	var ckv mvccpb.KeyValue
 	if len(ckvs) != 0 {
 		ckv = ckvs[0]
 	} else {
@@ -557,7 +557,7 @@ func (a *quotaApplierV3) LeaseGrant(lc *pb.LeaseGrantRequest) (*pb.LeaseGrantRes
 	return resp, err
 }
 
-type kvSort struct{ kvs []storagepb.KeyValue }
+type kvSort struct{ kvs []mvccpb.KeyValue }
 
 func (s *kvSort) Swap(i, j int) {
 	t := s.kvs[i]
@@ -625,10 +625,10 @@ func (a *applierV3backend) checkRequestRange(reqs []*pb.RequestUnion) error {
 		}
 
 		if greq.Revision > a.s.KV().Rev() {
-			return dstorage.ErrFutureRev
+			return mvcc.ErrFutureRev
 		}
 		if greq.Revision < a.s.KV().FirstRev() {
-			return dstorage.ErrCompacted
+			return mvcc.ErrCompacted
 		}
 	}
 	return nil
diff --git a/etcdserver/consistent_index.go b/etcdserver/consistent_index.go
index a387b75..4379397 100644
--- a/etcdserver/consistent_index.go
+++ b/etcdserver/consistent_index.go
@@ -19,7 +19,7 @@ import (
 )
 
 // consistentIndex represents the offset of an entry in a consistent replica log.
-// It implements the storage.ConsistentIndexGetter interface.
+// It implements the mvcc.ConsistentIndexGetter interface.
 // It is always set to the offset of current entry before executing the entry,
 // so ConsistentWatchableKV could get the consistent index from it.
 type consistentIndex uint64
diff --git a/etcdserver/etcdserverpb/etcdserver.pb.go b/etcdserver/etcdserverpb/etcdserver.pb.go
index 872997b..56e4478 100644
--- a/etcdserver/etcdserverpb/etcdserver.pb.go
+++ b/etcdserver/etcdserverpb/etcdserver.pb.go
@@ -110,23 +110,23 @@ var _ = math.Inf
 const _ = proto.GoGoProtoPackageIsVersion1
 
 type Request struct {
-	ID               uint64 `protobuf:"varint,1,opt,name=ID,json=iD" json:"ID"`
-	Method           string `protobuf:"bytes,2,opt,name=Method,json=method" json:"Method"`
-	Path             string `protobuf:"bytes,3,opt,name=Path,json=path" json:"Path"`
-	Val              string `protobuf:"bytes,4,opt,name=Val,json=val" json:"Val"`
-	Dir              bool   `protobuf:"varint,5,opt,name=Dir,json=dir" json:"Dir"`
-	PrevValue        string `protobuf:"bytes,6,opt,name=PrevValue,json=prevValue" json:"PrevValue"`
-	PrevIndex        uint64 `protobuf:"varint,7,opt,name=PrevIndex,json=prevIndex" json:"PrevIndex"`
-	PrevExist        *bool  `protobuf:"varint,8,opt,name=PrevExist,json=prevExist" json:"PrevExist,omitempty"`
-	Expiration       int64  `protobuf:"varint,9,opt,name=Expiration,json=expiration" json:"Expiration"`
-	Wait             bool   `protobuf:"varint,10,opt,name=Wait,json=wait" json:"Wait"`
-	Since            uint64 `protobuf:"varint,11,opt,name=Since,json=since" json:"Since"`
-	Recursive        bool   `protobuf:"varint,12,opt,name=Recursive,json=recursive" json:"Recursive"`
-	Sorted           bool   `protobuf:"varint,13,opt,name=Sorted,json=sorted" json:"Sorted"`
-	Quorum           bool   `protobuf:"varint,14,opt,name=Quorum,json=quorum" json:"Quorum"`
-	Time             int64  `protobuf:"varint,15,opt,name=Time,json=time" json:"Time"`
-	Stream           bool   `protobuf:"varint,16,opt,name=Stream,json=stream" json:"Stream"`
-	Refresh          *bool  `protobuf:"varint,17,opt,name=Refresh,json=refresh" json:"Refresh,omitempty"`
+	ID               uint64 `protobuf:"varint,1,opt,name=ID" json:"ID"`
+	Method           string `protobuf:"bytes,2,opt,name=Method" json:"Method"`
+	Path             string `protobuf:"bytes,3,opt,name=Path" json:"Path"`
+	Val              string `protobuf:"bytes,4,opt,name=Val" json:"Val"`
+	Dir              bool   `protobuf:"varint,5,opt,name=Dir" json:"Dir"`
+	PrevValue        string `protobuf:"bytes,6,opt,name=PrevValue" json:"PrevValue"`
+	PrevIndex        uint64 `protobuf:"varint,7,opt,name=PrevIndex" json:"PrevIndex"`
+	PrevExist        *bool  `protobuf:"varint,8,opt,name=PrevExist" json:"PrevExist,omitempty"`
+	Expiration       int64  `protobuf:"varint,9,opt,name=Expiration" json:"Expiration"`
+	Wait             bool   `protobuf:"varint,10,opt,name=Wait" json:"Wait"`
+	Since            uint64 `protobuf:"varint,11,opt,name=Since" json:"Since"`
+	Recursive        bool   `protobuf:"varint,12,opt,name=Recursive" json:"Recursive"`
+	Sorted           bool   `protobuf:"varint,13,opt,name=Sorted" json:"Sorted"`
+	Quorum           bool   `protobuf:"varint,14,opt,name=Quorum" json:"Quorum"`
+	Time             int64  `protobuf:"varint,15,opt,name=Time" json:"Time"`
+	Stream           bool   `protobuf:"varint,16,opt,name=Stream" json:"Stream"`
+	Refresh          *bool  `protobuf:"varint,17,opt,name=Refresh" json:"Refresh,omitempty"`
 	XXX_unrecognized []byte `json:"-"`
 }
 
@@ -136,8 +136,8 @@ func (*Request) ProtoMessage()               {}
 func (*Request) Descriptor() ([]byte, []int) { return fileDescriptorEtcdserver, []int{0} }
 
 type Metadata struct {
-	NodeID           uint64 `protobuf:"varint,1,opt,name=NodeID,json=nodeID" json:"NodeID"`
-	ClusterID        uint64 `protobuf:"varint,2,opt,name=ClusterID,json=clusterID" json:"ClusterID"`
+	NodeID           uint64 `protobuf:"varint,1,opt,name=NodeID" json:"NodeID"`
+	ClusterID        uint64 `protobuf:"varint,2,opt,name=ClusterID" json:"ClusterID"`
 	XXX_unrecognized []byte `json:"-"`
 }
 
@@ -1005,30 +1005,25 @@ var (
 )
 
 var fileDescriptorEtcdserver = []byte{
-	// 388 bytes of a gzipped FileDescriptorProto
-	0x1f, 0x8b, 0x08, 0x00, 0x00, 0x09, 0x6e, 0x88, 0x02, 0xff, 0x5c, 0x92, 0xd1, 0xee, 0xd2, 0x30,
-	0x14, 0xc6, 0xff, 0xdb, 0xca, 0x60, 0x15, 0x15, 0x1b, 0x62, 0x4e, 0x88, 0x41, 0x43, 0xbc, 0xf0,
-	0x4a, 0xdf, 0x01, 0xe1, 0x82, 0x44, 0x0d, 0x82, 0xd1, 0xeb, 0xba, 0x1d, 0xa1, 0x09, 0x5b, 0x47,
-	0xdb, 0x4d, 0xde, 0xc0, 0x57, 0xe3, 0xd2, 0x27, 0x30, 0xea, 0x93, 0xd8, 0x16, 0x86, 0xd5, 0x8b,
-	0x26, 0xcb, 0xef, 0xfb, 0xce, 0xe9, 0xd7, 0x73, 0x46, 0x47, 0x68, 0xf2, 0x42, 0xa3, 0x6a, 0x51,
-	0xbd, 0xac, 0x95, 0x34, 0x92, 0x0d, 0xff, 0x92, 0xfa, 0xf3, 0x64, 0xbc, 0x93, 0x3b, 0xe9, 0x85,
-	0x57, 0xee, 0xeb, 0xe2, 0x99, 0x7d, 0x23, 0xb4, 0xbf, 0xc1, 0x63, 0x83, 0xda, 0xb0, 0x31, 0x8d,
-	0x57, 0x0b, 0x88, 0x9e, 0x45, 0x2f, 0xc8, 0x9c, 0x9c, 0x7f, 0x3c, 0xbd, 0xdb, 0xc4, 0x62, 0xc1,
-	0x9e, 0xd0, 0xf4, 0x2d, 0x9a, 0xbd, 0x2c, 0x20, 0xb6, 0x4a, 0x76, 0x55, 0xd2, 0xd2, 0x33, 0x06,
-	0x94, 0xac, 0xb9, 0xd9, 0x43, 0x12, 0x68, 0xa4, 0xb6, 0x84, 0x3d, 0xa6, 0xc9, 0x47, 0x7e, 0x00,
-	0x12, 0x08, 0x49, 0xcb, 0x0f, 0x8e, 0x2f, 0x84, 0x82, 0x9e, 0xe5, 0x83, 0x8e, 0x17, 0x42, 0xb1,
-	0x19, 0xcd, 0xd6, 0x0a, 0x5b, 0x5b, 0xd3, 0x20, 0xa4, 0x41, 0x55, 0x56, 0x77, 0xb8, 0xf3, 0xac,
-	0xaa, 0x02, 0x4f, 0xd0, 0x0f, 0x82, 0x7a, 0x8f, 0xc7, 0x9d, 0x67, 0x79, 0x12, 0xda, 0xc0, 0xe0,
-	0x76, 0x4b, 0x74, 0xf1, 0x78, 0xcc, 0x9e, 0x53, 0xba, 0x3c, 0xd5, 0x42, 0x71, 0x23, 0x64, 0x05,
-	0x99, 0x35, 0x25, 0xd7, 0x46, 0x14, 0x6f, 0xdc, 0xbd, 0xed, 0x13, 0x17, 0x06, 0x68, 0x10, 0x95,
-	0x7c, 0xb5, 0x84, 0x4d, 0x68, 0x6f, 0x2b, 0xaa, 0x1c, 0xe1, 0x5e, 0x90, 0xa1, 0xa7, 0x1d, 0x72,
-	0xf7, 0x6f, 0x30, 0x6f, 0x94, 0x16, 0x2d, 0xc2, 0x30, 0x28, 0xcd, 0x54, 0x87, 0xdd, 0x4c, 0xb7,
-	0x52, 0x19, 0x2c, 0xe0, 0x7e, 0x60, 0x48, 0xb5, 0x67, 0x4e, 0x7d, 0xdf, 0x48, 0xd5, 0x94, 0xf0,
-	0x20, 0x54, 0x8f, 0x9e, 0xb9, 0x54, 0x1f, 0x44, 0x89, 0xf0, 0x30, 0x48, 0x4d, 0x8c, 0x25, 0xbe,
-	0xab, 0x51, 0xc8, 0x4b, 0x18, 0xfd, 0xd3, 0xd5, 0x33, 0x36, 0x75, 0x8b, 0xfe, 0xa2, 0x50, 0xef,
-	0xe1, 0x51, 0x30, 0x95, 0xbe, 0xba, 0xc0, 0xd9, 0x1b, 0x3a, 0xb0, 0x7b, 0xe6, 0x05, 0x37, 0xdc,
-	0x75, 0x7a, 0x27, 0x0b, 0xfc, 0xef, 0x6f, 0x48, 0x2b, 0xcf, 0xdc, 0x0b, 0x5f, 0x1f, 0x1a, 0x6d,
-	0x50, 0x59, 0x43, 0x1c, 0x6e, 0x21, 0xef, 0xf0, 0x7c, 0x74, 0xfe, 0x35, 0xbd, 0x3b, 0xff, 0x9e,
-	0x46, 0xdf, 0xed, 0xf9, 0x69, 0xcf, 0x9f, 0x00, 0x00, 0x00, 0xff, 0xff, 0xfb, 0x8e, 0x1a, 0x0d,
-	0xa0, 0x02, 0x00, 0x00,
+	// 320 bytes of a gzipped FileDescriptorProto
+	0x1f, 0x8b, 0x08, 0x00, 0x00, 0x09, 0x6e, 0x88, 0x02, 0xff, 0x5c, 0x91, 0x4f, 0x4f, 0xf2, 0x40,
+	0x10, 0xc6, 0x69, 0x29, 0xff, 0xf6, 0xe5, 0xd5, 0x52, 0x31, 0x4e, 0x3c, 0xa0, 0xe1, 0xe4, 0x49,
+	0xcf, 0x5e, 0x11, 0x0e, 0x1c, 0x34, 0x08, 0x46, 0xcf, 0x2b, 0x1d, 0x61, 0x13, 0x60, 0x71, 0x3a,
+	0x4b, 0xf8, 0x88, 0x1c, 0xfd, 0x04, 0x46, 0xfd, 0x22, 0xba, 0xd5, 0xc6, 0x6e, 0x3d, 0x6c, 0xd2,
+	0xfc, 0x9e, 0x67, 0x66, 0x9e, 0x99, 0x8a, 0x10, 0x79, 0x1a, 0x27, 0x48, 0x1b, 0xa4, 0xf3, 0x35,
+	0x69, 0xd6, 0x51, 0x33, 0x27, 0xeb, 0xc7, 0xe3, 0xf6, 0x4c, 0xcf, 0xf4, 0xb7, 0x70, 0x91, 0x7e,
+	0xfd, 0x78, 0xba, 0x9f, 0xbe, 0xa8, 0x8d, 0xf1, 0xd9, 0x60, 0xc2, 0x51, 0x28, 0xfc, 0x61, 0x1f,
+	0xbc, 0x53, 0xef, 0x2c, 0xe8, 0x05, 0xbb, 0xd7, 0x93, 0x52, 0xd4, 0x16, 0xd5, 0x6b, 0xe4, 0xb9,
+	0x8e, 0xc1, 0xb7, 0xb4, 0x91, 0xd1, 0x48, 0x04, 0x23, 0xc9, 0x73, 0x28, 0x3b, 0xac, 0x25, 0xca,
+	0xf7, 0x72, 0x01, 0x41, 0x11, 0xf5, 0x15, 0x41, 0xc5, 0xa2, 0x7a, 0x86, 0x8e, 0x44, 0x63, 0x44,
+	0xb8, 0xb1, 0x4e, 0x83, 0x50, 0x75, 0xbc, 0x99, 0x30, 0x5c, 0xc5, 0xb8, 0x85, 0x9a, 0x93, 0x20,
+	0x13, 0x06, 0x5b, 0x95, 0x30, 0xd4, 0x7f, 0x5b, 0x79, 0x11, 0x08, 0x31, 0xd8, 0xae, 0x15, 0x49,
+	0x56, 0x7a, 0x05, 0x0d, 0xab, 0x94, 0xf3, 0x78, 0x0f, 0x52, 0x31, 0x08, 0x67, 0xf0, 0x81, 0xa8,
+	0x4c, 0xd4, 0x6a, 0x8a, 0xf0, 0xaf, 0xd8, 0x7b, 0x8c, 0x53, 0x43, 0x89, 0xda, 0x20, 0x34, 0x1d,
+	0xb7, 0x5d, 0x7b, 0xa2, 0x89, 0x31, 0x86, 0xff, 0x45, 0x7a, 0x6b, 0x34, 0x99, 0x25, 0xec, 0x39,
+	0xd4, 0x4e, 0xbb, 0x53, 0x4b, 0x84, 0x7d, 0x27, 0x41, 0x5a, 0xcf, 0x84, 0x72, 0x09, 0xa1, 0xe3,
+	0x3c, 0x4c, 0x2f, 0xfd, 0x44, 0x98, 0xcc, 0xa1, 0x95, 0x2f, 0xd2, 0xbd, 0x14, 0x75, 0x7b, 0x63,
+	0x19, 0x4b, 0x96, 0x69, 0xe1, 0x8d, 0x8e, 0xf1, 0xcf, 0x5f, 0xb0, 0x39, 0xaf, 0x16, 0x26, 0x61,
+	0x24, 0x2b, 0xf8, 0xb9, 0xd0, 0x0b, 0x77, 0xef, 0x9d, 0xd2, 0xee, 0xa3, 0xe3, 0xbd, 0xd8, 0xf7,
+	0x66, 0xdf, 0x57, 0x00, 0x00, 0x00, 0xff, 0xff, 0x32, 0x78, 0xf7, 0x38, 0x05, 0x02, 0x00, 0x00,
 }
diff --git a/etcdserver/etcdserverpb/raft_internal.pb.go b/etcdserver/etcdserverpb/raft_internal.pb.go
index 7771eac..b3c1c5c 100644
--- a/etcdserver/etcdserverpb/raft_internal.pb.go
+++ b/etcdserver/etcdserverpb/raft_internal.pb.go
@@ -22,22 +22,22 @@ var _ = math.Inf
 // An InternalRaftRequest is the union of all requests which can be
 // sent via raft.
 type InternalRaftRequest struct {
-	ID                     uint64                         `protobuf:"varint,1,opt,name=ID,json=iD,proto3" json:"ID,omitempty"`
+	ID                     uint64                         `protobuf:"varint,1,opt,name=ID,proto3" json:"ID,omitempty"`
 	V2                     *Request                       `protobuf:"bytes,2,opt,name=v2" json:"v2,omitempty"`
 	Range                  *RangeRequest                  `protobuf:"bytes,3,opt,name=range" json:"range,omitempty"`
 	Put                    *PutRequest                    `protobuf:"bytes,4,opt,name=put" json:"put,omitempty"`
-	DeleteRange            *DeleteRangeRequest            `protobuf:"bytes,5,opt,name=delete_range,json=deleteRange" json:"delete_range,omitempty"`
+	DeleteRange            *DeleteRangeRequest            `protobuf:"bytes,5,opt,name=delete_range" json:"delete_range,omitempty"`
 	Txn                    *TxnRequest                    `protobuf:"bytes,6,opt,name=txn" json:"txn,omitempty"`
 	Compaction             *CompactionRequest             `protobuf:"bytes,7,opt,name=compaction" json:"compaction,omitempty"`
-	LeaseGrant             *LeaseGrantRequest             `protobuf:"bytes,8,opt,name=lease_grant,json=leaseGrant" json:"lease_grant,omitempty"`
-	LeaseRevoke            *LeaseRevokeRequest            `protobuf:"bytes,9,opt,name=lease_revoke,json=leaseRevoke" json:"lease_revoke,omitempty"`
-	AuthEnable             *AuthEnableRequest             `protobuf:"bytes,10,opt,name=auth_enable,json=authEnable" json:"auth_enable,omitempty"`
-	AuthUserAdd            *AuthUserAddRequest            `protobuf:"bytes,11,opt,name=auth_user_add,json=authUserAdd" json:"auth_user_add,omitempty"`
-	AuthUserDelete         *AuthUserDeleteRequest         `protobuf:"bytes,12,opt,name=auth_user_delete,json=authUserDelete" json:"auth_user_delete,omitempty"`
-	AuthUserChangePassword *AuthUserChangePasswordRequest `protobuf:"bytes,13,opt,name=auth_user_change_password,json=authUserChangePassword" json:"auth_user_change_password,omitempty"`
-	AuthUserGrant          *AuthUserGrantRequest          `protobuf:"bytes,14,opt,name=auth_user_grant,json=authUserGrant" json:"auth_user_grant,omitempty"`
-	AuthRoleAdd            *AuthRoleAddRequest            `protobuf:"bytes,15,opt,name=auth_role_add,json=authRoleAdd" json:"auth_role_add,omitempty"`
-	AuthRoleGrant          *AuthRoleGrantRequest          `protobuf:"bytes,16,opt,name=auth_role_grant,json=authRoleGrant" json:"auth_role_grant,omitempty"`
+	LeaseGrant             *LeaseGrantRequest             `protobuf:"bytes,8,opt,name=lease_grant" json:"lease_grant,omitempty"`
+	LeaseRevoke            *LeaseRevokeRequest            `protobuf:"bytes,9,opt,name=lease_revoke" json:"lease_revoke,omitempty"`
+	AuthEnable             *AuthEnableRequest             `protobuf:"bytes,10,opt,name=auth_enable" json:"auth_enable,omitempty"`
+	AuthUserAdd            *AuthUserAddRequest            `protobuf:"bytes,11,opt,name=auth_user_add" json:"auth_user_add,omitempty"`
+	AuthUserDelete         *AuthUserDeleteRequest         `protobuf:"bytes,12,opt,name=auth_user_delete" json:"auth_user_delete,omitempty"`
+	AuthUserChangePassword *AuthUserChangePasswordRequest `protobuf:"bytes,13,opt,name=auth_user_change_password" json:"auth_user_change_password,omitempty"`
+	AuthUserGrant          *AuthUserGrantRequest          `protobuf:"bytes,14,opt,name=auth_user_grant" json:"auth_user_grant,omitempty"`
+	AuthRoleAdd            *AuthRoleAddRequest            `protobuf:"bytes,15,opt,name=auth_role_add" json:"auth_role_add,omitempty"`
+	AuthRoleGrant          *AuthRoleGrantRequest          `protobuf:"bytes,16,opt,name=auth_role_grant" json:"auth_role_grant,omitempty"`
 	Authenticate           *AuthenticateRequest           `protobuf:"bytes,17,opt,name=authenticate" json:"authenticate,omitempty"`
 	Alarm                  *AlarmRequest                  `protobuf:"bytes,18,opt,name=alarm" json:"alarm,omitempty"`
 }
@@ -1185,39 +1185,35 @@ var (
 )
 
 var fileDescriptorRaftInternal = []byte{
-	// 534 bytes of a gzipped FileDescriptorProto
-	0x1f, 0x8b, 0x08, 0x00, 0x00, 0x09, 0x6e, 0x88, 0x02, 0xff, 0x74, 0x94, 0xdf, 0x6e, 0xd3, 0x30,
-	0x14, 0xc6, 0x69, 0xf7, 0x87, 0xcd, 0xfd, 0x8b, 0x07, 0xc8, 0xf4, 0x62, 0x8c, 0x22, 0x24, 0x04,
-	0x52, 0x41, 0xe3, 0x01, 0xa0, 0xb4, 0x15, 0x1a, 0x02, 0x69, 0x8a, 0xe0, 0x3a, 0x72, 0x93, 0xb3,
-	0xae, 0x22, 0x8d, 0x83, 0xe3, 0x94, 0xf1, 0x86, 0xbb, 0xe4, 0x11, 0x80, 0xb7, 0xe0, 0x0e, 0xfb,
-	0x24, 0x71, 0x9a, 0xcd, 0xbd, 0xa8, 0x94, 0x7c, 0xe7, 0x3b, 0xbf, 0xf3, 0xc5, 0x47, 0x2e, 0x39,
-	0x92, 0xfc, 0x42, 0xf9, 0xcb, 0x58, 0x81, 0x8c, 0x79, 0x34, 0x4a, 0xa4, 0x50, 0x82, 0xb6, 0x41,
-	0x05, 0x61, 0x0a, 0x72, 0x0d, 0x32, 0x99, 0x0f, 0xee, 0x2f, 0xc4, 0x42, 0x60, 0xe1, 0x95, 0x79,
-	0xca, 0x3d, 0x83, 0x7e, 0xe5, 0x29, 0x94, 0x43, 0x99, 0x04, 0xf9, 0xe3, 0xf0, 0xdf, 0x01, 0x39,
-	0x3a, 0x2b, 0x98, 0x9e, 0x1e, 0xe0, 0xc1, 0xf7, 0x0c, 0x52, 0x45, 0xbb, 0xa4, 0x79, 0x36, 0x65,
-	0x8d, 0x93, 0xc6, 0xf3, 0x5d, 0xaf, 0xb9, 0x9c, 0xd2, 0x67, 0xa4, 0xb9, 0x3e, 0x65, 0x4d, 0xfd,
-	0xde, 0x3a, 0x7d, 0x30, 0xda, 0x9c, 0x3a, 0x2a, 0x5a, 0x3c, 0x6d, 0xa0, 0xaf, 0xc9, 0x9e, 0xe4,
-	0xf1, 0x02, 0xd8, 0x0e, 0x3a, 0x07, 0x37, 0x9c, 0xa6, 0x54, 0xda, 0x73, 0x23, 0x7d, 0x41, 0x76,
-	0x92, 0x4c, 0xb1, 0x5d, 0xf4, 0xb3, 0xba, 0xff, 0x3c, 0x2b, 0xf3, 0x78, 0xc6, 0x44, 0x27, 0xa4,
-	0x1d, 0x42, 0x04, 0x0a, 0xfc, 0x7c, 0xc8, 0x1e, 0x36, 0x9d, 0xd4, 0x9b, 0xa6, 0xe8, 0xa8, 0x8d,
-	0x6a, 0x85, 0x95, 0x66, 0x06, 0xaa, 0xab, 0x98, 0xed, 0xbb, 0x06, 0x7e, 0xb9, 0x8a, 0xed, 0x40,
-	0x6d, 0xa2, 0x6f, 0x09, 0x09, 0xc4, 0x2a, 0xe1, 0x81, 0x5a, 0x8a, 0x98, 0xdd, 0xc5, 0x96, 0xc7,
-	0xf5, 0x96, 0x89, 0xad, 0x97, 0x9d, 0x1b, 0x2d, 0xf4, 0x1d, 0x69, 0x45, 0xc0, 0x53, 0xf0, 0x17,
-	0x3a, 0xb1, 0x62, 0x07, 0x2e, 0xc2, 0x27, 0x63, 0xf8, 0x60, 0xea, 0x96, 0x10, 0x59, 0xc9, 0x7c,
-	0x73, 0x4e, 0x90, 0xb0, 0x16, 0xdf, 0x80, 0x1d, 0xba, 0xbe, 0x19, 0x11, 0x1e, 0x1a, 0xec, 0x37,
-	0x47, 0x95, 0x66, 0x62, 0xf0, 0x4c, 0x5d, 0xfa, 0x10, 0xf3, 0x79, 0x04, 0x8c, 0xb8, 0x62, 0x8c,
-	0xb5, 0x61, 0x86, 0x75, 0x1b, 0x83, 0x5b, 0x89, 0x4e, 0x49, 0x07, 0x09, 0x99, 0xf6, 0xfb, 0x3c,
-	0x0c, 0x59, 0xcb, 0x95, 0xc3, 0x30, 0xbe, 0xea, 0xb7, 0x71, 0x18, 0xda, 0x1c, 0xbc, 0xd2, 0xe8,
-	0x67, 0xd2, 0xaf, 0x28, 0xf9, 0x52, 0x58, 0x1b, 0x41, 0x4f, 0xdd, 0xa0, 0x62, 0x99, 0x05, 0xab,
-	0xcb, 0x6b, 0x32, 0xbd, 0x20, 0x8f, 0x2a, 0x5c, 0x70, 0x69, 0xd6, 0xeb, 0x27, 0x3c, 0x4d, 0x7f,
-	0x08, 0x19, 0xb2, 0x0e, 0x72, 0x5f, 0xba, 0xb9, 0x13, 0x34, 0x9f, 0x17, 0xde, 0x92, 0xff, 0x90,
-	0x3b, 0xcb, 0xf4, 0x23, 0xe9, 0x55, 0x73, 0xf2, 0x4d, 0x76, 0x91, 0x3e, 0x74, 0xd3, 0x6b, 0xcb,
-	0xec, 0xf0, 0x4d, 0xd5, 0x1e, 0xa4, 0x14, 0x11, 0xe0, 0x41, 0xf6, 0xb6, 0x1d, 0xa4, 0xa7, 0x1d,
-	0x37, 0x0f, 0xb2, 0xd0, 0x6c, 0x22, 0xa4, 0xe4, 0x89, 0xfa, 0xdb, 0x12, 0x99, 0x9e, 0xdb, 0x89,
-	0xac, 0x4a, 0x67, 0xa4, 0x6d, 0x04, 0x88, 0xd5, 0x32, 0xe0, 0x7a, 0x21, 0xf7, 0x10, 0xf4, 0xe4,
-	0x36, 0xa8, 0x74, 0x94, 0x9c, 0x5a, 0x9b, 0xb9, 0xfa, 0x3c, 0xe2, 0x72, 0xc5, 0xa8, 0xeb, 0xea,
-	0x8f, 0x4d, 0xc9, 0x5e, 0x7d, 0x34, 0x0e, 0x7b, 0xa4, 0x33, 0x5b, 0x25, 0xea, 0xa7, 0x07, 0x69,
-	0x22, 0xe2, 0x14, 0xde, 0xf7, 0xaf, 0xff, 0x1c, 0xdf, 0xb9, 0xfe, 0x7b, 0xdc, 0xf8, 0xa5, 0x7f,
-	0xbf, 0xf5, 0x6f, 0xbe, 0x8f, 0xff, 0x52, 0x6f, 0xfe, 0x07, 0x00, 0x00, 0xff, 0xff, 0xee, 0xd7,
-	0xc5, 0x26, 0xfd, 0x04, 0x00, 0x00,
+	// 473 bytes of a gzipped FileDescriptorProto
+	0x1f, 0x8b, 0x08, 0x00, 0x00, 0x09, 0x6e, 0x88, 0x02, 0xff, 0x74, 0x93, 0xdf, 0x6e, 0xd3, 0x30,
+	0x14, 0xc6, 0x69, 0xf7, 0x8f, 0x9d, 0xb6, 0xb4, 0x74, 0x20, 0x99, 0x5e, 0x8c, 0xad, 0x08, 0x09,
+	0x84, 0x54, 0xa4, 0x0d, 0xc1, 0x05, 0xe2, 0xa2, 0x6c, 0x13, 0x9a, 0x84, 0xd0, 0x14, 0xc1, 0x75,
+	0xe4, 0x26, 0x87, 0xae, 0x22, 0xb5, 0x83, 0xe3, 0x94, 0xf1, 0x50, 0xbc, 0xc7, 0x2e, 0x79, 0x04,
+	0xe0, 0x49, 0xb0, 0x4f, 0xe2, 0x35, 0xe9, 0xdc, 0x8b, 0x48, 0xde, 0xce, 0xef, 0xf7, 0x9d, 0xe4,
+	0xab, 0x0c, 0x7b, 0x8a, 0x7f, 0xd5, 0xe1, 0x4c, 0x68, 0x54, 0x82, 0x27, 0xa3, 0x54, 0x49, 0x2d,
+	0xfb, 0x6d, 0xd4, 0x51, 0x9c, 0xa1, 0x5a, 0xa0, 0x4a, 0x27, 0x83, 0x07, 0x53, 0x39, 0x95, 0x34,
+	0x78, 0x69, 0x4f, 0x05, 0x33, 0xe8, 0x2d, 0x99, 0xf2, 0x3f, 0xbb, 0x2a, 0x8d, 0x8a, 0xe3, 0xf0,
+	0xd7, 0x0e, 0xec, 0x9d, 0x97, 0x99, 0x81, 0x59, 0x10, 0xe0, 0xf7, 0x1c, 0x33, 0xdd, 0x07, 0x68,
+	0x9e, 0x9f, 0xb2, 0xc6, 0x41, 0xe3, 0xd9, 0x66, 0xff, 0x10, 0x9a, 0x8b, 0x23, 0xd6, 0x34, 0xe7,
+	0xd6, 0xd1, 0xc3, 0x51, 0x75, 0xe3, 0xc8, 0xe1, 0xcf, 0x61, 0x4b, 0x71, 0x31, 0x45, 0xb6, 0x41,
+	0xd4, 0x60, 0x85, 0xb2, 0x23, 0x87, 0x3e, 0x85, 0x8d, 0x34, 0xd7, 0x6c, 0x93, 0x40, 0x56, 0x07,
+	0x2f, 0xf2, 0x9b, 0x17, 0x78, 0x0d, 0xed, 0x18, 0x13, 0xd4, 0x18, 0x16, 0xc1, 0x5b, 0xc4, 0x1f,
+	0xd4, 0xf9, 0x53, 0x22, 0x56, 0xe3, 0xf5, 0x95, 0x60, 0xdb, 0xbe, 0xf8, 0xcf, 0x57, 0xc2, 0x61,
+	0xc7, 0x00, 0x91, 0x9c, 0xa7, 0x3c, 0xd2, 0x33, 0x29, 0xd8, 0x0e, 0xd1, 0x8f, 0xeb, 0xf4, 0xc9,
+	0xcd, 0xdc, 0x49, 0xaf, 0xa0, 0x95, 0x20, 0xcf, 0x30, 0x9c, 0x9a, 0x77, 0xd2, 0xec, 0xae, 0xcf,
+	0xfa, 0x68, 0x81, 0x0f, 0x76, 0x5e, 0xf9, 0x92, 0xc2, 0x52, 0xb8, 0x90, 0xdf, 0x90, 0xed, 0xfa,
+	0xbe, 0x84, 0xb4, 0x80, 0x80, 0xca, 0x36, 0x9e, 0xeb, 0xcb, 0x10, 0x05, 0x9f, 0x24, 0xc8, 0xc0,
+	0xb7, 0x6d, 0x6c, 0x80, 0x33, 0x9a, 0x3b, 0xeb, 0x0d, 0x74, 0xc8, 0xca, 0x0d, 0x13, 0xf2, 0x38,
+	0x66, 0x2d, 0xdf, 0x3a, 0xeb, 0x7d, 0x31, 0x7f, 0x8d, 0xe3, 0xd8, 0x89, 0xef, 0xa0, 0xb7, 0x14,
+	0x8b, 0xea, 0x59, 0x9b, 0xdc, 0x27, 0x7e, 0xb7, 0x2c, 0xbf, 0xd4, 0x3f, 0xc1, 0xa3, 0xa5, 0x1e,
+	0x5d, 0xda, 0x9f, 0x24, 0x4c, 0x79, 0x96, 0xfd, 0x90, 0x2a, 0x66, 0x1d, 0xca, 0x79, 0xe1, 0xcf,
+	0x39, 0x21, 0xf8, 0xa2, 0x64, 0x5d, 0xde, 0x5b, 0xe8, 0x2e, 0xf3, 0x8a, 0xbe, 0xef, 0x51, 0xca,
+	0xd0, 0x9f, 0x52, 0xab, 0xdc, 0x95, 0xa0, 0x64, 0x82, 0x54, 0x42, 0x77, 0x5d, 0x09, 0x81, 0x21,
+	0x2a, 0x25, 0xb8, 0xad, 0x24, 0x16, 0x5b, 0x7b, 0xeb, 0xb6, 0x5a, 0x75, 0x65, 0x6b, 0xdb, 0xca,
+	0x28, 0xf4, 0x2c, 0xe2, 0xa6, 0xbd, 0xfb, 0x64, 0x1e, 0xde, 0x36, 0x1d, 0x51, 0xb9, 0x3d, 0x3c,
+	0xe1, 0x6a, 0xce, 0xfa, 0xbe, 0xdb, 0x33, 0xb6, 0xa3, 0x12, 0x1d, 0x76, 0xa1, 0x73, 0x36, 0x4f,
+	0xf5, 0xcf, 0x00, 0xb3, 0x54, 0x8a, 0x0c, 0xdf, 0xf7, 0xae, 0xff, 0xee, 0xdf, 0xb9, 0xfe, 0xb7,
+	0xdf, 0xf8, 0x6d, 0x9e, 0x3f, 0xe6, 0x99, 0x6c, 0xd3, 0xcd, 0x3e, 0xfe, 0x1f, 0x00, 0x00, 0xff,
+	0xff, 0x8e, 0xdf, 0x9b, 0x7d, 0x31, 0x04, 0x00, 0x00,
 }
diff --git a/etcdserver/etcdserverpb/rpc.pb.go b/etcdserver/etcdserverpb/rpc.pb.go
index d2a9924..31cfb67 100644
--- a/etcdserver/etcdserverpb/rpc.pb.go
+++ b/etcdserver/etcdserverpb/rpc.pb.go
@@ -16,7 +16,7 @@ import (
 	io "io"
 )
 
-import storagepb "github.com/coreos/etcd/storage/storagepb"
+import mvccpb "github.com/coreos/etcd/mvcc/mvccpb"
 
 import (
 	context "golang.org/x/net/context"
@@ -182,13 +182,13 @@ func (AlarmRequest_AlarmAction) EnumDescriptor() ([]byte, []int) {
 
 type ResponseHeader struct {
 	// cluster_id is the ID of the cluster which sent the response.
-	ClusterId uint64 `protobuf:"varint,1,opt,name=cluster_id,json=clusterId,proto3" json:"cluster_id,omitempty"`
+	ClusterId uint64 `protobuf:"varint,1,opt,name=cluster_id,proto3" json:"cluster_id,omitempty"`
 	// member_id is the ID of the member which sent the response.
-	MemberId uint64 `protobuf:"varint,2,opt,name=member_id,json=memberId,proto3" json:"member_id,omitempty"`
+	MemberId uint64 `protobuf:"varint,2,opt,name=member_id,proto3" json:"member_id,omitempty"`
 	// revision is the key-value store revision when the request was applied.
 	Revision int64 `protobuf:"varint,3,opt,name=revision,proto3" json:"revision,omitempty"`
 	// raft_term is the raft term when the request was applied.
-	RaftTerm uint64 `protobuf:"varint,4,opt,name=raft_term,json=raftTerm,proto3" json:"raft_term,omitempty"`
+	RaftTerm uint64 `protobuf:"varint,4,opt,name=raft_term,proto3" json:"raft_term,omitempty"`
 }
 
 func (m *ResponseHeader) Reset()                    { *m = ResponseHeader{} }
@@ -201,7 +201,7 @@ type RangeRequest struct {
 	Key []byte `protobuf:"bytes,1,opt,name=key,proto3" json:"key,omitempty"`
 	// range_end is the upper bound on the requested range [key, range_end).
 	// If range_end is '\0', the range is all keys >= key.
-	RangeEnd []byte `protobuf:"bytes,2,opt,name=range_end,json=rangeEnd,proto3" json:"range_end,omitempty"`
+	RangeEnd []byte `protobuf:"bytes,2,opt,name=range_end,proto3" json:"range_end,omitempty"`
 	// limit is a limit on the number of keys returned for the request.
 	Limit int64 `protobuf:"varint,3,opt,name=limit,proto3" json:"limit,omitempty"`
 	// revision is the point-in-time of the key-value store to use for the range.
@@ -209,9 +209,9 @@ type RangeRequest struct {
 	// If the revision has been compacted, ErrCompaction is returned as a response.
 	Revision int64 `protobuf:"varint,4,opt,name=revision,proto3" json:"revision,omitempty"`
 	// sort_order is the order for returned sorted results.
-	SortOrder RangeRequest_SortOrder `protobuf:"varint,5,opt,name=sort_order,json=sortOrder,proto3,enum=etcdserverpb.RangeRequest_SortOrder" json:"sort_order,omitempty"`
+	SortOrder RangeRequest_SortOrder `protobuf:"varint,5,opt,name=sort_order,proto3,enum=etcdserverpb.RangeRequest_SortOrder" json:"sort_order,omitempty"`
 	// sort_target is the key-value field to use for sorting.
-	SortTarget RangeRequest_SortTarget `protobuf:"varint,6,opt,name=sort_target,json=sortTarget,proto3,enum=etcdserverpb.RangeRequest_SortTarget" json:"sort_target,omitempty"`
+	SortTarget RangeRequest_SortTarget `protobuf:"varint,6,opt,name=sort_target,proto3,enum=etcdserverpb.RangeRequest_SortTarget" json:"sort_target,omitempty"`
 	// serializable sets the range request to use serializable member-local reads.
 	// Range requests are linearizable by default; linearizable requests have higher
 	// latency and lower throughput than serializable requests but reflect the current
@@ -229,7 +229,7 @@ func (*RangeRequest) Descriptor() ([]byte, []int) { return fileDescriptorRpc, []
 type RangeResponse struct {
 	Header *ResponseHeader `protobuf:"bytes,1,opt,name=header" json:"header,omitempty"`
 	// kvs is the list of key-value pairs matched by the range request.
-	Kvs []*storagepb.KeyValue `protobuf:"bytes,2,rep,name=kvs" json:"kvs,omitempty"`
+	Kvs []*mvccpb.KeyValue `protobuf:"bytes,2,rep,name=kvs" json:"kvs,omitempty"`
 	// more indicates if there are more keys to return in the requested range.
 	More bool `protobuf:"varint,3,opt,name=more,proto3" json:"more,omitempty"`
 }
@@ -246,7 +246,7 @@ func (m *RangeResponse) GetHeader() *ResponseHeader {
 	return nil
 }
 
-func (m *RangeResponse) GetKvs() []*storagepb.KeyValue {
+func (m *RangeResponse) GetKvs() []*mvccpb.KeyValue {
 	if m != nil {
 		return m.Kvs
 	}
@@ -290,7 +290,7 @@ type DeleteRangeRequest struct {
 	// range_end is the key following the last key to delete for the range [key, range_end).
 	// If range_end is not given, the range is defined to contain only the key argument.
 	// If range_end is '\0', the range is all keys greater than or equal to the key argument.
-	RangeEnd []byte `protobuf:"bytes,2,opt,name=range_end,json=rangeEnd,proto3" json:"range_end,omitempty"`
+	RangeEnd []byte `protobuf:"bytes,2,opt,name=range_end,proto3" json:"range_end,omitempty"`
 }
 
 func (m *DeleteRangeRequest) Reset()                    { *m = DeleteRangeRequest{} }
@@ -338,13 +338,13 @@ type isRequestUnion_Request interface {
 }
 
 type RequestUnion_RequestRange struct {
-	RequestRange *RangeRequest `protobuf:"bytes,1,opt,name=request_range,json=requestRange,oneof"`
+	RequestRange *RangeRequest `protobuf:"bytes,1,opt,name=request_range,oneof"`
 }
 type RequestUnion_RequestPut struct {
-	RequestPut *PutRequest `protobuf:"bytes,2,opt,name=request_put,json=requestPut,oneof"`
+	RequestPut *PutRequest `protobuf:"bytes,2,opt,name=request_put,oneof"`
 }
 type RequestUnion_RequestDeleteRange struct {
-	RequestDeleteRange *DeleteRangeRequest `protobuf:"bytes,3,opt,name=request_delete_range,json=requestDeleteRange,oneof"`
+	RequestDeleteRange *DeleteRangeRequest `protobuf:"bytes,3,opt,name=request_delete_range,oneof"`
 }
 
 func (*RequestUnion_RequestRange) isRequestUnion_Request()       {}
@@ -494,13 +494,13 @@ type isResponseUnion_Response interface {
 }
 
 type ResponseUnion_ResponseRange struct {
-	ResponseRange *RangeResponse `protobuf:"bytes,1,opt,name=response_range,json=responseRange,oneof"`
+	ResponseRange *RangeResponse `protobuf:"bytes,1,opt,name=response_range,oneof"`
 }
 type ResponseUnion_ResponsePut struct {
-	ResponsePut *PutResponse `protobuf:"bytes,2,opt,name=response_put,json=responsePut,oneof"`
+	ResponsePut *PutResponse `protobuf:"bytes,2,opt,name=response_put,oneof"`
 }
 type ResponseUnion_ResponseDeleteRange struct {
-	ResponseDeleteRange *DeleteRangeResponse `protobuf:"bytes,3,opt,name=response_delete_range,json=responseDeleteRange,oneof"`
+	ResponseDeleteRange *DeleteRangeResponse `protobuf:"bytes,3,opt,name=response_delete_range,oneof"`
 }
 
 func (*ResponseUnion_ResponseRange) isResponseUnion_Response()       {}
@@ -658,10 +658,10 @@ type Compare_Version struct {
 	Version int64 `protobuf:"varint,4,opt,name=version,proto3,oneof"`
 }
 type Compare_CreateRevision struct {
-	CreateRevision int64 `protobuf:"varint,5,opt,name=create_revision,json=createRevision,proto3,oneof"`
+	CreateRevision int64 `protobuf:"varint,5,opt,name=create_revision,proto3,oneof"`
 }
 type Compare_ModRevision struct {
-	ModRevision int64 `protobuf:"varint,6,opt,name=mod_revision,json=modRevision,proto3,oneof"`
+	ModRevision int64 `protobuf:"varint,6,opt,name=mod_revision,proto3,oneof"`
 }
 type Compare_Value struct {
 	Value []byte `protobuf:"bytes,7,opt,name=value,proto3,oneof"`
@@ -953,7 +953,7 @@ type SnapshotResponse struct {
 	// stream indicates the point in time of the snapshot.
 	Header *ResponseHeader `protobuf:"bytes,1,opt,name=header" json:"header,omitempty"`
 	// remaining_bytes is the number of blob bytes to be sent after this message
-	RemainingBytes uint64 `protobuf:"varint,2,opt,name=remaining_bytes,json=remainingBytes,proto3" json:"remaining_bytes,omitempty"`
+	RemainingBytes uint64 `protobuf:"varint,2,opt,name=remaining_bytes,proto3" json:"remaining_bytes,omitempty"`
 	// blob contains the next chunk of the snapshot in the snapshot stream.
 	Blob []byte `protobuf:"bytes,3,opt,name=blob,proto3" json:"blob,omitempty"`
 }
@@ -991,10 +991,10 @@ type isWatchRequest_RequestUnion interface {
 }
 
 type WatchRequest_CreateRequest struct {
-	CreateRequest *WatchCreateRequest `protobuf:"bytes,1,opt,name=create_request,json=createRequest,oneof"`
+	CreateRequest *WatchCreateRequest `protobuf:"bytes,1,opt,name=create_request,oneof"`
 }
 type WatchRequest_CancelRequest struct {
-	CancelRequest *WatchCancelRequest `protobuf:"bytes,2,opt,name=cancel_request,json=cancelRequest,oneof"`
+	CancelRequest *WatchCancelRequest `protobuf:"bytes,2,opt,name=cancel_request,oneof"`
 }
 
 func (*WatchRequest_CreateRequest) isWatchRequest_RequestUnion() {}
@@ -1101,14 +1101,14 @@ type WatchCreateRequest struct {
 	// range_end is the end of the range [key, range_end) to watch. If range_end is not given,
 	// only the key argument is watched. If range_end is equal to '\0', all keys greater than
 	// or equal to the key argument are watched.
-	RangeEnd []byte `protobuf:"bytes,2,opt,name=range_end,json=rangeEnd,proto3" json:"range_end,omitempty"`
+	RangeEnd []byte `protobuf:"bytes,2,opt,name=range_end,proto3" json:"range_end,omitempty"`
 	// start_revision is an optional revision to watch from (inclusive). No start_revision is "now".
-	StartRevision int64 `protobuf:"varint,3,opt,name=start_revision,json=startRevision,proto3" json:"start_revision,omitempty"`
+	StartRevision int64 `protobuf:"varint,3,opt,name=start_revision,proto3" json:"start_revision,omitempty"`
 	// progress_notify is set so that the etcd server will periodically send a WatchResponse with
 	// no events to the new watcher if there are no recent events. It is useful when clients
 	// wish to recover a disconnected watcher starting from a recent known revision.
 	// The etcd server may decide how often it will send notifications based on current load.
-	ProgressNotify bool `protobuf:"varint,4,opt,name=progress_notify,json=progressNotify,proto3" json:"progress_notify,omitempty"`
+	ProgressNotify bool `protobuf:"varint,4,opt,name=progress_notify,proto3" json:"progress_notify,omitempty"`
 }
 
 func (m *WatchCreateRequest) Reset()                    { *m = WatchCreateRequest{} }
@@ -1118,7 +1118,7 @@ func (*WatchCreateRequest) Descriptor() ([]byte, []int) { return fileDescriptorR
 
 type WatchCancelRequest struct {
 	// watch_id is the watcher id to cancel so that no more events are transmitted.
-	WatchId int64 `protobuf:"varint,1,opt,name=watch_id,json=watchId,proto3" json:"watch_id,omitempty"`
+	WatchId int64 `protobuf:"varint,1,opt,name=watch_id,proto3" json:"watch_id,omitempty"`
 }
 
 func (m *WatchCancelRequest) Reset()                    { *m = WatchCancelRequest{} }
@@ -1129,7 +1129,7 @@ func (*WatchCancelRequest) Descriptor() ([]byte, []int) { return fileDescriptorR
 type WatchResponse struct {
 	Header *ResponseHeader `protobuf:"bytes,1,opt,name=header" json:"header,omitempty"`
 	// watch_id is the ID of the watcher that corresponds to the response.
-	WatchId int64 `protobuf:"varint,2,opt,name=watch_id,json=watchId,proto3" json:"watch_id,omitempty"`
+	WatchId int64 `protobuf:"varint,2,opt,name=watch_id,proto3" json:"watch_id,omitempty"`
 	// created is set to true if the response is for a create watch request.
 	// The client should record the watch_id and expect to receive events for
 	// the created watcher from the same stream.
@@ -1146,8 +1146,8 @@ type WatchResponse struct {
 	//
 	// The client should treat the watcher as canceled and should not try to create any
 	// watcher with the same start_revision again.
-	CompactRevision int64              `protobuf:"varint,5,opt,name=compact_revision,json=compactRevision,proto3" json:"compact_revision,omitempty"`
-	Events          []*storagepb.Event `protobuf:"bytes,11,rep,name=events" json:"events,omitempty"`
+	CompactRevision int64           `protobuf:"varint,5,opt,name=compact_revision,proto3" json:"compact_revision,omitempty"`
+	Events          []*mvccpb.Event `protobuf:"bytes,11,rep,name=events" json:"events,omitempty"`
 }
 
 func (m *WatchResponse) Reset()                    { *m = WatchResponse{} }
@@ -1162,7 +1162,7 @@ func (m *WatchResponse) GetHeader() *ResponseHeader {
 	return nil
 }
 
-func (m *WatchResponse) GetEvents() []*storagepb.Event {
+func (m *WatchResponse) GetEvents() []*mvccpb.Event {
 	if m != nil {
 		return m.Events
 	}
@@ -1171,9 +1171,9 @@ func (m *WatchResponse) GetEvents() []*storagepb.Event {
 
 type LeaseGrantRequest struct {
 	// TTL is the advisory time-to-live in seconds.
-	TTL int64 `protobuf:"varint,1,opt,name=TTL,json=tTL,proto3" json:"TTL,omitempty"`
+	TTL int64 `protobuf:"varint,1,opt,name=TTL,proto3" json:"TTL,omitempty"`
 	// ID is the requested ID for the lease. If ID is set to 0, the lessor chooses an ID.
-	ID int64 `protobuf:"varint,2,opt,name=ID,json=iD,proto3" json:"ID,omitempty"`
+	ID int64 `protobuf:"varint,2,opt,name=ID,proto3" json:"ID,omitempty"`
 }
 
 func (m *LeaseGrantRequest) Reset()                    { *m = LeaseGrantRequest{} }
@@ -1184,9 +1184,9 @@ func (*LeaseGrantRequest) Descriptor() ([]byte, []int) { return fileDescriptorRp
 type LeaseGrantResponse struct {
 	Header *ResponseHeader `protobuf:"bytes,1,opt,name=header" json:"header,omitempty"`
 	// ID is the lease ID for the granted lease.
-	ID int64 `protobuf:"varint,2,opt,name=ID,json=iD,proto3" json:"ID,omitempty"`
+	ID int64 `protobuf:"varint,2,opt,name=ID,proto3" json:"ID,omitempty"`
 	// TTL is the server chosen lease time-to-live in seconds.
-	TTL   int64  `protobuf:"varint,3,opt,name=TTL,json=tTL,proto3" json:"TTL,omitempty"`
+	TTL   int64  `protobuf:"varint,3,opt,name=TTL,proto3" json:"TTL,omitempty"`
 	Error string `protobuf:"bytes,4,opt,name=error,proto3" json:"error,omitempty"`
 }
 
@@ -1204,7 +1204,7 @@ func (m *LeaseGrantResponse) GetHeader() *ResponseHeader {
 
 type LeaseRevokeRequest struct {
 	// ID is the lease ID to revoke. When the ID is revoked, all associated keys will be deleted.
-	ID int64 `protobuf:"varint,1,opt,name=ID,json=iD,proto3" json:"ID,omitempty"`
+	ID int64 `protobuf:"varint,1,opt,name=ID,proto3" json:"ID,omitempty"`
 }
 
 func (m *LeaseRevokeRequest) Reset()                    { *m = LeaseRevokeRequest{} }
@@ -1230,7 +1230,7 @@ func (m *LeaseRevokeResponse) GetHeader() *ResponseHeader {
 
 type LeaseKeepAliveRequest struct {
 	// ID is the lease ID for the lease to keep alive.
-	ID int64 `protobuf:"varint,1,opt,name=ID,json=iD,proto3" json:"ID,omitempty"`
+	ID int64 `protobuf:"varint,1,opt,name=ID,proto3" json:"ID,omitempty"`
 }
 
 func (m *LeaseKeepAliveRequest) Reset()                    { *m = LeaseKeepAliveRequest{} }
@@ -1241,9 +1241,9 @@ func (*LeaseKeepAliveRequest) Descriptor() ([]byte, []int) { return fileDescript
 type LeaseKeepAliveResponse struct {
 	Header *ResponseHeader `protobuf:"bytes,1,opt,name=header" json:"header,omitempty"`
 	// ID is the lease ID from the keep alive request.
-	ID int64 `protobuf:"varint,2,opt,name=ID,json=iD,proto3" json:"ID,omitempty"`
+	ID int64 `protobuf:"varint,2,opt,name=ID,proto3" json:"ID,omitempty"`
 	// TTL is the new time-to-live for the lease.
-	TTL int64 `protobuf:"varint,3,opt,name=TTL,json=tTL,proto3" json:"TTL,omitempty"`
+	TTL int64 `protobuf:"varint,3,opt,name=TTL,proto3" json:"TTL,omitempty"`
 }
 
 func (m *LeaseKeepAliveResponse) Reset()                    { *m = LeaseKeepAliveResponse{} }
@@ -1260,7 +1260,7 @@ func (m *LeaseKeepAliveResponse) GetHeader() *ResponseHeader {
 
 type Member struct {
 	// ID is the member ID for this member.
-	ID uint64 `protobuf:"varint,1,opt,name=ID,json=iD,proto3" json:"ID,omitempty"`
+	ID uint64 `protobuf:"varint,1,opt,name=ID,proto3" json:"ID,omitempty"`
 	// name is the human-readable name of the member. If the member is not started, the name will be an empty string.
 	Name string `protobuf:"bytes,2,opt,name=name,proto3" json:"name,omitempty"`
 	// peerURLs is the list of URLs the member exposes to the cluster for communication.
@@ -1311,7 +1311,7 @@ func (m *MemberAddResponse) GetMember() *Member {
 
 type MemberRemoveRequest struct {
 	// ID is the member ID of the member to remove.
-	ID uint64 `protobuf:"varint,1,opt,name=ID,json=iD,proto3" json:"ID,omitempty"`
+	ID uint64 `protobuf:"varint,1,opt,name=ID,proto3" json:"ID,omitempty"`
 }
 
 func (m *MemberRemoveRequest) Reset()                    { *m = MemberRemoveRequest{} }
@@ -1337,7 +1337,7 @@ func (m *MemberRemoveResponse) GetHeader() *ResponseHeader {
 
 type MemberUpdateRequest struct {
 	// ID is the member ID of the member to update.
-	ID uint64 `protobuf:"varint,1,opt,name=ID,json=iD,proto3" json:"ID,omitempty"`
+	ID uint64 `protobuf:"varint,1,opt,name=ID,proto3" json:"ID,omitempty"`
 	// peerURLs is the new list of URLs the member will use to communicate with the cluster.
 	PeerURLs []string `protobuf:"bytes,2,rep,name=peerURLs" json:"peerURLs,omitempty"`
 }
@@ -7224,7 +7224,7 @@ func (m *RangeResponse) Unmarshal(data []byte) error {
 			if postIndex > l {
 				return io.ErrUnexpectedEOF
 			}
-			m.Kvs = append(m.Kvs, &storagepb.KeyValue{})
+			m.Kvs = append(m.Kvs, &mvccpb.KeyValue{})
 			if err := m.Kvs[len(m.Kvs)-1].Unmarshal(data[iNdEx:postIndex]); err != nil {
 				return err
 			}
@@ -9483,7 +9483,7 @@ func (m *WatchResponse) Unmarshal(data []byte) error {
 			if postIndex > l {
 				return io.ErrUnexpectedEOF
 			}
-			m.Events = append(m.Events, &storagepb.Event{})
+			m.Events = append(m.Events, &mvccpb.Event{})
 			if err := m.Events[len(m.Events)-1].Unmarshal(data[iNdEx:postIndex]); err != nil {
 				return err
 			}
@@ -13966,167 +13966,152 @@ var (
 )
 
 var fileDescriptorRpc = []byte{
-	// 2577 bytes of a gzipped FileDescriptorProto
-	0x1f, 0x8b, 0x08, 0x00, 0x00, 0x09, 0x6e, 0x88, 0x02, 0xff, 0xb4, 0x5a, 0xcd, 0x72, 0x1b, 0xc7,
-	0x11, 0x26, 0x7e, 0x08, 0x10, 0x0d, 0x10, 0xa2, 0x86, 0x94, 0x4c, 0x43, 0xb6, 0x24, 0xaf, 0x24,
-	0x5b, 0x89, 0x1d, 0x2a, 0x66, 0x9c, 0x43, 0x2a, 0x2e, 0xa5, 0x40, 0x02, 0x96, 0x68, 0x52, 0xa4,
-	0xbc, 0x04, 0xa9, 0xf8, 0xc4, 0x5a, 0x02, 0x23, 0x12, 0x25, 0xfc, 0x79, 0x77, 0x41, 0x89, 0x3a,
-	0xa5, 0x52, 0x95, 0x27, 0x70, 0x4e, 0xa9, 0xbc, 0x40, 0x1e, 0x20, 0xef, 0x90, 0xca, 0x25, 0x79,
-	0x82, 0x24, 0x95, 0x63, 0x72, 0xc8, 0x3d, 0xb9, 0xa4, 0xe7, 0x77, 0x67, 0x07, 0xbb, 0x94, 0x9c,
-	0x65, 0x0e, 0x14, 0x77, 0x7a, 0xba, 0xbf, 0xe9, 0xee, 0xe9, 0xe9, 0xe9, 0x1e, 0x0a, 0x2a, 0xfe,
-	0xa4, 0xbb, 0x36, 0xf1, 0xc7, 0xe1, 0x98, 0xd4, 0x68, 0xd8, 0xed, 0x05, 0xd4, 0x3f, 0xa3, 0xfe,
-	0xe4, 0xb8, 0xb1, 0x72, 0x32, 0x3e, 0x19, 0xf3, 0x89, 0x07, 0xec, 0x4b, 0xf0, 0x34, 0x6e, 0x31,
-	0x9e, 0x07, 0x41, 0x38, 0xf6, 0xbd, 0x13, 0xaa, 0x7e, 0x4f, 0x8e, 0x1f, 0xbc, 0x38, 0x93, 0x0c,
-	0x37, 0x38, 0x83, 0x37, 0x0d, 0x4f, 0xf9, 0x3f, 0x38, 0xc5, 0x7e, 0x89, 0x49, 0xe7, 0x57, 0x39,
-	0xa8, 0xbb, 0x34, 0x98, 0x8c, 0x47, 0x01, 0x7d, 0x4c, 0xbd, 0x1e, 0xf5, 0xc9, 0xfb, 0x00, 0xdd,
-	0xc1, 0x34, 0x08, 0xa9, 0x7f, 0xd4, 0xef, 0xad, 0xe6, 0x6e, 0xe7, 0xee, 0x17, 0xdd, 0x8a, 0xa4,
-	0x6c, 0xf5, 0xc8, 0x0d, 0xa8, 0x0c, 0xe9, 0xf0, 0x58, 0xcc, 0xe6, 0xf9, 0xec, 0x82, 0x20, 0xe0,
-	0x64, 0x03, 0x16, 0x7c, 0x7a, 0xd6, 0x0f, 0xfa, 0xe3, 0xd1, 0x6a, 0x01, 0xe7, 0x0a, 0xae, 0x1e,
-	0x33, 0x41, 0xdf, 0x7b, 0x1e, 0x1e, 0x21, 0xcc, 0x70, 0xb5, 0x28, 0x04, 0x19, 0xa1, 0x83, 0x63,
-	0xe7, 0xd7, 0x05, 0xa8, 0xb9, 0xde, 0xe8, 0x84, 0xba, 0xf4, 0x9b, 0x29, 0x0d, 0x42, 0xb2, 0x04,
-	0x85, 0x17, 0xf4, 0x9c, 0x2f, 0x5f, 0x73, 0xd9, 0xa7, 0x90, 0x47, 0x8e, 0x23, 0x3a, 0x12, 0x0b,
-	0xd7, 0x98, 0x3c, 0x12, 0xda, 0xa3, 0x1e, 0x59, 0x81, 0xf9, 0x41, 0x7f, 0xd8, 0x0f, 0xe5, 0xaa,
-	0x62, 0x10, 0x53, 0xa7, 0x68, 0xa9, 0xb3, 0x09, 0x10, 0x8c, 0xfd, 0xf0, 0x68, 0xec, 0xa3, 0xd1,
-	0xab, 0xf3, 0x38, 0x5b, 0x5f, 0xbf, 0xbb, 0x66, 0x3a, 0x7c, 0xcd, 0x54, 0x68, 0x6d, 0x1f, 0x99,
-	0xf7, 0x18, 0xaf, 0x5b, 0x09, 0xd4, 0x27, 0xf9, 0x02, 0xaa, 0x1c, 0x24, 0xf4, 0xfc, 0x13, 0x1a,
-	0xae, 0x96, 0x38, 0xca, 0xbd, 0x37, 0xa0, 0x74, 0x38, 0xb3, 0xcb, 0x97, 0x17, 0xdf, 0xc4, 0x81,
-	0x1a, 0xf2, 0xf7, 0xbd, 0x41, 0xff, 0xb5, 0x77, 0x3c, 0xa0, 0xab, 0x65, 0x04, 0x5a, 0x70, 0x63,
-	0x34, 0x67, 0x0d, 0x2a, 0x5a, 0x07, 0xb2, 0x00, 0xc5, 0xdd, 0xbd, 0xdd, 0xf6, 0xd2, 0x1c, 0x01,
-	0x28, 0x35, 0xf7, 0x37, 0xdb, 0xbb, 0xad, 0xa5, 0x1c, 0xa9, 0x42, 0xb9, 0xd5, 0x16, 0x83, 0xbc,
-	0xb3, 0x01, 0x10, 0xad, 0x46, 0xca, 0x50, 0xd8, 0x6e, 0x7f, 0x8d, 0xfc, 0xc8, 0x73, 0xd8, 0x76,
-	0xf7, 0xb7, 0xf6, 0x76, 0x51, 0x00, 0x85, 0x37, 0xdd, 0x76, 0xb3, 0xd3, 0x5e, 0xca, 0x33, 0x8e,
-	0x27, 0x7b, 0xad, 0xa5, 0x02, 0xa9, 0xc0, 0xfc, 0x61, 0x73, 0xe7, 0xa0, 0xbd, 0x54, 0x74, 0x7e,
-	0x91, 0x83, 0x45, 0xa9, 0xbf, 0x88, 0x11, 0xf2, 0x19, 0x94, 0x4e, 0x79, 0x9c, 0xf0, 0xad, 0xa9,
-	0xae, 0xbf, 0x67, 0x19, 0x1b, 0x8b, 0x25, 0x57, 0xf2, 0x92, 0x7b, 0xb8, 0x9b, 0x67, 0x01, 0xee,
-	0x5a, 0x01, 0x45, 0x96, 0xd7, 0x74, 0x94, 0xae, 0x6d, 0xd3, 0xf3, 0x43, 0x6f, 0x30, 0xa5, 0x2e,
-	0x9b, 0x27, 0x04, 0x8a, 0xc3, 0xb1, 0x4f, 0xf9, 0x26, 0x2e, 0xb8, 0xfc, 0xdb, 0xf9, 0x12, 0xe0,
-	0xe9, 0x34, 0x4c, 0x0f, 0x0b, 0xdc, 0xf9, 0x33, 0x86, 0x20, 0x43, 0x42, 0x0c, 0x78, 0x3c, 0x50,
-	0x2f, 0xa0, 0x3a, 0x1e, 0xd8, 0xc0, 0xd9, 0x84, 0x2a, 0xc7, 0xca, 0x62, 0x0b, 0x82, 0x90, 0x16,
-	0x1d, 0xd0, 0x90, 0x66, 0x88, 0x57, 0x87, 0xc2, 0x72, 0x0c, 0x24, 0x93, 0x77, 0x57, 0xa1, 0xdc,
-	0xe3, 0x60, 0x62, 0x9d, 0x82, 0xab, 0x86, 0xce, 0xbf, 0x72, 0x78, 0xac, 0x84, 0x86, 0x07, 0x23,
-	0x16, 0xf5, 0x4d, 0x58, 0xf4, 0xc5, 0xf8, 0x88, 0xeb, 0x22, 0xd7, 0x69, 0xa4, 0x87, 0xec, 0xe3,
-	0x39, 0xb7, 0x26, 0x45, 0x38, 0x99, 0xfc, 0x14, 0xaa, 0x0a, 0x62, 0x32, 0x0d, 0xf9, 0x8a, 0xd5,
-	0xf5, 0xd5, 0x38, 0x40, 0xb4, 0x63, 0x28, 0x0e, 0x92, 0x1d, 0x89, 0xa4, 0x03, 0x2b, 0x4a, 0x58,
-	0xe8, 0x28, 0xd5, 0x28, 0x70, 0x94, 0xdb, 0x71, 0x94, 0x59, 0x37, 0x23, 0x1a, 0x91, 0xf2, 0xc6,
-	0xe4, 0x46, 0x05, 0xca, 0x92, 0xea, 0xfc, 0x9b, 0x45, 0xac, 0x74, 0x93, 0x30, 0xb9, 0x05, 0x75,
-	0x5f, 0x12, 0x62, 0x36, 0xdf, 0x48, 0xb4, 0x59, 0x3a, 0x78, 0xce, 0x5d, 0x54, 0x42, 0xc2, 0xea,
-	0x87, 0x50, 0xd3, 0x28, 0x91, 0xd9, 0xef, 0x26, 0x98, 0xad, 0x11, 0xaa, 0x4a, 0x80, 0x19, 0xfe,
-	0x0c, 0xae, 0x69, 0xf9, 0x04, 0xcb, 0x3f, 0xb8, 0xc0, 0x72, 0x0d, 0xb8, 0xac, 0x10, 0x4c, 0xdb,
-	0x81, 0xe5, 0x38, 0x41, 0x76, 0x7e, 0x53, 0x80, 0xf2, 0xe6, 0x78, 0x38, 0xf1, 0x7c, 0xb6, 0x4d,
-	0x25, 0xa4, 0x4f, 0x07, 0x21, 0x37, 0xb7, 0xbe, 0x7e, 0x27, 0xbe, 0x82, 0x64, 0x53, 0xbf, 0x5d,
-	0xce, 0xea, 0x4a, 0x11, 0x26, 0x2c, 0x53, 0x5a, 0xfe, 0x2d, 0x84, 0x65, 0x42, 0x93, 0x22, 0xea,
-	0x28, 0x14, 0xa2, 0xa3, 0xd0, 0x80, 0x32, 0x0a, 0x46, 0x69, 0x18, 0x6d, 0x51, 0x04, 0xf2, 0x3d,
-	0xb8, 0xd2, 0xf5, 0xa9, 0xc7, 0xfc, 0xa1, 0x52, 0xf5, 0xbc, 0xe4, 0xa9, 0x8b, 0x09, 0x57, 0xa5,
-	0xec, 0x3b, 0x50, 0x1b, 0x8e, 0x7b, 0x11, 0x5f, 0x49, 0xf2, 0x55, 0x91, 0xaa, 0x99, 0xae, 0xab,
-	0x7c, 0xc0, 0x72, 0x68, 0x0d, 0x67, 0xc5, 0xd0, 0xf9, 0x14, 0x16, 0x63, 0xb6, 0xb2, 0x34, 0xd7,
-	0xfe, 0xea, 0xa0, 0xb9, 0x23, 0x72, 0xe2, 0x23, 0x9e, 0x06, 0x5d, 0xcc, 0x89, 0x98, 0x5a, 0x77,
-	0xda, 0xfb, 0xfb, 0x98, 0x41, 0x3f, 0xd7, 0x22, 0x32, 0x89, 0x1a, 0xb9, 0x73, 0xce, 0xc8, 0x9d,
-	0x39, 0x95, 0x3b, 0xf3, 0x51, 0xee, 0x2c, 0x6c, 0xd4, 0xa1, 0x26, 0x1c, 0x72, 0x34, 0x65, 0x71,
-	0xe8, 0xfc, 0x2e, 0x07, 0xd0, 0x79, 0x35, 0x52, 0x09, 0xe3, 0x01, 0x94, 0xbb, 0x02, 0x1c, 0x37,
-	0x88, 0xa5, 0xc5, 0x6b, 0x89, 0x3e, 0x76, 0x15, 0x17, 0xe6, 0x86, 0x72, 0x30, 0xed, 0x76, 0x69,
-	0xa0, 0xf2, 0xa8, 0x7d, 0x68, 0x8d, 0x73, 0xee, 0x2a, 0x56, 0x26, 0xf5, 0xdc, 0xeb, 0x0f, 0xa6,
-	0x3c, 0xab, 0xbe, 0x51, 0x4a, 0xb2, 0x3a, 0xbf, 0xcd, 0x41, 0x95, 0xeb, 0x9a, 0x29, 0x2f, 0xbd,
-	0x07, 0x15, 0xae, 0x06, 0xed, 0xc9, 0xcc, 0xb4, 0xe0, 0x46, 0x04, 0xf2, 0x13, 0xcc, 0x8f, 0x52,
-	0x2e, 0x90, 0xba, 0xdd, 0x48, 0x86, 0x15, 0xca, 0x45, 0xdc, 0xce, 0x36, 0x5c, 0xe5, 0xee, 0xe9,
-	0x86, 0x6c, 0x42, 0x3a, 0xd4, 0xbc, 0xec, 0x73, 0xd6, 0x65, 0x8f, 0x73, 0x93, 0xd3, 0xf3, 0xa0,
-	0xdf, 0xf5, 0x06, 0x52, 0x11, 0x3d, 0xc6, 0x0b, 0x86, 0x98, 0x60, 0x99, 0xee, 0x86, 0x45, 0xa8,
-	0x3e, 0xf6, 0x82, 0x53, 0xa9, 0x92, 0xf3, 0x73, 0xa8, 0x89, 0x61, 0x26, 0x37, 0xe2, 0xad, 0x78,
-	0x8a, 0x28, 0x5c, 0xf1, 0x45, 0x97, 0x7f, 0x3b, 0x57, 0xe1, 0xca, 0xfe, 0xc8, 0x9b, 0x04, 0xa7,
-	0x63, 0x95, 0x68, 0x59, 0x29, 0xb7, 0x14, 0xd1, 0x32, 0xad, 0xf8, 0x11, 0x5c, 0xf1, 0xe9, 0xd0,
-	0xeb, 0x8f, 0xfa, 0xa3, 0x93, 0xa3, 0xe3, 0xf3, 0x90, 0x06, 0xb2, 0xd2, 0xab, 0x6b, 0xf2, 0x06,
-	0xa3, 0x32, 0xd5, 0x8e, 0x07, 0xe3, 0x63, 0x79, 0xd6, 0xf9, 0xb7, 0xf3, 0x7b, 0xbc, 0x73, 0x9e,
-	0x79, 0x61, 0x57, 0x79, 0x81, 0x6c, 0x41, 0x5d, 0x9f, 0x70, 0x4e, 0x91, 0xba, 0x58, 0xd9, 0x9e,
-	0xcb, 0x6c, 0xca, 0x13, 0xaf, 0xb2, 0xfd, 0x62, 0xd7, 0x24, 0x70, 0x28, 0x6f, 0xd4, 0xa5, 0x03,
-	0x0d, 0x95, 0x4f, 0x87, 0xe2, 0x8c, 0x26, 0x94, 0x49, 0xd8, 0xb8, 0x12, 0xdd, 0x84, 0xe2, 0x7c,
-	0x7e, 0x9b, 0x03, 0x32, 0xab, 0xc3, 0x77, 0x2d, 0x44, 0xef, 0x41, 0x3d, 0xc0, 0x63, 0x1f, 0x1e,
-	0x59, 0x75, 0xf0, 0x22, 0xa7, 0xea, 0x2c, 0x85, 0x1e, 0xc6, 0x02, 0xfc, 0x04, 0x43, 0x3a, 0x38,
-	0x1a, 0x8d, 0xc3, 0xfe, 0xf3, 0x73, 0x9e, 0x19, 0x17, 0xdc, 0xba, 0x22, 0xef, 0x72, 0xaa, 0xf3,
-	0x40, 0x29, 0x65, 0x2a, 0x4f, 0xde, 0x85, 0x85, 0x97, 0x8c, 0xaa, 0x2a, 0x74, 0xbc, 0xf2, 0xf9,
-	0x78, 0xab, 0xe7, 0xfc, 0x13, 0x2f, 0x40, 0xe9, 0xfe, 0x4c, 0x31, 0x60, 0x2e, 0x91, 0x8f, 0x2d,
-	0xc1, 0xea, 0x0d, 0xb1, 0x2d, 0x3d, 0x59, 0xa9, 0xa9, 0x21, 0x3b, 0x67, 0xc2, 0xcb, 0x38, 0x25,
-	0xec, 0xd1, 0x63, 0x4c, 0xf4, 0x4b, 0x5d, 0x71, 0xce, 0xac, 0x4c, 0xef, 0x5e, 0x91, 0x74, 0xed,
-	0x9d, 0xfb, 0x50, 0xa2, 0x67, 0x74, 0x14, 0x06, 0xab, 0x55, 0x9e, 0x17, 0x96, 0x8c, 0x8a, 0xb1,
-	0xcd, 0x26, 0x5c, 0x39, 0xef, 0xfc, 0x18, 0xae, 0xee, 0xb0, 0xd2, 0xee, 0x11, 0x6e, 0x80, 0x59,
-	0x24, 0x76, 0x3a, 0x3b, 0xd2, 0x31, 0x85, 0xb0, 0xb3, 0x43, 0xea, 0x90, 0xdf, 0x6a, 0x49, 0x33,
-	0xf2, 0xfd, 0x96, 0xf3, 0x4b, 0xdc, 0x6b, 0x53, 0x2e, 0x93, 0xa7, 0x2c, 0x70, 0xb5, 0x7c, 0x21,
-	0x5a, 0x1e, 0xab, 0x51, 0xea, 0xfb, 0x63, 0x9f, 0xfb, 0xa4, 0xe2, 0x8a, 0x81, 0x73, 0x57, 0xea,
-	0x80, 0x66, 0x8f, 0x5f, 0xe8, 0x78, 0x13, 0x68, 0x39, 0xad, 0xea, 0x36, 0x2c, 0xc7, 0xb8, 0x32,
-	0xe5, 0xa7, 0x8f, 0xe0, 0x1a, 0x07, 0xdb, 0xa6, 0x74, 0xd2, 0x1c, 0xf4, 0xcf, 0x52, 0x57, 0x9d,
-	0xc0, 0x75, 0x9b, 0xf1, 0xff, 0xeb, 0x23, 0xe7, 0x14, 0x4a, 0x4f, 0x78, 0x1b, 0x69, 0xe8, 0x52,
-	0xe4, 0xbc, 0x98, 0x64, 0x46, 0xde, 0x50, 0x14, 0xf8, 0x15, 0x97, 0x7f, 0xf3, 0x84, 0x4e, 0xa9,
-	0x7f, 0xe0, 0xee, 0x88, 0xbb, 0xa3, 0xe2, 0xea, 0x31, 0xb9, 0xc9, 0x1a, 0xd8, 0x3e, 0x86, 0x07,
-	0x9f, 0x2d, 0xf2, 0x59, 0x83, 0x82, 0x8d, 0xd4, 0x92, 0x58, 0xa9, 0xd9, 0xeb, 0x19, 0x97, 0x87,
-	0xc6, 0xcb, 0xc5, 0xf1, 0x9c, 0x97, 0x70, 0xd5, 0xe0, 0xcf, 0xe4, 0x86, 0x4f, 0xa0, 0x24, 0x7a,
-	0x65, 0x99, 0xb7, 0x56, 0xe2, 0x52, 0x62, 0x19, 0x57, 0xf2, 0x38, 0xf7, 0x60, 0x59, 0x52, 0xe8,
-	0x70, 0x9c, 0xb4, 0x57, 0xdc, 0x3f, 0xce, 0x0e, 0xac, 0xc4, 0xd9, 0x32, 0x85, 0x48, 0x53, 0x2d,
-	0x7a, 0x30, 0xe9, 0x19, 0x69, 0xd0, 0xde, 0x14, 0xd3, 0x61, 0x79, 0xcb, 0x61, 0x5a, 0x21, 0x05,
-	0x91, 0x49, 0xa1, 0x65, 0xe5, 0xfe, 0x9d, 0x7e, 0xa0, 0x2f, 0xbb, 0xd7, 0x40, 0x4c, 0x62, 0xa6,
-	0x4d, 0x59, 0x83, 0xb2, 0x70, 0xb8, 0x2a, 0xac, 0x92, 0x77, 0x45, 0x31, 0x31, 0x85, 0x5a, 0xf4,
-	0x39, 0xa6, 0xa3, 0x21, 0xd5, 0x39, 0x87, 0x55, 0x11, 0x26, 0x31, 0x93, 0xc5, 0x7f, 0xc2, 0x1b,
-	0xb4, 0x39, 0xf0, 0xfc, 0xa1, 0x72, 0xfe, 0x43, 0x28, 0x89, 0xf2, 0x44, 0xd6, 0xf2, 0x1f, 0xc6,
-	0x61, 0x4c, 0x5e, 0x31, 0x68, 0x8a, 0x62, 0x46, 0x4a, 0xb1, 0xcd, 0x92, 0x4f, 0x34, 0x2d, 0xeb,
-	0xc9, 0xa6, 0x45, 0x7e, 0x00, 0xf3, 0x1e, 0x13, 0xe1, 0x67, 0xb1, 0xbe, 0xfe, 0x4e, 0x02, 0x74,
-	0xe7, 0x7c, 0x42, 0x5d, 0xc1, 0xe5, 0x7c, 0x06, 0x55, 0x63, 0x05, 0x56, 0xf8, 0x3e, 0x6a, 0x77,
-	0xb0, 0x1a, 0xae, 0xc1, 0x42, 0x73, 0xb3, 0xb3, 0x75, 0x28, 0xea, 0xe1, 0x3a, 0x40, 0xab, 0xad,
-	0xc7, 0x79, 0x2c, 0x84, 0x84, 0x94, 0x3c, 0xe1, 0xa6, 0x3e, 0xb9, 0x34, 0x7d, 0xf2, 0x6f, 0xa5,
-	0xcf, 0x2b, 0x58, 0x94, 0xe6, 0x67, 0x8a, 0x81, 0x4f, 0xd1, 0xc3, 0x0c, 0x46, 0x85, 0xc0, 0xbb,
-	0x09, 0xcb, 0xaa, 0xd3, 0x29, 0x18, 0x1d, 0x2c, 0x20, 0xf6, 0x43, 0x2f, 0x9c, 0x06, 0x2a, 0x04,
-	0xfe, 0x98, 0x83, 0xba, 0xa2, 0x64, 0xed, 0xe7, 0x55, 0xbb, 0x24, 0x72, 0x9e, 0x6e, 0x96, 0xae,
-	0x43, 0xa9, 0x77, 0xbc, 0xdf, 0x7f, 0xad, 0xde, 0x35, 0xe4, 0x88, 0xd1, 0x07, 0x62, 0x1d, 0xf1,
-	0xb0, 0x26, 0x47, 0xac, 0x02, 0x67, 0x4f, 0x6c, 0x5b, 0xa3, 0x1e, 0x7d, 0xc5, 0x2f, 0xdb, 0xa2,
-	0x1b, 0x11, 0x78, 0xc5, 0x2c, 0x1f, 0xe0, 0x78, 0x2f, 0x65, 0x3e, 0xc8, 0x61, 0x90, 0x37, 0xa7,
-	0xe1, 0x69, 0x7b, 0xc4, 0xde, 0x9e, 0x94, 0x85, 0x2b, 0x40, 0x18, 0xb1, 0xd5, 0x0f, 0x4c, 0x6a,
-	0x1b, 0x96, 0x19, 0x15, 0xe3, 0x1e, 0xeb, 0xe9, 0x28, 0x63, 0xa8, 0xb4, 0x9d, 0xb3, 0xd2, 0xb6,
-	0x17, 0x04, 0x2f, 0xc7, 0x7e, 0x4f, 0x9a, 0xa6, 0xc7, 0x4e, 0x4b, 0x80, 0x1f, 0x04, 0xb1, 0xc4,
-	0xfc, 0x5d, 0x51, 0x56, 0x22, 0x94, 0x47, 0x54, 0x9f, 0xce, 0x8f, 0xe1, 0x9a, 0xa2, 0xca, 0xde,
-	0x39, 0x1d, 0xde, 0xd9, 0x83, 0xf7, 0x15, 0xf3, 0xe6, 0x29, 0xab, 0xeb, 0x9e, 0x4a, 0xf0, 0xff,
-	0x55, 0xa7, 0x87, 0xb0, 0xa2, 0x75, 0x32, 0xeb, 0x14, 0xc4, 0x99, 0x06, 0x32, 0x36, 0x10, 0x87,
-	0x7d, 0x33, 0x9a, 0x3f, 0x1e, 0xe8, 0xcb, 0x8e, 0x7d, 0x3b, 0xef, 0x44, 0xda, 0xc7, 0x6a, 0x05,
-	0xe7, 0xbe, 0x30, 0xd6, 0x45, 0xa6, 0x8b, 0x5d, 0xa6, 0xdc, 0xc2, 0x38, 0x0d, 0xb7, 0x48, 0x60,
-	0x46, 0x8d, 0xb9, 0xc5, 0x71, 0x85, 0xc6, 0x9c, 0xdd, 0xd2, 0x78, 0xc6, 0xf2, 0x0f, 0xa1, 0x38,
-	0xa1, 0xf2, 0xbc, 0x56, 0xd7, 0xc9, 0x9a, 0x78, 0x64, 0x5e, 0x7b, 0x8a, 0xb4, 0x7e, 0xc0, 0xa2,
-	0xd6, 0xe5, 0xf3, 0xe6, 0x62, 0x71, 0x2b, 0xbe, 0x14, 0xba, 0xa9, 0x50, 0xcb, 0x94, 0x3a, 0xb7,
-	0x45, 0x2c, 0xea, 0x08, 0xcd, 0x04, 0x76, 0x2c, 0xbc, 0x10, 0x05, 0x76, 0xa6, 0x53, 0x8d, 0x45,
-	0x60, 0x88, 0x56, 0xab, 0x33, 0x2d, 0x06, 0x4a, 0x61, 0x1d, 0xf5, 0x97, 0x61, 0xbd, 0x0e, 0xfe,
-	0x4c, 0x60, 0xbb, 0x70, 0xdd, 0x3e, 0x33, 0x99, 0xf0, 0x0e, 0xe1, 0x66, 0xda, 0xb1, 0xca, 0x84,
-	0xfb, 0x24, 0x3a, 0x1d, 0x97, 0x50, 0xcd, 0x9b, 0x66, 0x5f, 0x4a, 0xc9, 0x2d, 0xf7, 0x44, 0x9f,
-	0xd1, 0xcb, 0x02, 0xbb, 0xb4, 0x0d, 0x36, 0x4f, 0xff, 0x65, 0x6c, 0x84, 0x91, 0x34, 0x2e, 0x4b,
-	0xbd, 0xcb, 0xd8, 0x88, 0xef, 0x3b, 0x50, 0xd1, 0xd5, 0x83, 0xf1, 0xf7, 0x93, 0x2a, 0x94, 0x77,
-	0xf7, 0xf6, 0x9f, 0x36, 0x37, 0xb1, 0x6e, 0x59, 0xff, 0x47, 0x1e, 0xf2, 0xdb, 0x87, 0x64, 0x03,
-	0xe6, 0xc5, 0xab, 0xef, 0x05, 0xef, 0xe2, 0x8d, 0x8b, 0xde, 0x8f, 0x9d, 0x39, 0xf2, 0x39, 0x14,
-	0xd8, 0xbb, 0x6f, 0xea, 0xc3, 0x78, 0x23, 0xfd, 0xed, 0x18, 0xa5, 0x3b, 0x50, 0x35, 0x1e, 0x79,
-	0xc9, 0x1b, 0x1f, 0xc6, 0x1b, 0x6f, 0x7e, 0x40, 0x16, 0x3a, 0x75, 0x5e, 0x8d, 0x6c, 0x9d, 0xa2,
-	0x47, 0x49, 0x5b, 0x27, 0xe3, 0x09, 0x10, 0xa5, 0x77, 0xe5, 0xe3, 0x72, 0x37, 0x24, 0xb7, 0x12,
-	0xde, 0x2a, 0xcd, 0xc7, 0xb8, 0xc6, 0xed, 0x74, 0x06, 0x85, 0xb7, 0xbe, 0x07, 0xf3, 0xfc, 0xa1,
-	0x82, 0x7c, 0xa1, 0x3e, 0x1a, 0x09, 0xcf, 0x38, 0x29, 0xee, 0x8e, 0x3d, 0x71, 0x38, 0x73, 0xf7,
-	0x73, 0x3f, 0xcc, 0xad, 0x7f, 0x9b, 0x87, 0x79, 0xde, 0xb5, 0x92, 0xaf, 0x00, 0xa2, 0xf6, 0xde,
-	0xd6, 0x76, 0xe6, 0xc1, 0xc0, 0xd6, 0x76, 0xf6, 0x65, 0x40, 0xec, 0x88, 0xd1, 0x87, 0x93, 0x24,
-	0x91, 0xd8, 0xb5, 0x66, 0xef, 0x48, 0x42, 0x13, 0x8f, 0xa8, 0x1e, 0xd4, 0xe3, 0x7d, 0x36, 0xb9,
-	0x93, 0x20, 0x66, 0xb7, 0xeb, 0x8d, 0xbb, 0x17, 0x33, 0xc5, 0xbc, 0xf2, 0x97, 0x3c, 0xee, 0x9b,
-	0xf8, 0xf3, 0x2d, 0x6e, 0x61, 0x45, 0xb7, 0xb2, 0xe4, 0x66, 0x52, 0x9b, 0x13, 0xd5, 0x11, 0x8d,
-	0x5b, 0xa9, 0xf3, 0x5a, 0xfd, 0x67, 0x50, 0x33, 0x5b, 0x4f, 0xf2, 0x41, 0x62, 0xe7, 0x64, 0x76,
-	0xaf, 0x0d, 0xe7, 0x22, 0x96, 0x59, 0x60, 0xd1, 0x42, 0x26, 0x03, 0xc7, 0x3a, 0xd4, 0x64, 0xe0,
-	0x78, 0x07, 0x8a, 0xc0, 0x18, 0x19, 0x51, 0xe3, 0x48, 0x12, 0x4d, 0x34, 0xfa, 0x4c, 0x3b, 0x32,
-	0x66, 0x7b, 0x4e, 0x8c, 0xe3, 0xff, 0xe4, 0xa1, 0xfa, 0xc4, 0xeb, 0x8f, 0x42, 0x3a, 0x62, 0x6f,
-	0x5d, 0x2c, 0x7b, 0xf0, 0x44, 0x63, 0x87, 0xb3, 0xd9, 0xa6, 0xd9, 0xe1, 0x1c, 0xeb, 0x61, 0x50,
-	0xcd, 0x36, 0x94, 0x44, 0x2b, 0x41, 0x2c, 0xc6, 0x58, 0xcb, 0xd1, 0x78, 0x2f, 0x79, 0xd2, 0xb4,
-	0x36, 0xea, 0x4a, 0x6d, 0x6b, 0x67, 0x9a, 0xd8, 0xc6, 0xed, 0x74, 0x06, 0x0d, 0xf9, 0x33, 0x28,
-	0xb2, 0x37, 0x6d, 0x62, 0xa5, 0x0a, 0xe3, 0xd9, 0xbb, 0xd1, 0x48, 0x9a, 0xd2, 0x00, 0x4f, 0x60,
-	0x41, 0x3d, 0x53, 0x93, 0xf7, 0x2d, 0xfd, 0xe3, 0x4f, 0xda, 0x8d, 0x9b, 0x69, 0xd3, 0x0a, 0x0c,
-	0xc3, 0xfb, 0xaf, 0x15, 0x28, 0xb2, 0x7b, 0x82, 0xd9, 0x1a, 0x95, 0x91, 0xb6, 0xad, 0x33, 0xbd,
-	0x8c, 0x6d, 0xeb, 0x6c, 0x05, 0x2a, 0xce, 0xbc, 0x51, 0x4d, 0x92, 0x04, 0x91, 0x78, 0x2b, 0x64,
-	0x9f, 0xf9, 0x84, 0x52, 0x54, 0xc4, 0xb6, 0x59, 0x56, 0x92, 0x04, 0x21, 0xab, 0x97, 0xb2, 0x63,
-	0x3b, 0xa9, 0x2a, 0x45, 0xe0, 0xa7, 0x50, 0x96, 0x75, 0x64, 0x92, 0xaa, 0xf1, 0xc6, 0x2a, 0x49,
-	0x55, 0xab, 0x08, 0x8d, 0x10, 0xb1, 0xd6, 0x48, 0x43, 0x8c, 0xba, 0x89, 0x34, 0x44, 0xa3, 0x50,
-	0x41, 0xc4, 0xaf, 0x01, 0xa2, 0x8a, 0xd2, 0x4e, 0x76, 0x89, 0x3d, 0x9a, 0x9d, 0xec, 0x92, 0x8b,
-	0x52, 0x84, 0xfe, 0x06, 0xc8, 0x6c, 0x71, 0x49, 0x3e, 0x4e, 0x96, 0x4e, 0xec, 0xec, 0x1a, 0x9f,
-	0xbc, 0x1d, 0xb3, 0x5e, 0xf2, 0x10, 0x2a, 0xba, 0xee, 0x24, 0x4e, 0x8a, 0xfd, 0xe6, 0x4d, 0x73,
-	0xe7, 0x42, 0x1e, 0xdb, 0x4b, 0xf2, 0xae, 0x49, 0x11, 0x8a, 0x5f, 0x37, 0x77, 0x2f, 0x66, 0x32,
-	0xb7, 0x54, 0xd6, 0xa2, 0x49, 0x5b, 0x1a, 0x6f, 0x25, 0x93, 0xb6, 0xd4, 0x2a, 0x64, 0x23, 0xc4,
-	0x94, 0x20, 0x89, 0xb7, 0x9c, 0x69, 0x88, 0x33, 0x41, 0x12, 0x55, 0xa5, 0x49, 0xe6, 0xcf, 0x74,
-	0xac, 0x49, 0xe6, 0xcf, 0x16, 0xb6, 0x62, 0xc7, 0x74, 0x81, 0x9a, 0xb4, 0x63, 0x76, 0xcb, 0xdb,
-	0xb8, 0x73, 0x21, 0x8f, 0xad, 0x72, 0xfa, 0x8e, 0xcd, 0xf4, 0xbd, 0x69, 0x2a, 0xdb, 0x3b, 0xb6,
-	0x51, 0xfb, 0xc3, 0xdf, 0x6f, 0xe6, 0xfe, 0x8c, 0x3f, 0x7f, 0xc3, 0x9f, 0xe3, 0x12, 0xff, 0x8f,
-	0x5b, 0x3f, 0xfa, 0x6f, 0x00, 0x00, 0x00, 0xff, 0xff, 0x05, 0x71, 0xf8, 0x4c, 0x27, 0x26, 0x00,
-	0x00,
+	// 2339 bytes of a gzipped FileDescriptorProto
+	0x1f, 0x8b, 0x08, 0x00, 0x00, 0x09, 0x6e, 0x88, 0x02, 0xff, 0xac, 0x3a, 0x4b, 0x53, 0x1b, 0xd9,
+	0xd5, 0xe8, 0x81, 0x84, 0x8e, 0x84, 0x90, 0x2f, 0xd8, 0xc6, 0xf2, 0xd8, 0x83, 0x1b, 0xec, 0x9a,
+	0xef, 0x1b, 0x17, 0xce, 0xc8, 0x99, 0x4c, 0x2a, 0x99, 0xaa, 0x89, 0x00, 0x19, 0x1c, 0x63, 0x60,
+	0x24, 0xc1, 0xd4, 0x54, 0xa5, 0x4a, 0xd5, 0x48, 0xd7, 0xd0, 0x63, 0xbd, 0xa6, 0xbb, 0xc5, 0x98,
+	0x59, 0xe4, 0x27, 0x64, 0x93, 0x54, 0xe5, 0x47, 0x64, 0x9f, 0x7d, 0x76, 0xc9, 0x2a, 0x59, 0x66,
+	0x95, 0xa4, 0xb2, 0xcc, 0x5f, 0xc8, 0x26, 0xe7, 0xbe, 0xba, 0x6f, 0xb7, 0xae, 0xc0, 0x69, 0xb2,
+	0x30, 0xd0, 0xe7, 0x9e, 0xf7, 0xeb, 0x9e, 0x73, 0x67, 0xa0, 0xe0, 0x8e, 0xbb, 0x9b, 0x63, 0x77,
+	0xe4, 0x8f, 0x48, 0x89, 0xfa, 0xdd, 0x9e, 0x47, 0xdd, 0x0b, 0xea, 0x8e, 0x4f, 0xab, 0x2b, 0x67,
+	0xa3, 0xb3, 0x11, 0x3f, 0x78, 0xc6, 0xfe, 0x12, 0x38, 0xd5, 0x7b, 0x0c, 0xe7, 0xd9, 0xe0, 0xa2,
+	0xdb, 0xe5, 0x3f, 0xc6, 0xa7, 0xcf, 0xde, 0x5e, 0xc8, 0xa3, 0xfb, 0xfc, 0xc8, 0x9e, 0xf8, 0xe7,
+	0xfc, 0x07, 0x1e, 0xb1, 0x5f, 0xe2, 0xd0, 0xfa, 0x05, 0x94, 0x9b, 0xd4, 0x1b, 0x8f, 0x86, 0x1e,
+	0xdd, 0xa3, 0x76, 0x8f, 0xba, 0x84, 0x00, 0x74, 0xfb, 0x13, 0xcf, 0xa7, 0x6e, 0xc7, 0xe9, 0xad,
+	0xa6, 0xd6, 0x52, 0x1f, 0x65, 0xc9, 0x2d, 0x28, 0x0c, 0xe8, 0xe0, 0x54, 0x80, 0xd2, 0x1c, 0x54,
+	0x81, 0x05, 0x97, 0x5e, 0x38, 0x9e, 0x33, 0x1a, 0xae, 0x66, 0x10, 0x92, 0x61, 0x48, 0xae, 0xfd,
+	0xc6, 0xef, 0x20, 0xe5, 0x60, 0x35, 0xcb, 0x90, 0xac, 0x3f, 0xa7, 0xa1, 0xd4, 0xb4, 0x87, 0x67,
+	0xb4, 0x49, 0xbf, 0x9d, 0x50, 0xcf, 0x27, 0x45, 0xc8, 0xbc, 0xa5, 0x97, 0x9c, 0x6b, 0x49, 0x10,
+	0xe0, 0x61, 0x87, 0x0e, 0x05, 0xd7, 0x12, 0x59, 0x84, 0xf9, 0xbe, 0x33, 0x70, 0x7c, 0xc9, 0x52,
+	0x17, 0x92, 0xe5, 0x90, 0x1f, 0x03, 0x78, 0x23, 0xd7, 0xef, 0x8c, 0x5c, 0xd4, 0x75, 0x75, 0x1e,
+	0x61, 0xe5, 0xda, 0xc6, 0xa6, 0xee, 0xa0, 0x4d, 0x5d, 0xe0, 0x66, 0x0b, 0x91, 0x0f, 0x19, 0x2e,
+	0xf9, 0x09, 0x14, 0x39, 0xa5, 0x6f, 0xbb, 0x67, 0xd4, 0x5f, 0xcd, 0x71, 0xd2, 0xc7, 0xd7, 0x90,
+	0xb6, 0x39, 0x32, 0x59, 0x81, 0x12, 0xe2, 0x38, 0x76, 0xdf, 0xf9, 0xde, 0x3e, 0xed, 0xd3, 0xd5,
+	0x3c, 0x12, 0x2f, 0x58, 0x9b, 0x50, 0x08, 0xd9, 0x2f, 0x40, 0xf6, 0xe0, 0xf0, 0xa0, 0x51, 0x99,
+	0x43, 0xff, 0xe5, 0xea, 0xad, 0xed, 0xc6, 0xc1, 0x4e, 0x25, 0x85, 0xf6, 0xe6, 0x77, 0x1a, 0xe2,
+	0x23, 0x6d, 0x6d, 0x01, 0x68, 0x3c, 0xf3, 0x90, 0x79, 0xd5, 0xf8, 0x1a, 0xf1, 0x11, 0xe7, 0xa4,
+	0xd1, 0x6c, 0xbd, 0x3c, 0x3c, 0x40, 0x02, 0x24, 0xde, 0x6e, 0x36, 0xea, 0xed, 0x46, 0x25, 0xcd,
+	0x30, 0x5e, 0x1f, 0xee, 0x54, 0x32, 0xa4, 0x00, 0xf3, 0x27, 0xf5, 0xfd, 0xe3, 0x46, 0x25, 0x6b,
+	0x7d, 0x03, 0x8b, 0x52, 0x49, 0x11, 0x34, 0xf2, 0x14, 0x72, 0xe7, 0x3c, 0x70, 0xdc, 0xa9, 0xc5,
+	0xda, 0x07, 0x31, 0x8b, 0xa2, 0xc1, 0x7d, 0x80, 0xfe, 0xbf, 0xf0, 0xd0, 0xd9, 0x19, 0x44, 0xad,
+	0x6c, 0x8a, 0x54, 0xd9, 0x7c, 0x45, 0x2f, 0x4f, 0xec, 0xfe, 0x84, 0x92, 0x12, 0x64, 0x07, 0x23,
+	0x97, 0x72, 0xef, 0x2f, 0x58, 0x9f, 0x01, 0x1c, 0x4d, 0x7c, 0x63, 0xe8, 0x30, 0x4e, 0x17, 0x8c,
+	0x42, 0x0b, 0x1b, 0xb5, 0x3d, 0x41, 0x98, 0xb1, 0x7e, 0x0a, 0x45, 0x4e, 0x98, 0x44, 0x45, 0xeb,
+	0x87, 0x40, 0x76, 0x68, 0x9f, 0xfa, 0xf4, 0xbf, 0x49, 0x1c, 0xab, 0x0d, 0xcb, 0x11, 0xaa, 0x44,
+	0xde, 0x59, 0x82, 0x7c, 0x8f, 0x33, 0x11, 0x5c, 0x33, 0xd6, 0x1f, 0x52, 0x98, 0xbf, 0x42, 0x83,
+	0xe3, 0x21, 0x26, 0x21, 0x79, 0x0e, 0x8b, 0xae, 0xf8, 0xee, 0x70, 0x0d, 0x24, 0xdb, 0xea, 0xec,
+	0x34, 0xda, 0x9b, 0x23, 0xcf, 0xa0, 0xa8, 0x88, 0xc6, 0x13, 0x9f, 0xb3, 0x2e, 0xd6, 0x56, 0xa3,
+	0x24, 0xa1, 0xa3, 0x91, 0xe0, 0x67, 0xb0, 0xa2, 0x08, 0x84, 0x3e, 0x52, 0x58, 0x86, 0x53, 0xae,
+	0x45, 0x29, 0xa7, 0x9d, 0xb5, 0x37, 0xb7, 0x55, 0x80, 0xbc, 0xe4, 0x60, 0xfd, 0x29, 0x85, 0x29,
+	0x23, 0xed, 0x14, 0x46, 0x7c, 0x0a, 0x65, 0x57, 0x02, 0x22, 0x56, 0xdc, 0x37, 0x5a, 0x21, 0x3d,
+	0x34, 0x47, 0x3e, 0x81, 0x52, 0x40, 0x16, 0xda, 0x71, 0xcf, 0x60, 0x47, 0x40, 0xb2, 0x05, 0xb7,
+	0x03, 0x12, 0x83, 0x25, 0x8f, 0xae, 0xb0, 0x44, 0xf1, 0xc0, 0xa2, 0x59, 0x50, 0x3c, 0xac, 0xbf,
+	0xa6, 0x21, 0xbf, 0x3d, 0x1a, 0x8c, 0x6d, 0x97, 0x62, 0x28, 0x72, 0x08, 0x9f, 0xf4, 0x7d, 0xae,
+	0x7d, 0xb9, 0xb6, 0x1e, 0x65, 0x26, 0xd1, 0xd4, 0xef, 0x26, 0x47, 0x65, 0x44, 0xb2, 0xfe, 0xd3,
+	0xef, 0x41, 0x24, 0x2b, 0x55, 0xe6, 0x5e, 0x46, 0xe6, 0x5e, 0x1e, 0x71, 0xc3, 0x8e, 0x84, 0x56,
+	0xde, 0x83, 0xa5, 0xae, 0x4b, 0x6d, 0x66, 0x9c, 0x6a, 0x56, 0xf3, 0xf2, 0xe8, 0x0e, 0x94, 0x06,
+	0xa3, 0x5e, 0x08, 0xcf, 0x49, 0xf8, 0x92, 0xaa, 0x1f, 0xd6, 0x49, 0x4a, 0x7b, 0x73, 0xd6, 0x27,
+	0xb0, 0x18, 0xd5, 0x14, 0x6b, 0xbe, 0xf1, 0xe5, 0x71, 0x7d, 0x5f, 0x34, 0x88, 0x5d, 0xde, 0x13,
+	0x9a, 0xd8, 0x20, 0xb0, 0xcf, 0xec, 0x37, 0x5a, 0x2d, 0x6c, 0x27, 0x9f, 0x07, 0x24, 0x81, 0x9e,
+	0x41, 0x23, 0x99, 0xd3, 0x1a, 0x49, 0x4a, 0x35, 0x92, 0x74, 0xd8, 0x48, 0x32, 0x5b, 0x65, 0x28,
+	0x09, 0x4f, 0x74, 0x26, 0x2c, 0x29, 0xac, 0x5f, 0xa5, 0x00, 0xda, 0xef, 0x86, 0xaa, 0xde, 0x9e,
+	0x40, 0xbe, 0x2b, 0x98, 0xa3, 0x7b, 0x59, 0xb3, 0xb8, 0x6d, 0xf4, 0x14, 0xf9, 0x18, 0xf2, 0xde,
+	0xa4, 0xdb, 0xa5, 0x9e, 0x6a, 0x2a, 0xf1, 0x52, 0xd0, 0xab, 0x07, 0x91, 0xdf, 0xd8, 0x4e, 0x7f,
+	0xc2, 0x3b, 0xcc, 0x35, 0xc8, 0xd6, 0x2f, 0xa1, 0xc8, 0xf5, 0x49, 0x54, 0xc9, 0xd8, 0x21, 0xb8,
+	0x5a, 0xb4, 0x27, 0x6b, 0x79, 0x81, 0x60, 0xb7, 0x56, 0x79, 0xe4, 0x49, 0xf1, 0xf7, 0xcd, 0x3c,
+	0x84, 0xfc, 0xcf, 0xe0, 0x16, 0x37, 0xb2, 0xeb, 0xe3, 0x97, 0x72, 0x8b, 0x7e, 0x21, 0xa5, 0xd4,
+	0x15, 0x35, 0x3e, 0xbf, 0xf4, 0x9c, 0xae, 0xdd, 0x17, 0x82, 0xb0, 0xcd, 0x13, 0x9d, 0x30, 0x51,
+	0x13, 0x5c, 0x84, 0xe2, 0x9e, 0xed, 0x9d, 0x4b, 0xb1, 0xd6, 0xcf, 0xa1, 0x24, 0x3e, 0x13, 0x39,
+	0x03, 0xbb, 0xfa, 0x39, 0x52, 0x73, 0xf5, 0x16, 0xad, 0x5b, 0xb0, 0xd4, 0x1a, 0xda, 0x63, 0xef,
+	0x7c, 0xa4, 0x3a, 0x8e, 0x75, 0x06, 0x95, 0x10, 0x94, 0x48, 0xc4, 0x5d, 0x58, 0x72, 0xe9, 0xc0,
+	0x76, 0x86, 0xce, 0xf0, 0xac, 0x73, 0x7a, 0xe9, 0x53, 0x4f, 0x8e, 0x09, 0x28, 0xfb, 0xb4, 0x3f,
+	0x3a, 0x15, 0xc5, 0x63, 0xfd, 0x16, 0xfb, 0xe9, 0x57, 0xb6, 0xdf, 0x55, 0x86, 0xe1, 0xa5, 0x5c,
+	0x0e, 0x4a, 0x87, 0x43, 0xa4, 0xb4, 0x58, 0x8f, 0xe3, 0x34, 0xdb, 0x1c, 0x31, 0xec, 0x92, 0x8c,
+	0xd6, 0x1e, 0x76, 0x69, 0x3f, 0xa0, 0x4d, 0xcf, 0xa6, 0xe5, 0x88, 0x61, 0x7f, 0x5c, 0x0a, 0xfb,
+	0xb8, 0x48, 0x7f, 0x0a, 0x64, 0x5a, 0xc8, 0xb5, 0xe3, 0xca, 0x1d, 0x28, 0x7b, 0x58, 0x45, 0x7e,
+	0x27, 0x36, 0x0a, 0xa1, 0x3b, 0x70, 0xbc, 0x3a, 0xc3, 0x7c, 0xf3, 0x3a, 0xc3, 0x91, 0xef, 0xbc,
+	0xb9, 0xe4, 0xcd, 0x62, 0xc1, 0x7a, 0xa2, 0xc4, 0xe8, 0xfa, 0xb0, 0x1c, 0xfa, 0x8e, 0x41, 0xd5,
+	0xc0, 0x95, 0xb1, 0x7e, 0x87, 0x4d, 0x5b, 0x3a, 0x2a, 0x51, 0x3c, 0x74, 0x8e, 0xfc, 0x2a, 0x63,
+	0x77, 0x9b, 0xf0, 0x74, 0x4f, 0xdc, 0xee, 0x0c, 0x45, 0xb8, 0x0f, 0x21, 0x5c, 0x39, 0xb2, 0x0a,
+	0x95, 0xae, 0x48, 0xdc, 0x58, 0x23, 0xc3, 0xb1, 0x21, 0x47, 0x2f, 0xe8, 0xd0, 0xf7, 0x56, 0x8b,
+	0xbc, 0x70, 0x16, 0xd5, 0xe4, 0xd0, 0x60, 0x50, 0xeb, 0x29, 0xdc, 0xda, 0x67, 0xd7, 0xff, 0x2e,
+	0xfa, 0x47, 0x9f, 0x17, 0xda, 0xed, 0x7d, 0x59, 0x25, 0x00, 0xe9, 0x97, 0x3b, 0xf2, 0x52, 0xfd,
+	0x06, 0x88, 0x8e, 0x9d, 0xc8, 0x3e, 0x8d, 0x9f, 0x12, 0x24, 0x3c, 0x8f, 0x93, 0x08, 0x75, 0xdd,
+	0x91, 0xcb, 0x4d, 0x2a, 0x58, 0x6b, 0x52, 0x56, 0x93, 0x5e, 0x8c, 0xde, 0x06, 0x61, 0x15, 0xd4,
+	0xc2, 0xd3, 0xdb, 0xb0, 0x1c, 0xc1, 0x48, 0x54, 0xae, 0xeb, 0x70, 0x9b, 0x33, 0x79, 0x45, 0xe9,
+	0xb8, 0xde, 0x77, 0x2e, 0x8c, 0x92, 0x3a, 0x70, 0x27, 0x8e, 0xf4, 0x3f, 0xb5, 0xdd, 0xda, 0x83,
+	0xdc, 0x6b, 0x3e, 0xa5, 0x6b, 0x62, 0x79, 0x05, 0x0e, 0xed, 0x81, 0x98, 0xd4, 0x0a, 0xbc, 0x5d,
+	0x51, 0xea, 0x1e, 0x37, 0xf7, 0x45, 0x13, 0x2c, 0x88, 0x79, 0xdf, 0xc1, 0x30, 0x72, 0x58, 0x96,
+	0xc1, 0xac, 0x0d, 0xa8, 0x08, 0x4e, 0xf5, 0x5e, 0x4f, 0x4b, 0xd2, 0x80, 0x32, 0xc5, 0xb1, 0xce,
+	0xe0, 0x96, 0x86, 0x95, 0xc8, 0x96, 0x0d, 0xc8, 0x89, 0xc5, 0x42, 0xd6, 0xee, 0x4a, 0x14, 0x5b,
+	0xb0, 0xb7, 0x1e, 0xc1, 0xb2, 0xf8, 0xab, 0x49, 0x07, 0x23, 0x93, 0x73, 0xb3, 0xd6, 0x0e, 0xac,
+	0x44, 0x51, 0x12, 0xc5, 0xf1, 0xb9, 0x12, 0x74, 0x3c, 0xee, 0x69, 0x6d, 0x40, 0x77, 0xa7, 0xee,
+	0x86, 0x34, 0x77, 0x43, 0x20, 0x5a, 0x11, 0x25, 0x12, 0xbd, 0xac, 0x9c, 0xb9, 0xef, 0x78, 0x41,
+	0x63, 0x76, 0x80, 0xe8, 0xc0, 0x44, 0x2e, 0x7e, 0x0c, 0x79, 0xe1, 0x62, 0x75, 0x43, 0x9b, 0x7d,
+	0x8c, 0xf2, 0x77, 0xe8, 0x1b, 0xd7, 0x3e, 0x1b, 0xd0, 0xa0, 0x86, 0xd9, 0x55, 0xa6, 0x03, 0x13,
+	0x19, 0xf6, 0x7b, 0xec, 0xf9, 0xf5, 0xbe, 0xed, 0x0e, 0x94, 0x37, 0x7f, 0x04, 0x39, 0x71, 0x37,
+	0xca, 0xc1, 0xed, 0x49, 0x94, 0x5c, 0xc7, 0x15, 0x1f, 0x75, 0x8e, 0xcd, 0x3c, 0x2f, 0x0c, 0x91,
+	0xd9, 0x9f, 0xc5, 0x21, 0x65, 0xde, 0x66, 0x08, 0x3c, 0xff, 0xcb, 0xb5, 0xbb, 0x06, 0x46, 0xed,
+	0xcb, 0x31, 0xc5, 0x95, 0xa2, 0xa8, 0x33, 0xc2, 0x71, 0x68, 0xb7, 0xd1, 0xc6, 0x19, 0xa9, 0x04,
+	0x0b, 0xf5, 0xed, 0xf6, 0xcb, 0x13, 0x31, 0x25, 0x95, 0x01, 0x76, 0x1a, 0xc1, 0x77, 0xda, 0xda,
+	0x95, 0x54, 0xb2, 0xa6, 0x74, 0xf1, 0xa9, 0xa8, 0xf8, 0xf4, 0xd5, 0xe2, 0xcf, 0x61, 0x51, 0x1a,
+	0x95, 0x28, 0x80, 0xff, 0x87, 0xfe, 0x62, 0xe4, 0x2a, 0x7e, 0xf7, 0x0c, 0x72, 0x64, 0x10, 0xf1,
+	0x5a, 0x6b, 0xf9, 0xb6, 0x3f, 0xf1, 0x54, 0x00, 0x7f, 0x93, 0x82, 0xb2, 0x82, 0x24, 0x5d, 0x89,
+	0xd4, 0xb8, 0x2b, 0x1a, 0x48, 0x19, 0x72, 0xbd, 0xd3, 0x96, 0xf3, 0xbd, 0xdc, 0xf5, 0xd8, 0x77,
+	0x5f, 0xb0, 0xcb, 0xaa, 0xa7, 0x02, 0xf6, 0x0a, 0xf0, 0x72, 0xd8, 0xa3, 0xef, 0xf8, 0xed, 0x21,
+	0x9e, 0x0a, 0x10, 0xd4, 0x66, 0xef, 0x02, 0x39, 0x5e, 0xad, 0x98, 0x6c, 0xf5, 0x89, 0x7f, 0xde,
+	0x18, 0xb2, 0x6d, 0x5a, 0xe9, 0xba, 0x02, 0x84, 0x01, 0x77, 0x1c, 0x4f, 0x87, 0x7e, 0x0a, 0xcb,
+	0x0c, 0x8a, 0xf9, 0x87, 0x23, 0x56, 0x58, 0x92, 0xaa, 0xab, 0xa5, 0x82, 0xae, 0x66, 0x7b, 0xde,
+	0x77, 0x23, 0x57, 0x5c, 0x77, 0x05, 0xb6, 0x45, 0x32, 0xb2, 0x63, 0x2f, 0xd2, 0xc3, 0xae, 0xa3,
+	0x5a, 0x09, 0xa9, 0x76, 0x69, 0x50, 0x05, 0x8f, 0xe1, 0xb6, 0x82, 0xca, 0x15, 0xc5, 0xc4, 0xce,
+	0xfa, 0x02, 0x1e, 0x28, 0xb4, 0xed, 0x73, 0x36, 0x28, 0x1c, 0x49, 0xe6, 0xef, 0x2b, 0xbd, 0x06,
+	0x2b, 0x81, 0x74, 0xfd, 0x26, 0x45, 0xba, 0x89, 0x27, 0xe3, 0x55, 0x60, 0x5f, 0xee, 0xa8, 0x2f,
+	0xfb, 0xb9, 0x75, 0x37, 0xd4, 0x2d, 0x72, 0xc7, 0x59, 0x96, 0x30, 0xa5, 0x89, 0xa8, 0xb3, 0x1c,
+	0xa0, 0xcc, 0x65, 0x38, 0x9a, 0xb9, 0x92, 0x25, 0x83, 0x46, 0xcc, 0xb5, 0x5e, 0x08, 0xfd, 0x38,
+	0x7a, 0x4c, 0x3f, 0xcd, 0xae, 0x35, 0xc8, 0x8e, 0xa9, 0x2c, 0x8a, 0x62, 0x8d, 0x6c, 0x8a, 0x37,
+	0xa7, 0xcd, 0x23, 0x84, 0x39, 0x1e, 0x4b, 0x24, 0x5d, 0x40, 0x54, 0xe7, 0x2d, 0xa1, 0x8f, 0x4a,
+	0x8b, 0x44, 0xed, 0x66, 0x5b, 0xe4, 0x4b, 0x90, 0x45, 0x89, 0x98, 0xb4, 0x84, 0xa5, 0x61, 0xd2,
+	0x25, 0xaa, 0x1d, 0x9c, 0x45, 0x7c, 0xb4, 0x4e, 0x56, 0x8e, 0xd2, 0x2c, 0x48, 0xc9, 0x9b, 0x98,
+	0x17, 0x64, 0x68, 0x22, 0x26, 0x2f, 0xe0, 0x4e, 0x3c, 0xa1, 0x13, 0xf1, 0x39, 0x80, 0x87, 0xb3,
+	0x32, 0x3e, 0x11, 0xbf, 0x46, 0x98, 0xcc, 0x37, 0x18, 0x0e, 0x75, 0xf3, 0x6e, 0x34, 0xd5, 0x49,
+	0x5f, 0x07, 0x25, 0x74, 0x53, 0x26, 0x37, 0x0e, 0x98, 0x5e, 0x92, 0x37, 0x71, 0xb0, 0x56, 0xc1,
+	0x37, 0x55, 0xe7, 0x26, 0x0e, 0xfe, 0x7f, 0x0b, 0x0a, 0xc1, 0x2d, 0xa9, 0x3d, 0xa0, 0x16, 0x21,
+	0x7f, 0x70, 0xd8, 0x3a, 0xaa, 0x6f, 0xe3, 0xad, 0x5c, 0xfb, 0x57, 0x1a, 0xd2, 0xaf, 0x4e, 0xc8,
+	0x16, 0xcc, 0xf3, 0x87, 0x21, 0x72, 0xc5, 0x53, 0x5b, 0xf5, 0xaa, 0x07, 0x2c, 0x6b, 0x8e, 0x7c,
+	0x0e, 0x99, 0xa3, 0x89, 0x4f, 0x66, 0xbe, 0xbc, 0x55, 0x67, 0xbf, 0x65, 0x21, 0x75, 0x1b, 0x8a,
+	0xda, 0x03, 0x15, 0xb9, 0xf6, 0x15, 0xae, 0x7a, 0xfd, 0xeb, 0x96, 0xd0, 0xa9, 0xfd, 0x6e, 0x18,
+	0xd7, 0x29, 0x7c, 0x88, 0x89, 0xeb, 0xa4, 0x3d, 0x89, 0x20, 0xf5, 0x81, 0x7c, 0x0e, 0xeb, 0xfa,
+	0xe4, 0x43, 0xc3, 0xfb, 0x8c, 0xfe, 0x74, 0x51, 0x5d, 0x9b, 0x8d, 0xa0, 0xf8, 0xd5, 0x0e, 0x61,
+	0x9e, 0x6f, 0x9d, 0xe4, 0x85, 0xfa, 0xa3, 0x6a, 0x58, 0xa6, 0x67, 0xb8, 0x3b, 0xb2, 0xaf, 0x5a,
+	0x73, 0x1f, 0xa5, 0x7e, 0x90, 0xaa, 0xfd, 0x3a, 0x0d, 0xf3, 0x7c, 0xe9, 0x21, 0x5f, 0x02, 0x84,
+	0x5b, 0x5f, 0x5c, 0xdb, 0xa9, 0xed, 0x31, 0xae, 0xed, 0xf4, 0xc2, 0x28, 0x22, 0xa2, 0xad, 0x6e,
+	0xc4, 0x44, 0x12, 0xb9, 0x5f, 0xe2, 0x11, 0x31, 0xec, 0x7d, 0xc8, 0xd5, 0x86, 0x72, 0x74, 0x4d,
+	0x23, 0xeb, 0x06, 0xb2, 0xf8, 0xa6, 0x57, 0xdd, 0xb8, 0x1a, 0x29, 0xe2, 0x95, 0xbf, 0xb1, 0x67,
+	0x4c, 0xf1, 0xdf, 0x58, 0x30, 0x84, 0x85, 0x60, 0x89, 0x22, 0x0f, 0x4d, 0xa3, 0x79, 0x78, 0x7d,
+	0x57, 0x3f, 0x9c, 0x79, 0x1e, 0xa8, 0xff, 0x15, 0x94, 0xf4, 0x45, 0x88, 0x3c, 0x32, 0x91, 0x44,
+	0xf6, 0xa8, 0xaa, 0x75, 0x15, 0xca, 0x34, 0x63, 0xb1, 0xe6, 0x98, 0x19, 0x47, 0xf6, 0x26, 0x33,
+	0xe3, 0xe8, 0x96, 0x84, 0x8c, 0x31, 0x33, 0xc2, 0x25, 0x87, 0x18, 0x4d, 0xd4, 0x76, 0xa2, 0x78,
+	0x66, 0x4c, 0xef, 0x47, 0x98, 0xc7, 0xff, 0x4e, 0x43, 0xf1, 0xb5, 0xed, 0x0c, 0x7d, 0x3a, 0x64,
+	0x4f, 0x1c, 0xac, 0x7b, 0xf0, 0x46, 0x13, 0x4f, 0x67, 0x7d, 0xd7, 0x88, 0xa7, 0x73, 0x64, 0x64,
+	0x47, 0x35, 0x1b, 0x90, 0x13, 0x93, 0x34, 0x89, 0x21, 0x46, 0x26, 0xee, 0xea, 0x07, 0xe6, 0x43,
+	0xdd, 0xda, 0x70, 0xa5, 0x8a, 0x5b, 0x3b, 0xb5, 0x81, 0x55, 0xd7, 0x66, 0x23, 0x04, 0x2c, 0xbf,
+	0x80, 0x2c, 0x7b, 0x1d, 0x24, 0xb1, 0x56, 0xa1, 0x3d, 0x20, 0x56, 0xab, 0xa6, 0xa3, 0x80, 0xc1,
+	0x6b, 0x58, 0x50, 0xef, 0x7f, 0xe4, 0x41, 0x4c, 0xff, 0xe8, 0x53, 0x61, 0xf5, 0xe1, 0xac, 0x63,
+	0xc5, 0x0c, 0xd3, 0xfb, 0xef, 0x05, 0xc8, 0xb2, 0xfb, 0x81, 0xd9, 0x1a, 0xce, 0x73, 0x71, 0x5b,
+	0xa7, 0x16, 0x80, 0xb8, 0xad, 0xd3, 0xa3, 0xa0, 0xa8, 0x79, 0x6d, 0xbc, 0x23, 0x06, 0x92, 0xe8,
+	0xfe, 0x10, 0xaf, 0x79, 0xc3, 0x6c, 0x28, 0x72, 0x5b, 0x9f, 0xf7, 0x88, 0x81, 0x28, 0xb6, 0x80,
+	0xc4, 0x73, 0xdb, 0x34, 0x2e, 0x22, 0xe3, 0x23, 0xc8, 0xcb, 0x79, 0xcf, 0xa4, 0x6a, 0x74, 0x3b,
+	0x31, 0xa9, 0x1a, 0x1b, 0x16, 0x43, 0x8e, 0x38, 0x4b, 0xcc, 0xe2, 0x18, 0x8e, 0xf2, 0xb3, 0x38,
+	0x6a, 0x83, 0x08, 0x72, 0xfc, 0x1a, 0x20, 0x9c, 0x04, 0xe3, 0xcd, 0xce, 0xb8, 0xf8, 0xc4, 0x9b,
+	0x9d, 0x79, 0x98, 0x44, 0xd6, 0xdf, 0x02, 0x99, 0x1e, 0x0e, 0xc9, 0xc7, 0x66, 0x6a, 0xe3, 0xd2,
+	0x54, 0x7d, 0xfa, 0x7e, 0xc8, 0x81, 0xc8, 0x13, 0x28, 0x04, 0xf3, 0x23, 0xb1, 0x66, 0xd8, 0xaf,
+	0xdf, 0x34, 0xeb, 0x57, 0xe2, 0xc4, 0xbd, 0x24, 0xef, 0x9a, 0x19, 0x44, 0xd1, 0xeb, 0x66, 0xe3,
+	0x6a, 0x24, 0x3d, 0xa4, 0x72, 0xc6, 0x34, 0x85, 0x34, 0xba, 0xc1, 0x99, 0x42, 0x1a, 0x1b, 0x50,
+	0x43, 0x8e, 0x33, 0x92, 0x24, 0xba, 0xef, 0xcd, 0xe2, 0x38, 0x95, 0x24, 0xe1, 0xf4, 0x69, 0x32,
+	0x7f, 0x6a, 0x5d, 0x34, 0x99, 0x3f, 0x3d, 0xc0, 0x8a, 0x88, 0x05, 0x03, 0xa9, 0x29, 0x62, 0xf1,
+	0x7d, 0xb3, 0xba, 0x7e, 0x25, 0x4e, 0x5c, 0xe5, 0xd9, 0x11, 0x9b, 0x5a, 0x40, 0x67, 0xa9, 0x1c,
+	0x8f, 0xd8, 0x56, 0xe9, 0x8f, 0xff, 0x7c, 0x98, 0xfa, 0x0b, 0xfe, 0xfb, 0x07, 0xfe, 0x3b, 0xcd,
+	0xf1, 0xff, 0x95, 0xe2, 0xf9, 0x7f, 0x02, 0x00, 0x00, 0xff, 0xff, 0xe6, 0x6e, 0x06, 0x62, 0xb3,
+	0x21, 0x00, 0x00,
 }
diff --git a/etcdserver/etcdserverpb/rpc.proto b/etcdserver/etcdserverpb/rpc.proto
index c8fb8b0..232a636 100644
--- a/etcdserver/etcdserverpb/rpc.proto
+++ b/etcdserver/etcdserverpb/rpc.proto
@@ -2,7 +2,7 @@ syntax = "proto3";
 package etcdserverpb;
 
 import "gogoproto/gogo.proto";
-import "etcd/storage/storagepb/kv.proto";
+import "etcd/mvcc/mvccpb/kv.proto";
 import "etcd/auth/authpb/auth.proto";
 
 option (gogoproto.marshaler_all) = true;
@@ -192,7 +192,7 @@ message RangeRequest {
 message RangeResponse {
   ResponseHeader header = 1;
   // kvs is the list of key-value pairs matched by the range request.
-  repeated storagepb.KeyValue kvs = 2;
+  repeated mvccpb.KeyValue kvs = 2;
   // more indicates if there are more keys to return in the requested range.
   bool more = 3;
 }
@@ -401,7 +401,7 @@ message WatchResponse {
   // watcher with the same start_revision again.
   int64 compact_revision  = 5;
 
-  repeated storagepb.Event events = 11;
+  repeated mvccpb.Event events = 11;
 }
 
 message LeaseGrantRequest {
diff --git a/etcdserver/membership/cluster.go b/etcdserver/membership/cluster.go
index 3a67173..d001c0f 100644
--- a/etcdserver/membership/cluster.go
+++ b/etcdserver/membership/cluster.go
@@ -25,11 +25,11 @@ import (
 	"strings"
 	"sync"
 
+	"github.com/coreos/etcd/mvcc/backend"
 	"github.com/coreos/etcd/pkg/netutil"
 	"github.com/coreos/etcd/pkg/types"
 	"github.com/coreos/etcd/raft"
 	"github.com/coreos/etcd/raft/raftpb"
-	"github.com/coreos/etcd/storage/backend"
 	"github.com/coreos/etcd/store"
 	"github.com/coreos/etcd/version"
 	"github.com/coreos/go-semver/semver"
diff --git a/etcdserver/membership/store.go b/etcdserver/membership/store.go
index 5887ec9..aebc2d4 100644
--- a/etcdserver/membership/store.go
+++ b/etcdserver/membership/store.go
@@ -19,8 +19,8 @@ import (
 	"fmt"
 	"path"
 
+	"github.com/coreos/etcd/mvcc/backend"
 	"github.com/coreos/etcd/pkg/types"
-	"github.com/coreos/etcd/storage/backend"
 	"github.com/coreos/etcd/store"
 
 	"github.com/coreos/go-semver/semver"
diff --git a/etcdserver/quota.go b/etcdserver/quota.go
index 87c97f4..872bf5e 100644
--- a/etcdserver/quota.go
+++ b/etcdserver/quota.go
@@ -16,7 +16,7 @@ package etcdserver
 
 import (
 	pb "github.com/coreos/etcd/etcdserver/etcdserverpb"
-	"github.com/coreos/etcd/storage/backend"
+	"github.com/coreos/etcd/mvcc/backend"
 )
 
 // Quota represents an arbitrary quota against arbitrary requests. Each request
diff --git a/etcdserver/server.go b/etcdserver/server.go
index fdfa4ce..a55b48f 100644
--- a/etcdserver/server.go
+++ b/etcdserver/server.go
@@ -36,6 +36,8 @@ import (
 	"github.com/coreos/etcd/etcdserver/membership"
 	"github.com/coreos/etcd/etcdserver/stats"
 	"github.com/coreos/etcd/lease"
+	"github.com/coreos/etcd/mvcc"
+	"github.com/coreos/etcd/mvcc/backend"
 	"github.com/coreos/etcd/pkg/fileutil"
 	"github.com/coreos/etcd/pkg/idutil"
 	"github.com/coreos/etcd/pkg/pbutil"
@@ -47,8 +49,6 @@ import (
 	"github.com/coreos/etcd/raft/raftpb"
 	"github.com/coreos/etcd/rafthttp"
 	"github.com/coreos/etcd/snap"
-	dstorage "github.com/coreos/etcd/storage"
-	"github.com/coreos/etcd/storage/backend"
 	"github.com/coreos/etcd/store"
 	"github.com/coreos/etcd/version"
 	"github.com/coreos/etcd/wal"
@@ -181,7 +181,7 @@ type EtcdServer struct {
 	applyV2 applierV2
 
 	applyV3    applierV3
-	kv         dstorage.ConsistentWatchableKV
+	kv         mvcc.ConsistentWatchableKV
 	lessor     lease.Lessor
 	bemu       sync.Mutex
 	be         backend.Backend
@@ -392,7 +392,7 @@ func NewServer(cfg *ServerConfig) (srv *EtcdServer, err error) {
 
 	srv.be = be
 	srv.lessor = lease.NewLessor(srv.be)
-	srv.kv = dstorage.New(srv.be, srv.lessor, &srv.consistIndex)
+	srv.kv = mvcc.New(srv.be, srv.lessor, &srv.consistIndex)
 	srv.consistIndex.setConsistentIndex(srv.kv.ConsistentIndex())
 	srv.authStore = auth.NewAuthStore(srv.be)
 	if h := cfg.AutoCompactionRetention; h != 0 {
@@ -1258,7 +1258,7 @@ func (s *EtcdServer) parseProposeCtxErr(err error, start time.Time) error {
 	}
 }
 
-func (s *EtcdServer) KV() dstorage.ConsistentWatchableKV { return s.kv }
+func (s *EtcdServer) KV() mvcc.ConsistentWatchableKV { return s.kv }
 func (s *EtcdServer) Backend() backend.Backend {
 	s.bemu.Lock()
 	defer s.bemu.Unlock()
diff --git a/etcdserver/server_test.go b/etcdserver/server_test.go
index cbd6843..4509454 100644
--- a/etcdserver/server_test.go
+++ b/etcdserver/server_test.go
@@ -27,6 +27,8 @@ import (
 	pb "github.com/coreos/etcd/etcdserver/etcdserverpb"
 	"github.com/coreos/etcd/etcdserver/membership"
 	"github.com/coreos/etcd/lease"
+	"github.com/coreos/etcd/mvcc"
+	"github.com/coreos/etcd/mvcc/backend"
 	"github.com/coreos/etcd/pkg/idutil"
 	"github.com/coreos/etcd/pkg/mock/mockstorage"
 	"github.com/coreos/etcd/pkg/mock/mockstore"
@@ -38,8 +40,6 @@ import (
 	"github.com/coreos/etcd/raft"
 	"github.com/coreos/etcd/raft/raftpb"
 	"github.com/coreos/etcd/rafthttp"
-	dstorage "github.com/coreos/etcd/storage"
-	"github.com/coreos/etcd/storage/backend"
 	"github.com/coreos/etcd/store"
 	"golang.org/x/net/context"
 )
@@ -855,7 +855,7 @@ func TestSnapshot(t *testing.T) {
 		},
 		store: st,
 	}
-	srv.kv = dstorage.New(be, &lease.FakeLessor{}, &srv.consistIndex)
+	srv.kv = mvcc.New(be, &lease.FakeLessor{}, &srv.consistIndex)
 	srv.be = be
 
 	srv.snapshot(1, raftpb.ConfState{Nodes: []uint64{1}})
@@ -902,7 +902,7 @@ func TestTriggerSnap(t *testing.T) {
 	}
 	srv.applyV2 = &applierV2store{srv}
 
-	srv.kv = dstorage.New(be, &lease.FakeLessor{}, &srv.consistIndex)
+	srv.kv = mvcc.New(be, &lease.FakeLessor{}, &srv.consistIndex)
 	srv.be = be
 
 	srv.start()
@@ -974,7 +974,7 @@ func TestConcurrentApplyAndSnapshotV3(t *testing.T) {
 	defer func() {
 		os.RemoveAll(tmpPath)
 	}()
-	s.kv = dstorage.New(be, &lease.FakeLessor{}, &s.consistIndex)
+	s.kv = mvcc.New(be, &lease.FakeLessor{}, &s.consistIndex)
 	s.be = be
 
 	s.start()
diff --git a/etcdserver/snapshot_merge.go b/etcdserver/snapshot_merge.go
index 51599bc..c98463b 100644
--- a/etcdserver/snapshot_merge.go
+++ b/etcdserver/snapshot_merge.go
@@ -18,9 +18,9 @@ import (
 	"io"
 	"log"
 
+	"github.com/coreos/etcd/mvcc/backend"
 	"github.com/coreos/etcd/raft/raftpb"
 	"github.com/coreos/etcd/snap"
-	"github.com/coreos/etcd/storage/backend"
 )
 
 // createMergedSnapshotMessage creates a snapshot message that contains: raft status (term, conf),
diff --git a/etcdserver/v3_server.go b/etcdserver/v3_server.go
index d02e067..52a7328 100644
--- a/etcdserver/v3_server.go
+++ b/etcdserver/v3_server.go
@@ -20,7 +20,7 @@ import (
 	pb "github.com/coreos/etcd/etcdserver/etcdserverpb"
 	"github.com/coreos/etcd/lease"
 	"github.com/coreos/etcd/lease/leasehttp"
-	dstorage "github.com/coreos/etcd/storage"
+	"github.com/coreos/etcd/mvcc"
 	"golang.org/x/net/context"
 )
 
@@ -297,4 +297,4 @@ func (s *EtcdServer) processInternalRaftRequest(ctx context.Context, r pb.Intern
 }
 
 // Watchable returns a watchable interface attached to the etcdserver.
-func (s *EtcdServer) Watchable() dstorage.Watchable { return s.KV() }
+func (s *EtcdServer) Watchable() mvcc.Watchable { return s.KV() }
diff --git a/integration/v3_lease_test.go b/integration/v3_lease_test.go
index bd7d748..4a34bec 100644
--- a/integration/v3_lease_test.go
+++ b/integration/v3_lease_test.go
@@ -21,8 +21,8 @@ import (
 
 	"github.com/coreos/etcd/etcdserver/api/v3rpc/rpctypes"
 	pb "github.com/coreos/etcd/etcdserver/etcdserverpb"
+	"github.com/coreos/etcd/mvcc/mvccpb"
 	"github.com/coreos/etcd/pkg/testutil"
-	"github.com/coreos/etcd/storage/storagepb"
 	"golang.org/x/net/context"
 )
 
@@ -158,7 +158,7 @@ func TestV3LeaseExpire(t *testing.T) {
 				errc <- err
 			case len(resp.Events) != 1:
 				fallthrough
-			case resp.Events[0].Type != storagepb.DELETE:
+			case resp.Events[0].Type != mvccpb.DELETE:
 				errc <- fmt.Errorf("expected key delete, got %v", resp)
 			default:
 				errc <- nil
diff --git a/integration/v3_watch_test.go b/integration/v3_watch_test.go
index 28657e2..62eedd5 100644
--- a/integration/v3_watch_test.go
+++ b/integration/v3_watch_test.go
@@ -25,8 +25,8 @@ import (
 
 	"github.com/coreos/etcd/etcdserver/api/v3rpc"
 	pb "github.com/coreos/etcd/etcdserver/etcdserverpb"
+	"github.com/coreos/etcd/mvcc/mvccpb"
 	"github.com/coreos/etcd/pkg/testutil"
-	"github.com/coreos/etcd/storage/storagepb"
 	"golang.org/x/net/context"
 )
 
@@ -50,10 +50,10 @@ func TestV3WatchFromCurrentRevision(t *testing.T) {
 				{
 					Header:  &pb.ResponseHeader{Revision: 2},
 					Created: false,
-					Events: []*storagepb.Event{
+					Events: []*mvccpb.Event{
 						{
-							Type: storagepb.PUT,
-							Kv:   &storagepb.KeyValue{Key: []byte("foo"), Value: []byte("bar"), CreateRevision: 2, ModRevision: 2, Version: 1},
+							Type: mvccpb.PUT,
+							Kv:   &mvccpb.KeyValue{Key: []byte("foo"), Value: []byte("bar"), CreateRevision: 2, ModRevision: 2, Version: 1},
 						},
 					},
 				},
@@ -80,10 +80,10 @@ func TestV3WatchFromCurrentRevision(t *testing.T) {
 				{
 					Header:  &pb.ResponseHeader{Revision: 2},
 					Created: false,
-					Events: []*storagepb.Event{
+					Events: []*mvccpb.Event{
 						{
-							Type: storagepb.PUT,
-							Kv:   &storagepb.KeyValue{Key: []byte("fooLong"), Value: []byte("bar"), CreateRevision: 2, ModRevision: 2, Version: 1},
+							Type: mvccpb.PUT,
+							Kv:   &mvccpb.KeyValue{Key: []byte("fooLong"), Value: []byte("bar"), CreateRevision: 2, ModRevision: 2, Version: 1},
 						},
 					},
 				},
@@ -111,10 +111,10 @@ func TestV3WatchFromCurrentRevision(t *testing.T) {
 				{
 					Header:  &pb.ResponseHeader{Revision: 2},
 					Created: false,
-					Events: []*storagepb.Event{
+					Events: []*mvccpb.Event{
 						{
-							Type: storagepb.PUT,
-							Kv:   &storagepb.KeyValue{Key: []byte("fooLong"), Value: []byte("bar"), CreateRevision: 2, ModRevision: 2, Version: 1},
+							Type: mvccpb.PUT,
+							Kv:   &mvccpb.KeyValue{Key: []byte("fooLong"), Value: []byte("bar"), CreateRevision: 2, ModRevision: 2, Version: 1},
 						},
 					},
 				},
@@ -131,30 +131,30 @@ func TestV3WatchFromCurrentRevision(t *testing.T) {
 				{
 					Header:  &pb.ResponseHeader{Revision: 2},
 					Created: false,
-					Events: []*storagepb.Event{
+					Events: []*mvccpb.Event{
 						{
-							Type: storagepb.PUT,
-							Kv:   &storagepb.KeyValue{Key: []byte("foo"), Value: []byte("bar"), CreateRevision: 2, ModRevision: 2, Version: 1},
+							Type: mvccpb.PUT,
+							Kv:   &mvccpb.KeyValue{Key: []byte("foo"), Value: []byte("bar"), CreateRevision: 2, ModRevision: 2, Version: 1},
 						},
 					},
 				},
 				{
 					Header:  &pb.ResponseHeader{Revision: 3},
 					Created: false,
-					Events: []*storagepb.Event{
+					Events: []*mvccpb.Event{
 						{
-							Type: storagepb.PUT,
-							Kv:   &storagepb.KeyValue{Key: []byte("foo"), Value: []byte("bar"), CreateRevision: 2, ModRevision: 3, Version: 2},
+							Type: mvccpb.PUT,
+							Kv:   &mvccpb.KeyValue{Key: []byte("foo"), Value: []byte("bar"), CreateRevision: 2, ModRevision: 3, Version: 2},
 						},
 					},
 				},
 				{
 					Header:  &pb.ResponseHeader{Revision: 4},
 					Created: false,
-					Events: []*storagepb.Event{
+					Events: []*mvccpb.Event{
 						{
-							Type: storagepb.PUT,
-							Kv:   &storagepb.KeyValue{Key: []byte("foo"), Value: []byte("bar"), CreateRevision: 2, ModRevision: 4, Version: 3},
+							Type: mvccpb.PUT,
+							Kv:   &mvccpb.KeyValue{Key: []byte("foo"), Value: []byte("bar"), CreateRevision: 2, ModRevision: 4, Version: 3},
 						},
 					},
 				},
@@ -172,30 +172,30 @@ func TestV3WatchFromCurrentRevision(t *testing.T) {
 				{
 					Header:  &pb.ResponseHeader{Revision: 2},
 					Created: false,
-					Events: []*storagepb.Event{
+					Events: []*mvccpb.Event{
 						{
-							Type: storagepb.PUT,
-							Kv:   &storagepb.KeyValue{Key: []byte("foo"), Value: []byte("bar"), CreateRevision: 2, ModRevision: 2, Version: 1},
+							Type: mvccpb.PUT,
+							Kv:   &mvccpb.KeyValue{Key: []byte("foo"), Value: []byte("bar"), CreateRevision: 2, ModRevision: 2, Version: 1},
 						},
 					},
 				},
 				{
 					Header:  &pb.ResponseHeader{Revision: 3},
 					Created: false,
-					Events: []*storagepb.Event{
+					Events: []*mvccpb.Event{
 						{
-							Type: storagepb.PUT,
-							Kv:   &storagepb.KeyValue{Key: []byte("foo"), Value: []byte("bar"), CreateRevision: 2, ModRevision: 3, Version: 2},
+							Type: mvccpb.PUT,
+							Kv:   &mvccpb.KeyValue{Key: []byte("foo"), Value: []byte("bar"), CreateRevision: 2, ModRevision: 3, Version: 2},
 						},
 					},
 				},
 				{
 					Header:  &pb.ResponseHeader{Revision: 4},
 					Created: false,
-					Events: []*storagepb.Event{
+					Events: []*mvccpb.Event{
 						{
-							Type: storagepb.PUT,
-							Kv:   &storagepb.KeyValue{Key: []byte("foo"), Value: []byte("bar"), CreateRevision: 2, ModRevision: 4, Version: 3},
+							Type: mvccpb.PUT,
+							Kv:   &mvccpb.KeyValue{Key: []byte("foo"), Value: []byte("bar"), CreateRevision: 2, ModRevision: 4, Version: 3},
 						},
 					},
 				},
@@ -527,10 +527,10 @@ func TestV3WatchEmptyKey(t *testing.T) {
 	if rerr != nil {
 		t.Fatal(rerr)
 	}
-	wevs := []*storagepb.Event{
+	wevs := []*mvccpb.Event{
 		{
-			Type: storagepb.PUT,
-			Kv:   &storagepb.KeyValue{Key: []byte("foo"), CreateRevision: 2, ModRevision: 2, Version: 1},
+			Type: mvccpb.PUT,
+			Kv:   &mvccpb.KeyValue{Key: []byte("foo"), CreateRevision: 2, ModRevision: 2, Version: 1},
 		},
 	}
 	if !reflect.DeepEqual(resp.Events, wevs) {
@@ -691,7 +691,7 @@ func testV3WatchMultipleEventsTxn(t *testing.T, startRev int64) {
 		t.Fatalf("kvc.Txn failed: %+v", tresp)
 	}
 
-	events := []*storagepb.Event{}
+	events := []*mvccpb.Event{}
 	for len(events) < 3 {
 		resp, err := wStream.Recv()
 		if err != nil {
@@ -704,18 +704,18 @@ func testV3WatchMultipleEventsTxn(t *testing.T, startRev int64) {
 	}
 	sort.Sort(eventsSortByKey(events))
 
-	wevents := []*storagepb.Event{
+	wevents := []*mvccpb.Event{
 		{
-			Type: storagepb.PUT,
-			Kv:   &storagepb.KeyValue{Key: []byte("foo0"), Value: []byte("bar"), CreateRevision: 2, ModRevision: 2, Version: 1},
+			Type: mvccpb.PUT,
+			Kv:   &mvccpb.KeyValue{Key: []byte("foo0"), Value: []byte("bar"), CreateRevision: 2, ModRevision: 2, Version: 1},
 		},
 		{
-			Type: storagepb.PUT,
-			Kv:   &storagepb.KeyValue{Key: []byte("foo1"), Value: []byte("bar"), CreateRevision: 2, ModRevision: 2, Version: 1},
+			Type: mvccpb.PUT,
+			Kv:   &mvccpb.KeyValue{Key: []byte("foo1"), Value: []byte("bar"), CreateRevision: 2, ModRevision: 2, Version: 1},
 		},
 		{
-			Type: storagepb.PUT,
-			Kv:   &storagepb.KeyValue{Key: []byte("foo2"), Value: []byte("bar"), CreateRevision: 2, ModRevision: 2, Version: 1},
+			Type: mvccpb.PUT,
+			Kv:   &mvccpb.KeyValue{Key: []byte("foo2"), Value: []byte("bar"), CreateRevision: 2, ModRevision: 2, Version: 1},
 		},
 	}
 
@@ -732,7 +732,7 @@ func testV3WatchMultipleEventsTxn(t *testing.T, startRev int64) {
 	clus.Terminate(t)
 }
 
-type eventsSortByKey []*storagepb.Event
+type eventsSortByKey []*mvccpb.Event
 
 func (evs eventsSortByKey) Len() int           { return len(evs) }
 func (evs eventsSortByKey) Swap(i, j int)      { evs[i], evs[j] = evs[j], evs[i] }
@@ -773,26 +773,26 @@ func TestV3WatchMultipleEventsPutUnsynced(t *testing.T) {
 		t.Fatalf("couldn't put key (%v)", err)
 	}
 
-	allWevents := []*storagepb.Event{
+	allWevents := []*mvccpb.Event{
 		{
-			Type: storagepb.PUT,
-			Kv:   &storagepb.KeyValue{Key: []byte("foo0"), Value: []byte("bar"), CreateRevision: 2, ModRevision: 2, Version: 1},
+			Type: mvccpb.PUT,
+			Kv:   &mvccpb.KeyValue{Key: []byte("foo0"), Value: []byte("bar"), CreateRevision: 2, ModRevision: 2, Version: 1},
 		},
 		{
-			Type: storagepb.PUT,
-			Kv:   &storagepb.KeyValue{Key: []byte("foo1"), Value: []byte("bar"), CreateRevision: 3, ModRevision: 3, Version: 1},
+			Type: mvccpb.PUT,
+			Kv:   &mvccpb.KeyValue{Key: []byte("foo1"), Value: []byte("bar"), CreateRevision: 3, ModRevision: 3, Version: 1},
 		},
 		{
-			Type: storagepb.PUT,
-			Kv:   &storagepb.KeyValue{Key: []byte("foo0"), Value: []byte("bar"), CreateRevision: 2, ModRevision: 4, Version: 2},
+			Type: mvccpb.PUT,
+			Kv:   &mvccpb.KeyValue{Key: []byte("foo0"), Value: []byte("bar"), CreateRevision: 2, ModRevision: 4, Version: 2},
 		},
 		{
-			Type: storagepb.PUT,
-			Kv:   &storagepb.KeyValue{Key: []byte("foo1"), Value: []byte("bar"), CreateRevision: 3, ModRevision: 5, Version: 2},
+			Type: mvccpb.PUT,
+			Kv:   &mvccpb.KeyValue{Key: []byte("foo1"), Value: []byte("bar"), CreateRevision: 3, ModRevision: 5, Version: 2},
 		},
 	}
 
-	events := []*storagepb.Event{}
+	events := []*mvccpb.Event{}
 	for len(events) < 4 {
 		resp, err := wStream.Recv()
 		if err != nil {
@@ -866,10 +866,10 @@ func testV3WatchMultipleStreams(t *testing.T, startRev int64) {
 
 	var wg sync.WaitGroup
 	wg.Add(len(streams))
-	wevents := []*storagepb.Event{
+	wevents := []*mvccpb.Event{
 		{
-			Type: storagepb.PUT,
-			Kv:   &storagepb.KeyValue{Key: []byte("foo"), Value: []byte("bar"), CreateRevision: 2, ModRevision: 2, Version: 1},
+			Type: mvccpb.PUT,
+			Kv:   &mvccpb.KeyValue{Key: []byte("foo"), Value: []byte("bar"), CreateRevision: 2, ModRevision: 2, Version: 1},
 		},
 	}
 	for i := range streams {
diff --git a/lease/leasepb/lease.pb.go b/lease/leasepb/lease.pb.go
index 1c03019..34d90e8 100644
--- a/lease/leasepb/lease.pb.go
+++ b/lease/leasepb/lease.pb.go
@@ -33,8 +33,8 @@ var _ = math.Inf
 const _ = proto.GoGoProtoPackageIsVersion1
 
 type Lease struct {
-	ID  int64 `protobuf:"varint,1,opt,name=ID,json=iD,proto3" json:"ID,omitempty"`
-	TTL int64 `protobuf:"varint,2,opt,name=TTL,json=tTL,proto3" json:"TTL,omitempty"`
+	ID  int64 `protobuf:"varint,1,opt,name=ID,proto3" json:"ID,omitempty"`
+	TTL int64 `protobuf:"varint,2,opt,name=TTL,proto3" json:"TTL,omitempty"`
 }
 
 func (m *Lease) Reset()                    { *m = Lease{} }
@@ -319,13 +319,12 @@ var (
 )
 
 var fileDescriptorLease = []byte{
-	// 117 bytes of a gzipped FileDescriptorProto
+	// 108 bytes of a gzipped FileDescriptorProto
 	0x1f, 0x8b, 0x08, 0x00, 0x00, 0x09, 0x6e, 0x88, 0x02, 0xff, 0xe2, 0xe2, 0xce, 0x49, 0x4d, 0x2c,
 	0x4e, 0xd5, 0x2b, 0x28, 0xca, 0x2f, 0xc9, 0x17, 0x62, 0x07, 0x73, 0x0a, 0x92, 0xa4, 0x44, 0xd2,
-	0xf3, 0xd3, 0xf3, 0xc1, 0x62, 0xfa, 0x20, 0x16, 0x44, 0x5a, 0x49, 0x93, 0x8b, 0xd5, 0x07, 0xa4,
-	0x40, 0x88, 0x8f, 0x8b, 0xc9, 0xd3, 0x45, 0x82, 0x51, 0x81, 0x51, 0x83, 0x39, 0x88, 0x29, 0xd3,
-	0x45, 0x48, 0x80, 0x8b, 0x39, 0x24, 0xc4, 0x47, 0x82, 0x09, 0x2c, 0xc0, 0x5c, 0x12, 0xe2, 0xe3,
-	0x24, 0x72, 0xe2, 0xa1, 0x1c, 0xc3, 0x05, 0x20, 0x3e, 0xf1, 0x48, 0x8e, 0xf1, 0x02, 0x10, 0x3f,
-	0x00, 0xe2, 0x24, 0x36, 0xb0, 0x39, 0xc6, 0x80, 0x00, 0x00, 0x00, 0xff, 0xff, 0x8c, 0x1d, 0x92,
-	0xdc, 0x75, 0x00, 0x00, 0x00,
+	0xf3, 0xd3, 0xf3, 0xc1, 0x62, 0xfa, 0x20, 0x16, 0x44, 0x5a, 0x49, 0x81, 0x8b, 0xd5, 0x07, 0xa4,
+	0x40, 0x88, 0x8b, 0x8b, 0xc9, 0xd3, 0x45, 0x82, 0x51, 0x81, 0x51, 0x83, 0x59, 0x88, 0x9b, 0x8b,
+	0x39, 0x24, 0xc4, 0x47, 0x82, 0x09, 0xc4, 0x71, 0x12, 0x39, 0xf1, 0x50, 0x8e, 0xe1, 0x02, 0x10,
+	0x9f, 0x78, 0x24, 0xc7, 0x78, 0x01, 0x88, 0x1f, 0x00, 0x71, 0x12, 0x1b, 0x58, 0xbb, 0x31, 0x20,
+	0x00, 0x00, 0xff, 0xff, 0x87, 0x43, 0xe8, 0x0c, 0x6c, 0x00, 0x00, 0x00,
 }
diff --git a/lease/lessor.go b/lease/lessor.go
index 268eb8b..68f246d 100644
--- a/lease/lessor.go
+++ b/lease/lessor.go
@@ -22,7 +22,7 @@ import (
 	"time"
 
 	"github.com/coreos/etcd/lease/leasepb"
-	"github.com/coreos/etcd/storage/backend"
+	"github.com/coreos/etcd/mvcc/backend"
 )
 
 const (
@@ -47,7 +47,7 @@ type LeaseID int64
 
 // RangeDeleter defines an interface with DeleteRange method.
 // We define this interface only for lessor to limit the number
-// of methods of storage.KV to what lessor actually needs.
+// of methods of mvcc.KV to what lessor actually needs.
 //
 // Having a minimum interface makes testing easy.
 type RangeDeleter interface {
diff --git a/lease/lessor_test.go b/lease/lessor_test.go
index c018719..09c84bb 100644
--- a/lease/lessor_test.go
+++ b/lease/lessor_test.go
@@ -22,7 +22,7 @@ import (
 	"testing"
 	"time"
 
-	"github.com/coreos/etcd/storage/backend"
+	"github.com/coreos/etcd/mvcc/backend"
 )
 
 // TestLessorGrant ensures Lessor can grant wanted lease.
diff --git a/mvcc/backend/backend.go b/mvcc/backend/backend.go
new file mode 100644
index 0000000..2625e4d
--- /dev/null
+++ b/mvcc/backend/backend.go
@@ -0,0 +1,337 @@
+// Copyright 2015 CoreOS, Inc.
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//     http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+package backend
+
+import (
+	"fmt"
+	"hash/crc32"
+	"io"
+	"io/ioutil"
+	"log"
+	"os"
+	"path"
+	"sync"
+	"sync/atomic"
+	"time"
+
+	"github.com/boltdb/bolt"
+)
+
+var (
+	defaultBatchLimit    = 10000
+	defaultBatchInterval = 100 * time.Millisecond
+
+	defragLimit = 10000
+
+	// InitialMmapSize is the initial size of the mmapped region. Setting this larger than
+	// the potential max db size can prevent writer from blocking reader.
+	// This only works for linux.
+	InitialMmapSize = int64(10 * 1024 * 1024 * 1024)
+)
+
+const (
+	// DefaultQuotaBytes is the number of bytes the backend Size may
+	// consume before exceeding the space quota.
+	DefaultQuotaBytes = int64(2 * 1024 * 1024 * 1024) // 2GB
+	// MaxQuotaBytes is the maximum number of bytes suggested for a backend
+	// quota. A larger quota may lead to degraded performance.
+	MaxQuotaBytes = int64(8 * 1024 * 1024 * 1024) // 8GB
+)
+
+type Backend interface {
+	BatchTx() BatchTx
+	Snapshot() Snapshot
+	Hash() (uint32, error)
+	// Size returns the current size of the backend.
+	Size() int64
+	Defrag() error
+	ForceCommit()
+	Close() error
+}
+
+type Snapshot interface {
+	// Size gets the size of the snapshot.
+	Size() int64
+	// WriteTo writes the snapshot into the given writer.
+	WriteTo(w io.Writer) (n int64, err error)
+	// Close closes the snapshot.
+	Close() error
+}
+
+type backend struct {
+	// size and commits are used with atomic operations so they must be
+	// 64-bit aligned, otherwise 32-bit tests will crash
+
+	// size is the number of bytes in the backend
+	size int64
+	// commits counts number of commits since start
+	commits int64
+
+	mu sync.RWMutex
+	db *bolt.DB
+
+	batchInterval time.Duration
+	batchLimit    int
+	batchTx       *batchTx
+
+	stopc chan struct{}
+	donec chan struct{}
+}
+
+func New(path string, d time.Duration, limit int) Backend {
+	return newBackend(path, d, limit)
+}
+
+func NewDefaultBackend(path string) Backend {
+	return newBackend(path, defaultBatchInterval, defaultBatchLimit)
+}
+
+func newBackend(path string, d time.Duration, limit int) *backend {
+	db, err := bolt.Open(path, 0600, boltOpenOptions)
+	if err != nil {
+		log.Panicf("backend: cannot open database at %s (%v)", path, err)
+	}
+
+	b := &backend{
+		db: db,
+
+		batchInterval: d,
+		batchLimit:    limit,
+
+		stopc: make(chan struct{}),
+		donec: make(chan struct{}),
+	}
+	b.batchTx = newBatchTx(b)
+	go b.run()
+	return b
+}
+
+// BatchTx returns the current batch tx in coalescer. The tx can be used for read and
+// write operations. The write result can be retrieved within the same tx immediately.
+// The write result is isolated with other txs until the current one get committed.
+func (b *backend) BatchTx() BatchTx {
+	return b.batchTx
+}
+
+// ForceCommit forces the current batching tx to commit.
+func (b *backend) ForceCommit() {
+	b.batchTx.Commit()
+}
+
+func (b *backend) Snapshot() Snapshot {
+	b.batchTx.Commit()
+
+	b.mu.RLock()
+	defer b.mu.RUnlock()
+	tx, err := b.db.Begin(false)
+	if err != nil {
+		log.Fatalf("backend: cannot begin tx (%s)", err)
+	}
+	return &snapshot{tx}
+}
+
+func (b *backend) Hash() (uint32, error) {
+	h := crc32.New(crc32.MakeTable(crc32.Castagnoli))
+
+	b.mu.RLock()
+	defer b.mu.RUnlock()
+	err := b.db.View(func(tx *bolt.Tx) error {
+		c := tx.Cursor()
+		for next, _ := c.First(); next != nil; next, _ = c.Next() {
+			b := tx.Bucket(next)
+			if b == nil {
+				return fmt.Errorf("cannot get hash of bucket %s", string(next))
+			}
+			h.Write(next)
+			b.ForEach(func(k, v []byte) error {
+				h.Write(k)
+				h.Write(v)
+				return nil
+			})
+		}
+		return nil
+	})
+
+	if err != nil {
+		return 0, err
+	}
+
+	return h.Sum32(), nil
+}
+
+func (b *backend) Size() int64 {
+	return atomic.LoadInt64(&b.size)
+}
+
+func (b *backend) run() {
+	defer close(b.donec)
+
+	for {
+		select {
+		case <-time.After(b.batchInterval):
+		case <-b.stopc:
+			b.batchTx.CommitAndStop()
+			return
+		}
+		b.batchTx.Commit()
+	}
+}
+
+func (b *backend) Close() error {
+	close(b.stopc)
+	<-b.donec
+	return b.db.Close()
+}
+
+// Commits returns total number of commits since start
+func (b *backend) Commits() int64 {
+	return atomic.LoadInt64(&b.commits)
+}
+
+func (b *backend) Defrag() error {
+	err := b.defrag()
+	if err != nil {
+		return err
+	}
+
+	// commit to update metadata like db.size
+	b.batchTx.Commit()
+
+	return nil
+}
+
+func (b *backend) defrag() error {
+	// TODO: make this non-blocking?
+	// lock batchTx to ensure nobody is using previous tx, and then
+	// close previous ongoing tx.
+	b.batchTx.Lock()
+	defer b.batchTx.Unlock()
+
+	// lock database after lock tx to avoid deadlock.
+	b.mu.Lock()
+	defer b.mu.Unlock()
+
+	b.batchTx.commit(true)
+	b.batchTx.tx = nil
+
+	tmpdb, err := bolt.Open(b.db.Path()+".tmp", 0600, boltOpenOptions)
+	if err != nil {
+		return err
+	}
+
+	err = defragdb(b.db, tmpdb, defragLimit)
+
+	if err != nil {
+		tmpdb.Close()
+		os.RemoveAll(tmpdb.Path())
+		return err
+	}
+
+	dbp := b.db.Path()
+	tdbp := tmpdb.Path()
+
+	err = b.db.Close()
+	if err != nil {
+		log.Fatalf("backend: cannot close database (%s)", err)
+	}
+	err = tmpdb.Close()
+	if err != nil {
+		log.Fatalf("backend: cannot close database (%s)", err)
+	}
+	err = os.Rename(tdbp, dbp)
+	if err != nil {
+		log.Fatalf("backend: cannot rename database (%s)", err)
+	}
+
+	b.db, err = bolt.Open(dbp, 0600, boltOpenOptions)
+	if err != nil {
+		log.Panicf("backend: cannot open database at %s (%v)", dbp, err)
+	}
+	b.batchTx.tx, err = b.db.Begin(true)
+	if err != nil {
+		log.Fatalf("backend: cannot begin tx (%s)", err)
+	}
+
+	return nil
+}
+
+func defragdb(odb, tmpdb *bolt.DB, limit int) error {
+	// open a tx on tmpdb for writes
+	tmptx, err := tmpdb.Begin(true)
+	if err != nil {
+		return err
+	}
+
+	// open a tx on old db for read
+	tx, err := odb.Begin(false)
+	if err != nil {
+		return err
+	}
+	defer tx.Rollback()
+
+	c := tx.Cursor()
+
+	count := 0
+	for next, _ := c.First(); next != nil; next, _ = c.Next() {
+		b := tx.Bucket(next)
+		if b == nil {
+			return fmt.Errorf("backend: cannot defrag bucket %s", string(next))
+		}
+
+		tmpb, berr := tmptx.CreateBucketIfNotExists(next)
+		if berr != nil {
+			return berr
+		}
+
+		b.ForEach(func(k, v []byte) error {
+			count++
+			if count > limit {
+				err = tmptx.Commit()
+				if err != nil {
+					return err
+				}
+				tmptx, err = tmpdb.Begin(true)
+				if err != nil {
+					return err
+				}
+				tmpb = tmptx.Bucket(next)
+				count = 0
+			}
+			return tmpb.Put(k, v)
+		})
+	}
+
+	return tmptx.Commit()
+}
+
+// NewTmpBackend creates a backend implementation for testing.
+func NewTmpBackend(batchInterval time.Duration, batchLimit int) (*backend, string) {
+	dir, err := ioutil.TempDir(os.TempDir(), "etcd_backend_test")
+	if err != nil {
+		log.Fatal(err)
+	}
+	tmpPath := path.Join(dir, "database")
+	return newBackend(tmpPath, batchInterval, batchLimit), tmpPath
+}
+
+func NewDefaultTmpBackend() (*backend, string) {
+	return NewTmpBackend(defaultBatchInterval, defaultBatchLimit)
+}
+
+type snapshot struct {
+	*bolt.Tx
+}
+
+func (s *snapshot) Close() error { return s.Tx.Rollback() }
diff --git a/mvcc/backend/backend_bench_test.go b/mvcc/backend/backend_bench_test.go
new file mode 100644
index 0000000..1e798b8
--- /dev/null
+++ b/mvcc/backend/backend_bench_test.go
@@ -0,0 +1,50 @@
+// Copyright 2015 CoreOS, Inc.
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//     http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+package backend
+
+import (
+	"crypto/rand"
+	"os"
+	"testing"
+	"time"
+)
+
+func BenchmarkBackendPut(b *testing.B) {
+	backend := New("test", 100*time.Millisecond, 10000)
+	defer backend.Close()
+	defer os.Remove("test")
+
+	// prepare keys
+	keys := make([][]byte, b.N)
+	for i := 0; i < b.N; i++ {
+		keys[i] = make([]byte, 64)
+		rand.Read(keys[i])
+	}
+	value := make([]byte, 128)
+	rand.Read(value)
+
+	batchTx := backend.BatchTx()
+
+	batchTx.Lock()
+	batchTx.UnsafeCreateBucket([]byte("test"))
+	batchTx.Unlock()
+
+	b.ResetTimer()
+	for i := 0; i < b.N; i++ {
+		batchTx.Lock()
+		batchTx.UnsafePut([]byte("test"), keys[i], value)
+		batchTx.Unlock()
+	}
+}
diff --git a/mvcc/backend/backend_test.go b/mvcc/backend/backend_test.go
new file mode 100644
index 0000000..1ec10d7
--- /dev/null
+++ b/mvcc/backend/backend_test.go
@@ -0,0 +1,179 @@
+// Copyright 2015 CoreOS, Inc.
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//     http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+package backend
+
+import (
+	"fmt"
+	"io/ioutil"
+	"os"
+	"testing"
+	"time"
+
+	"github.com/boltdb/bolt"
+)
+
+func TestBackendClose(t *testing.T) {
+	b, tmpPath := NewTmpBackend(time.Hour, 10000)
+	defer os.Remove(tmpPath)
+
+	// check close could work
+	done := make(chan struct{})
+	go func() {
+		err := b.Close()
+		if err != nil {
+			t.Errorf("close error = %v, want nil", err)
+		}
+		done <- struct{}{}
+	}()
+	select {
+	case <-done:
+	case <-time.After(10 * time.Second):
+		t.Errorf("failed to close database in 10s")
+	}
+}
+
+func TestBackendSnapshot(t *testing.T) {
+	b, tmpPath := NewTmpBackend(time.Hour, 10000)
+	defer cleanup(b, tmpPath)
+
+	tx := b.BatchTx()
+	tx.Lock()
+	tx.UnsafeCreateBucket([]byte("test"))
+	tx.UnsafePut([]byte("test"), []byte("foo"), []byte("bar"))
+	tx.Unlock()
+	b.ForceCommit()
+
+	// write snapshot to a new file
+	f, err := ioutil.TempFile(os.TempDir(), "etcd_backend_test")
+	if err != nil {
+		t.Fatal(err)
+	}
+	snap := b.Snapshot()
+	defer snap.Close()
+	if _, err := snap.WriteTo(f); err != nil {
+		t.Fatal(err)
+	}
+	f.Close()
+
+	// bootstrap new backend from the snapshot
+	nb := New(f.Name(), time.Hour, 10000)
+	defer cleanup(nb, f.Name())
+
+	newTx := b.BatchTx()
+	newTx.Lock()
+	ks, _ := newTx.UnsafeRange([]byte("test"), []byte("foo"), []byte("goo"), 0)
+	if len(ks) != 1 {
+		t.Errorf("len(kvs) = %d, want 1", len(ks))
+	}
+	newTx.Unlock()
+}
+
+func TestBackendBatchIntervalCommit(t *testing.T) {
+	// start backend with super short batch interval so
+	// we do not need to wait long before commit to happen.
+	b, tmpPath := NewTmpBackend(time.Nanosecond, 10000)
+	defer cleanup(b, tmpPath)
+
+	pc := b.Commits()
+
+	tx := b.BatchTx()
+	tx.Lock()
+	tx.UnsafeCreateBucket([]byte("test"))
+	tx.UnsafePut([]byte("test"), []byte("foo"), []byte("bar"))
+	tx.Unlock()
+
+	for i := 0; i < 10; i++ {
+		if b.Commits() >= pc+1 {
+			break
+		}
+		time.Sleep(time.Duration(i*100) * time.Millisecond)
+	}
+
+	// check whether put happens via db view
+	b.db.View(func(tx *bolt.Tx) error {
+		bucket := tx.Bucket([]byte("test"))
+		if bucket == nil {
+			t.Errorf("bucket test does not exit")
+			return nil
+		}
+		v := bucket.Get([]byte("foo"))
+		if v == nil {
+			t.Errorf("foo key failed to written in backend")
+		}
+		return nil
+	})
+}
+
+func TestBackendDefrag(t *testing.T) {
+	b, tmpPath := NewDefaultTmpBackend()
+	defer cleanup(b, tmpPath)
+
+	tx := b.BatchTx()
+	tx.Lock()
+	tx.UnsafeCreateBucket([]byte("test"))
+	for i := 0; i < defragLimit+100; i++ {
+		tx.UnsafePut([]byte("test"), []byte(fmt.Sprintf("foo_%d", i)), []byte("bar"))
+	}
+	tx.Unlock()
+	b.ForceCommit()
+
+	// remove some keys to ensure the disk space will be reclaimed after defrag
+	tx = b.BatchTx()
+	tx.Lock()
+	for i := 0; i < 50; i++ {
+		tx.UnsafeDelete([]byte("test"), []byte(fmt.Sprintf("foo_%d", i)))
+	}
+	tx.Unlock()
+	b.ForceCommit()
+
+	size := b.Size()
+
+	// shrink and check hash
+	oh, err := b.Hash()
+	if err != nil {
+		t.Fatal(err)
+	}
+
+	err = b.Defrag()
+	if err != nil {
+		t.Fatal(err)
+	}
+
+	nh, err := b.Hash()
+	if err != nil {
+		t.Fatal(err)
+	}
+	if oh != nh {
+		t.Errorf("hash = %v, want %v", nh, oh)
+	}
+
+	nsize := b.Size()
+	if nsize >= size {
+		t.Errorf("new size = %v, want < %d", nsize, size)
+	}
+
+	// try put more keys after shrink.
+	tx = b.BatchTx()
+	tx.Lock()
+	tx.UnsafeCreateBucket([]byte("test"))
+	tx.UnsafePut([]byte("test"), []byte("more"), []byte("bar"))
+	tx.Unlock()
+	b.ForceCommit()
+}
+
+func cleanup(b Backend, path string) {
+	b.Close()
+	os.Remove(path)
+}
diff --git a/mvcc/backend/batch_tx.go b/mvcc/backend/batch_tx.go
new file mode 100644
index 0000000..08a0e54
--- /dev/null
+++ b/mvcc/backend/batch_tx.go
@@ -0,0 +1,189 @@
+// Copyright 2015 CoreOS, Inc.
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//     http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+package backend
+
+import (
+	"bytes"
+	"log"
+	"sync"
+	"sync/atomic"
+
+	"github.com/boltdb/bolt"
+)
+
+type BatchTx interface {
+	Lock()
+	Unlock()
+	UnsafeCreateBucket(name []byte)
+	UnsafePut(bucketName []byte, key []byte, value []byte)
+	UnsafeSeqPut(bucketName []byte, key []byte, value []byte)
+	UnsafeRange(bucketName []byte, key, endKey []byte, limit int64) (keys [][]byte, vals [][]byte)
+	UnsafeDelete(bucketName []byte, key []byte)
+	UnsafeForEach(bucketName []byte, visitor func(k, v []byte) error) error
+	Commit()
+	CommitAndStop()
+}
+
+type batchTx struct {
+	sync.Mutex
+	tx      *bolt.Tx
+	backend *backend
+	pending int
+}
+
+func newBatchTx(backend *backend) *batchTx {
+	tx := &batchTx{backend: backend}
+	tx.Commit()
+	return tx
+}
+
+func (t *batchTx) UnsafeCreateBucket(name []byte) {
+	_, err := t.tx.CreateBucket(name)
+	if err != nil && err != bolt.ErrBucketExists {
+		log.Fatalf("mvcc: cannot create bucket %s (%v)", string(name), err)
+	}
+	t.pending++
+}
+
+// UnsafePut must be called holding the lock on the tx.
+func (t *batchTx) UnsafePut(bucketName []byte, key []byte, value []byte) {
+	t.unsafePut(bucketName, key, value, false)
+}
+
+// UnsafeSeqPut must be called holding the lock on the tx.
+func (t *batchTx) UnsafeSeqPut(bucketName []byte, key []byte, value []byte) {
+	t.unsafePut(bucketName, key, value, true)
+}
+
+func (t *batchTx) unsafePut(bucketName []byte, key []byte, value []byte, seq bool) {
+	bucket := t.tx.Bucket(bucketName)
+	if bucket == nil {
+		log.Fatalf("mvcc: bucket %s does not exist", string(bucketName))
+	}
+	if seq {
+		// it is useful to increase fill percent when the workloads are mostly append-only.
+		// this can delay the page split and reduce space usage.
+		bucket.FillPercent = 0.9
+	}
+	if err := bucket.Put(key, value); err != nil {
+		log.Fatalf("mvcc: cannot put key into bucket (%v)", err)
+	}
+	t.pending++
+}
+
+// UnsafeRange must be called holding the lock on the tx.
+func (t *batchTx) UnsafeRange(bucketName []byte, key, endKey []byte, limit int64) (keys [][]byte, vs [][]byte) {
+	bucket := t.tx.Bucket(bucketName)
+	if bucket == nil {
+		log.Fatalf("mvcc: bucket %s does not exist", string(bucketName))
+	}
+
+	if len(endKey) == 0 {
+		if v := bucket.Get(key); v == nil {
+			return keys, vs
+		} else {
+			return append(keys, key), append(vs, v)
+		}
+	}
+
+	c := bucket.Cursor()
+	for ck, cv := c.Seek(key); ck != nil && bytes.Compare(ck, endKey) < 0; ck, cv = c.Next() {
+		vs = append(vs, cv)
+		keys = append(keys, ck)
+		if limit > 0 && limit == int64(len(keys)) {
+			break
+		}
+	}
+
+	return keys, vs
+}
+
+// UnsafeDelete must be called holding the lock on the tx.
+func (t *batchTx) UnsafeDelete(bucketName []byte, key []byte) {
+	bucket := t.tx.Bucket(bucketName)
+	if bucket == nil {
+		log.Fatalf("mvcc: bucket %s does not exist", string(bucketName))
+	}
+	err := bucket.Delete(key)
+	if err != nil {
+		log.Fatalf("mvcc: cannot delete key from bucket (%v)", err)
+	}
+	t.pending++
+}
+
+// UnsafeForEach must be called holding the lock on the tx.
+func (t *batchTx) UnsafeForEach(bucketName []byte, visitor func(k, v []byte) error) error {
+	b := t.tx.Bucket(bucketName)
+	if b == nil {
+		// bucket does not exist
+		return nil
+	}
+	return b.ForEach(visitor)
+}
+
+// Commit commits a previous tx and begins a new writable one.
+func (t *batchTx) Commit() {
+	t.Lock()
+	defer t.Unlock()
+	t.commit(false)
+}
+
+// CommitAndStop commits the previous tx and do not create a new one.
+func (t *batchTx) CommitAndStop() {
+	t.Lock()
+	defer t.Unlock()
+	t.commit(true)
+}
+
+func (t *batchTx) Unlock() {
+	if t.pending >= t.backend.batchLimit {
+		t.commit(false)
+		t.pending = 0
+	}
+	t.Mutex.Unlock()
+}
+
+func (t *batchTx) commit(stop bool) {
+	var err error
+	// commit the last tx
+	if t.tx != nil {
+		if t.pending == 0 && !stop {
+			t.backend.mu.RLock()
+			defer t.backend.mu.RUnlock()
+			atomic.StoreInt64(&t.backend.size, t.tx.Size())
+			return
+		}
+		err = t.tx.Commit()
+		atomic.AddInt64(&t.backend.commits, 1)
+
+		t.pending = 0
+		if err != nil {
+			log.Fatalf("mvcc: cannot commit tx (%s)", err)
+		}
+	}
+
+	if stop {
+		return
+	}
+
+	t.backend.mu.RLock()
+	defer t.backend.mu.RUnlock()
+	// begin a new tx
+	t.tx, err = t.backend.db.Begin(true)
+	if err != nil {
+		log.Fatalf("mvcc: cannot begin tx (%s)", err)
+	}
+	atomic.StoreInt64(&t.backend.size, t.tx.Size())
+}
diff --git a/mvcc/backend/batch_tx_test.go b/mvcc/backend/batch_tx_test.go
new file mode 100644
index 0000000..e8b1545
--- /dev/null
+++ b/mvcc/backend/batch_tx_test.go
@@ -0,0 +1,197 @@
+// Copyright 2015 CoreOS, Inc.
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//     http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+package backend
+
+import (
+	"reflect"
+	"testing"
+	"time"
+
+	"github.com/boltdb/bolt"
+)
+
+func TestBatchTxPut(t *testing.T) {
+	b, tmpPath := NewTmpBackend(time.Hour, 10000)
+	defer cleanup(b, tmpPath)
+
+	tx := b.batchTx
+	tx.Lock()
+	defer tx.Unlock()
+
+	// create bucket
+	tx.UnsafeCreateBucket([]byte("test"))
+
+	// put
+	v := []byte("bar")
+	tx.UnsafePut([]byte("test"), []byte("foo"), v)
+
+	// check put result before and after tx is committed
+	for k := 0; k < 2; k++ {
+		_, gv := tx.UnsafeRange([]byte("test"), []byte("foo"), nil, 0)
+		if !reflect.DeepEqual(gv[0], v) {
+			t.Errorf("v = %s, want %s", string(gv[0]), string(v))
+		}
+		tx.commit(false)
+	}
+}
+
+func TestBatchTxRange(t *testing.T) {
+	b, tmpPath := NewTmpBackend(time.Hour, 10000)
+	defer cleanup(b, tmpPath)
+
+	tx := b.batchTx
+	tx.Lock()
+	defer tx.Unlock()
+
+	tx.UnsafeCreateBucket([]byte("test"))
+	// put keys
+	allKeys := [][]byte{[]byte("foo"), []byte("foo1"), []byte("foo2")}
+	allVals := [][]byte{[]byte("bar"), []byte("bar1"), []byte("bar2")}
+	for i := range allKeys {
+		tx.UnsafePut([]byte("test"), allKeys[i], allVals[i])
+	}
+
+	tests := []struct {
+		key    []byte
+		endKey []byte
+		limit  int64
+
+		wkeys [][]byte
+		wvals [][]byte
+	}{
+		// single key
+		{
+			[]byte("foo"), nil, 0,
+			allKeys[:1], allVals[:1],
+		},
+		// single key, bad
+		{
+			[]byte("doo"), nil, 0,
+			nil, nil,
+		},
+		// key range
+		{
+			[]byte("foo"), []byte("foo1"), 0,
+			allKeys[:1], allVals[:1],
+		},
+		// key range, get all keys
+		{
+			[]byte("foo"), []byte("foo3"), 0,
+			allKeys, allVals,
+		},
+		// key range, bad
+		{
+			[]byte("goo"), []byte("goo3"), 0,
+			nil, nil,
+		},
+		// key range with effective limit
+		{
+			[]byte("foo"), []byte("foo3"), 1,
+			allKeys[:1], allVals[:1],
+		},
+		// key range with limit
+		{
+			[]byte("foo"), []byte("foo3"), 4,
+			allKeys, allVals,
+		},
+	}
+	for i, tt := range tests {
+		keys, vals := tx.UnsafeRange([]byte("test"), tt.key, tt.endKey, tt.limit)
+		if !reflect.DeepEqual(keys, tt.wkeys) {
+			t.Errorf("#%d: keys = %+v, want %+v", i, keys, tt.wkeys)
+		}
+		if !reflect.DeepEqual(vals, tt.wvals) {
+			t.Errorf("#%d: vals = %+v, want %+v", i, vals, tt.wvals)
+		}
+	}
+}
+
+func TestBatchTxDelete(t *testing.T) {
+	b, tmpPath := NewTmpBackend(time.Hour, 10000)
+	defer cleanup(b, tmpPath)
+
+	tx := b.batchTx
+	tx.Lock()
+	defer tx.Unlock()
+
+	tx.UnsafeCreateBucket([]byte("test"))
+	tx.UnsafePut([]byte("test"), []byte("foo"), []byte("bar"))
+
+	tx.UnsafeDelete([]byte("test"), []byte("foo"))
+
+	// check put result before and after tx is committed
+	for k := 0; k < 2; k++ {
+		ks, _ := tx.UnsafeRange([]byte("test"), []byte("foo"), nil, 0)
+		if len(ks) != 0 {
+			t.Errorf("keys on foo = %v, want nil", ks)
+		}
+		tx.commit(false)
+	}
+}
+
+func TestBatchTxCommit(t *testing.T) {
+	b, tmpPath := NewTmpBackend(time.Hour, 10000)
+	defer cleanup(b, tmpPath)
+
+	tx := b.batchTx
+	tx.Lock()
+	tx.UnsafeCreateBucket([]byte("test"))
+	tx.UnsafePut([]byte("test"), []byte("foo"), []byte("bar"))
+	tx.Unlock()
+
+	tx.Commit()
+
+	// check whether put happens via db view
+	b.db.View(func(tx *bolt.Tx) error {
+		bucket := tx.Bucket([]byte("test"))
+		if bucket == nil {
+			t.Errorf("bucket test does not exit")
+			return nil
+		}
+		v := bucket.Get([]byte("foo"))
+		if v == nil {
+			t.Errorf("foo key failed to written in backend")
+		}
+		return nil
+	})
+}
+
+func TestBatchTxBatchLimitCommit(t *testing.T) {
+	// start backend with batch limit 1 so one write can
+	// trigger a commit
+	b, tmpPath := NewTmpBackend(time.Hour, 1)
+	defer cleanup(b, tmpPath)
+
+	tx := b.batchTx
+	tx.Lock()
+	tx.UnsafeCreateBucket([]byte("test"))
+	tx.UnsafePut([]byte("test"), []byte("foo"), []byte("bar"))
+	tx.Unlock()
+
+	// batch limit commit should have been triggered
+	// check whether put happens via db view
+	b.db.View(func(tx *bolt.Tx) error {
+		bucket := tx.Bucket([]byte("test"))
+		if bucket == nil {
+			t.Errorf("bucket test does not exit")
+			return nil
+		}
+		v := bucket.Get([]byte("foo"))
+		if v == nil {
+			t.Errorf("foo key failed to written in backend")
+		}
+		return nil
+	})
+}
diff --git a/mvcc/backend/boltoption_darwin.go b/mvcc/backend/boltoption_darwin.go
new file mode 100644
index 0000000..67269f0
--- /dev/null
+++ b/mvcc/backend/boltoption_darwin.go
@@ -0,0 +1,19 @@
+// Copyright 2015 CoreOS, Inc.
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//     http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+package backend
+
+import "github.com/boltdb/bolt"
+
+var boltOpenOptions *bolt.Options = nil
diff --git a/mvcc/backend/boltoption_freebsd.go b/mvcc/backend/boltoption_freebsd.go
new file mode 100644
index 0000000..67269f0
--- /dev/null
+++ b/mvcc/backend/boltoption_freebsd.go
@@ -0,0 +1,19 @@
+// Copyright 2015 CoreOS, Inc.
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//     http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+package backend
+
+import "github.com/boltdb/bolt"
+
+var boltOpenOptions *bolt.Options = nil
diff --git a/mvcc/backend/boltoption_solaris.go b/mvcc/backend/boltoption_solaris.go
new file mode 100644
index 0000000..f12bf58
--- /dev/null
+++ b/mvcc/backend/boltoption_solaris.go
@@ -0,0 +1,19 @@
+// Copyright 2016 CoreOS, Inc.
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//     http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+package backend
+
+import "github.com/boltdb/bolt"
+
+var boltOpenOptions *bolt.Options = nil
diff --git a/mvcc/backend/boltoption_unix.go b/mvcc/backend/boltoption_unix.go
new file mode 100644
index 0000000..017f603
--- /dev/null
+++ b/mvcc/backend/boltoption_unix.go
@@ -0,0 +1,34 @@
+// Copyright 2015 CoreOS, Inc.
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//     http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+// +build linux
+
+package backend
+
+import (
+	"syscall"
+
+	"github.com/boltdb/bolt"
+)
+
+// syscall.MAP_POPULATE on linux 2.6.23+ does sequential read-ahead
+// which can speed up entire-database read with boltdb. We want to
+// enable MAP_POPULATE for faster key-value store recovery in storage
+// package. If your kernel version is lower than 2.6.23
+// (https://github.com/torvalds/linux/releases/tag/v2.6.23), mmap might
+// silently ignore this flag. Please update your kernel to prevent this.
+var boltOpenOptions = &bolt.Options{
+	MmapFlags:       syscall.MAP_POPULATE,
+	InitialMmapSize: int(InitialMmapSize),
+}
diff --git a/mvcc/backend/boltoption_windows.go b/mvcc/backend/boltoption_windows.go
new file mode 100644
index 0000000..1a790d1
--- /dev/null
+++ b/mvcc/backend/boltoption_windows.go
@@ -0,0 +1,21 @@
+// Copyright 2015 CoreOS, Inc.
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//     http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+package backend
+
+import "github.com/boltdb/bolt"
+
+// TODO: support syscall.MAP_POPULATE in windows.
+// Need upstream patch from boltdb/bolt.
+var boltOpenOptions *bolt.Options = nil
diff --git a/mvcc/backend/doc.go b/mvcc/backend/doc.go
new file mode 100644
index 0000000..de4e19a
--- /dev/null
+++ b/mvcc/backend/doc.go
@@ -0,0 +1,16 @@
+// Copyright 2015 CoreOS, Inc.
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//     http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+// Package backend defines a standard interface for etcd's backend MVCC storage.
+package backend
diff --git a/mvcc/doc.go b/mvcc/doc.go
new file mode 100644
index 0000000..2e16c5f
--- /dev/null
+++ b/mvcc/doc.go
@@ -0,0 +1,16 @@
+// Copyright 2015 CoreOS, Inc.
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//     http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+// Package mvcc defines etcd's stable MVCC storage.
+package mvcc
diff --git a/mvcc/index.go b/mvcc/index.go
new file mode 100644
index 0000000..49ecb30
--- /dev/null
+++ b/mvcc/index.go
@@ -0,0 +1,218 @@
+// Copyright 2015 CoreOS, Inc.
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//     http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+package mvcc
+
+import (
+	"log"
+	"sort"
+	"sync"
+
+	"github.com/google/btree"
+)
+
+type index interface {
+	Get(key []byte, atRev int64) (rev, created revision, ver int64, err error)
+	Range(key, end []byte, atRev int64) ([][]byte, []revision)
+	Put(key []byte, rev revision)
+	Restore(key []byte, created, modified revision, ver int64)
+	Tombstone(key []byte, rev revision) error
+	RangeSince(key, end []byte, rev int64) []revision
+	Compact(rev int64) map[revision]struct{}
+	Equal(b index) bool
+}
+
+type treeIndex struct {
+	sync.RWMutex
+	tree *btree.BTree
+}
+
+func newTreeIndex() index {
+	return &treeIndex{
+		tree: btree.New(32),
+	}
+}
+
+func (ti *treeIndex) Put(key []byte, rev revision) {
+	keyi := &keyIndex{key: key}
+
+	ti.Lock()
+	defer ti.Unlock()
+	item := ti.tree.Get(keyi)
+	if item == nil {
+		keyi.put(rev.main, rev.sub)
+		ti.tree.ReplaceOrInsert(keyi)
+		return
+	}
+	okeyi := item.(*keyIndex)
+	okeyi.put(rev.main, rev.sub)
+}
+
+func (ti *treeIndex) Restore(key []byte, created, modified revision, ver int64) {
+	keyi := &keyIndex{key: key}
+
+	ti.Lock()
+	defer ti.Unlock()
+	item := ti.tree.Get(keyi)
+	if item == nil {
+		keyi.restore(created, modified, ver)
+		ti.tree.ReplaceOrInsert(keyi)
+		return
+	}
+	okeyi := item.(*keyIndex)
+	okeyi.put(modified.main, modified.sub)
+}
+
+func (ti *treeIndex) Get(key []byte, atRev int64) (modified, created revision, ver int64, err error) {
+	keyi := &keyIndex{key: key}
+
+	ti.RLock()
+	defer ti.RUnlock()
+	item := ti.tree.Get(keyi)
+	if item == nil {
+		return revision{}, revision{}, 0, ErrRevisionNotFound
+	}
+
+	keyi = item.(*keyIndex)
+	return keyi.get(atRev)
+}
+
+func (ti *treeIndex) Range(key, end []byte, atRev int64) (keys [][]byte, revs []revision) {
+	if end == nil {
+		rev, _, _, err := ti.Get(key, atRev)
+		if err != nil {
+			return nil, nil
+		}
+		return [][]byte{key}, []revision{rev}
+	}
+
+	keyi := &keyIndex{key: key}
+	endi := &keyIndex{key: end}
+
+	ti.RLock()
+	defer ti.RUnlock()
+
+	ti.tree.AscendGreaterOrEqual(keyi, func(item btree.Item) bool {
+		if len(endi.key) > 0 && !item.Less(endi) {
+			return false
+		}
+		curKeyi := item.(*keyIndex)
+		rev, _, _, err := curKeyi.get(atRev)
+		if err != nil {
+			return true
+		}
+		revs = append(revs, rev)
+		keys = append(keys, curKeyi.key)
+		return true
+	})
+
+	return keys, revs
+}
+
+func (ti *treeIndex) Tombstone(key []byte, rev revision) error {
+	keyi := &keyIndex{key: key}
+
+	ti.Lock()
+	defer ti.Unlock()
+	item := ti.tree.Get(keyi)
+	if item == nil {
+		return ErrRevisionNotFound
+	}
+
+	ki := item.(*keyIndex)
+	return ki.tombstone(rev.main, rev.sub)
+}
+
+// RangeSince returns all revisions from key(including) to end(excluding)
+// at or after the given rev. The returned slice is sorted in the order
+// of revision.
+func (ti *treeIndex) RangeSince(key, end []byte, rev int64) []revision {
+	ti.RLock()
+	defer ti.RUnlock()
+
+	keyi := &keyIndex{key: key}
+	if end == nil {
+		item := ti.tree.Get(keyi)
+		if item == nil {
+			return nil
+		}
+		keyi = item.(*keyIndex)
+		return keyi.since(rev)
+	}
+
+	endi := &keyIndex{key: end}
+	var revs []revision
+	ti.tree.AscendGreaterOrEqual(keyi, func(item btree.Item) bool {
+		if len(endi.key) > 0 && !item.Less(endi) {
+			return false
+		}
+		curKeyi := item.(*keyIndex)
+		revs = append(revs, curKeyi.since(rev)...)
+		return true
+	})
+	sort.Sort(revisions(revs))
+
+	return revs
+}
+
+func (ti *treeIndex) Compact(rev int64) map[revision]struct{} {
+	available := make(map[revision]struct{})
+	var emptyki []*keyIndex
+	log.Printf("store.index: compact %d", rev)
+	// TODO: do not hold the lock for long time?
+	// This is probably OK. Compacting 10M keys takes O(10ms).
+	ti.Lock()
+	defer ti.Unlock()
+	ti.tree.Ascend(compactIndex(rev, available, &emptyki))
+	for _, ki := range emptyki {
+		item := ti.tree.Delete(ki)
+		if item == nil {
+			log.Panic("store.index: unexpected delete failure during compaction")
+		}
+	}
+	return available
+}
+
+func compactIndex(rev int64, available map[revision]struct{}, emptyki *[]*keyIndex) func(i btree.Item) bool {
+	return func(i btree.Item) bool {
+		keyi := i.(*keyIndex)
+		keyi.compact(rev, available)
+		if keyi.isEmpty() {
+			*emptyki = append(*emptyki, keyi)
+		}
+		return true
+	}
+}
+
+func (a *treeIndex) Equal(bi index) bool {
+	b := bi.(*treeIndex)
+
+	if a.tree.Len() != b.tree.Len() {
+		return false
+	}
+
+	equal := true
+
+	a.tree.Ascend(func(item btree.Item) bool {
+		aki := item.(*keyIndex)
+		bki := b.tree.Get(item).(*keyIndex)
+		if !aki.equal(bki) {
+			equal = false
+			return false
+		}
+		return true
+	})
+
+	return equal
+}
diff --git a/mvcc/index_test.go b/mvcc/index_test.go
new file mode 100644
index 0000000..bce549c
--- /dev/null
+++ b/mvcc/index_test.go
@@ -0,0 +1,323 @@
+// Copyright 2015 CoreOS, Inc.
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//     http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+package mvcc
+
+import (
+	"reflect"
+	"testing"
+)
+
+func TestIndexGet(t *testing.T) {
+	ti := newTreeIndex()
+	ti.Put([]byte("foo"), revision{main: 2})
+	ti.Put([]byte("foo"), revision{main: 4})
+	ti.Tombstone([]byte("foo"), revision{main: 6})
+
+	tests := []struct {
+		rev int64
+
+		wrev     revision
+		wcreated revision
+		wver     int64
+		werr     error
+	}{
+		{0, revision{}, revision{}, 0, ErrRevisionNotFound},
+		{1, revision{}, revision{}, 0, ErrRevisionNotFound},
+		{2, revision{main: 2}, revision{main: 2}, 1, nil},
+		{3, revision{main: 2}, revision{main: 2}, 1, nil},
+		{4, revision{main: 4}, revision{main: 2}, 2, nil},
+		{5, revision{main: 4}, revision{main: 2}, 2, nil},
+		{6, revision{}, revision{}, 0, ErrRevisionNotFound},
+	}
+	for i, tt := range tests {
+		rev, created, ver, err := ti.Get([]byte("foo"), tt.rev)
+		if err != tt.werr {
+			t.Errorf("#%d: err = %v, want %v", i, err, tt.werr)
+		}
+		if rev != tt.wrev {
+			t.Errorf("#%d: rev = %+v, want %+v", i, rev, tt.wrev)
+		}
+		if created != tt.wcreated {
+			t.Errorf("#%d: created = %+v, want %+v", i, created, tt.wcreated)
+		}
+		if ver != tt.wver {
+			t.Errorf("#%d: ver = %d, want %d", i, ver, tt.wver)
+		}
+	}
+}
+
+func TestIndexRange(t *testing.T) {
+	allKeys := [][]byte{[]byte("foo"), []byte("foo1"), []byte("foo2")}
+	allRevs := []revision{{main: 1}, {main: 2}, {main: 3}}
+
+	ti := newTreeIndex()
+	for i := range allKeys {
+		ti.Put(allKeys[i], allRevs[i])
+	}
+
+	atRev := int64(3)
+	tests := []struct {
+		key, end []byte
+		wkeys    [][]byte
+		wrevs    []revision
+	}{
+		// single key that not found
+		{
+			[]byte("bar"), nil, nil, nil,
+		},
+		// single key that found
+		{
+			[]byte("foo"), nil, allKeys[:1], allRevs[:1],
+		},
+		// range keys, return first member
+		{
+			[]byte("foo"), []byte("foo1"), allKeys[:1], allRevs[:1],
+		},
+		// range keys, return first two members
+		{
+			[]byte("foo"), []byte("foo2"), allKeys[:2], allRevs[:2],
+		},
+		// range keys, return all members
+		{
+			[]byte("foo"), []byte("fop"), allKeys, allRevs,
+		},
+		// range keys, return last two members
+		{
+			[]byte("foo1"), []byte("fop"), allKeys[1:], allRevs[1:],
+		},
+		// range keys, return last member
+		{
+			[]byte("foo2"), []byte("fop"), allKeys[2:], allRevs[2:],
+		},
+		// range keys, return nothing
+		{
+			[]byte("foo3"), []byte("fop"), nil, nil,
+		},
+	}
+	for i, tt := range tests {
+		keys, revs := ti.Range(tt.key, tt.end, atRev)
+		if !reflect.DeepEqual(keys, tt.wkeys) {
+			t.Errorf("#%d: keys = %+v, want %+v", i, keys, tt.wkeys)
+		}
+		if !reflect.DeepEqual(revs, tt.wrevs) {
+			t.Errorf("#%d: revs = %+v, want %+v", i, revs, tt.wrevs)
+		}
+	}
+}
+
+func TestIndexTombstone(t *testing.T) {
+	ti := newTreeIndex()
+	ti.Put([]byte("foo"), revision{main: 1})
+
+	err := ti.Tombstone([]byte("foo"), revision{main: 2})
+	if err != nil {
+		t.Errorf("tombstone error = %v, want nil", err)
+	}
+
+	_, _, _, err = ti.Get([]byte("foo"), 2)
+	if err != ErrRevisionNotFound {
+		t.Errorf("get error = %v, want nil", err)
+	}
+	err = ti.Tombstone([]byte("foo"), revision{main: 3})
+	if err != ErrRevisionNotFound {
+		t.Errorf("tombstone error = %v, want %v", err, ErrRevisionNotFound)
+	}
+}
+
+func TestIndexRangeSince(t *testing.T) {
+	allKeys := [][]byte{[]byte("foo"), []byte("foo1"), []byte("foo2"), []byte("foo2"), []byte("foo1"), []byte("foo")}
+	allRevs := []revision{{main: 1}, {main: 2}, {main: 3}, {main: 4}, {main: 5}, {main: 6}}
+
+	ti := newTreeIndex()
+	for i := range allKeys {
+		ti.Put(allKeys[i], allRevs[i])
+	}
+
+	atRev := int64(1)
+	tests := []struct {
+		key, end []byte
+		wrevs    []revision
+	}{
+		// single key that not found
+		{
+			[]byte("bar"), nil, nil,
+		},
+		// single key that found
+		{
+			[]byte("foo"), nil, []revision{{main: 1}, {main: 6}},
+		},
+		// range keys, return first member
+		{
+			[]byte("foo"), []byte("foo1"), []revision{{main: 1}, {main: 6}},
+		},
+		// range keys, return first two members
+		{
+			[]byte("foo"), []byte("foo2"), []revision{{main: 1}, {main: 2}, {main: 5}, {main: 6}},
+		},
+		// range keys, return all members
+		{
+			[]byte("foo"), []byte("fop"), allRevs,
+		},
+		// range keys, return last two members
+		{
+			[]byte("foo1"), []byte("fop"), []revision{{main: 2}, {main: 3}, {main: 4}, {main: 5}},
+		},
+		// range keys, return last member
+		{
+			[]byte("foo2"), []byte("fop"), []revision{{main: 3}, {main: 4}},
+		},
+		// range keys, return nothing
+		{
+			[]byte("foo3"), []byte("fop"), nil,
+		},
+	}
+	for i, tt := range tests {
+		revs := ti.RangeSince(tt.key, tt.end, atRev)
+		if !reflect.DeepEqual(revs, tt.wrevs) {
+			t.Errorf("#%d: revs = %+v, want %+v", i, revs, tt.wrevs)
+		}
+	}
+}
+
+func TestIndexCompact(t *testing.T) {
+	maxRev := int64(20)
+	tests := []struct {
+		key     []byte
+		remove  bool
+		rev     revision
+		created revision
+		ver     int64
+	}{
+		{[]byte("foo"), false, revision{main: 1}, revision{main: 1}, 1},
+		{[]byte("foo1"), false, revision{main: 2}, revision{main: 2}, 1},
+		{[]byte("foo2"), false, revision{main: 3}, revision{main: 3}, 1},
+		{[]byte("foo2"), false, revision{main: 4}, revision{main: 3}, 2},
+		{[]byte("foo"), false, revision{main: 5}, revision{main: 1}, 2},
+		{[]byte("foo1"), false, revision{main: 6}, revision{main: 2}, 2},
+		{[]byte("foo1"), true, revision{main: 7}, revision{}, 0},
+		{[]byte("foo2"), true, revision{main: 8}, revision{}, 0},
+		{[]byte("foo"), true, revision{main: 9}, revision{}, 0},
+		{[]byte("foo"), false, revision{10, 0}, revision{10, 0}, 1},
+		{[]byte("foo1"), false, revision{10, 1}, revision{10, 1}, 1},
+	}
+
+	// Continuous Compact
+	ti := newTreeIndex()
+	for _, tt := range tests {
+		if tt.remove {
+			ti.Tombstone(tt.key, tt.rev)
+		} else {
+			ti.Put(tt.key, tt.rev)
+		}
+	}
+	for i := int64(1); i < maxRev; i++ {
+		am := ti.Compact(i)
+
+		wti := newTreeIndex()
+		for _, tt := range tests {
+			if _, ok := am[tt.rev]; ok || tt.rev.GreaterThan(revision{main: i}) {
+				if tt.remove {
+					wti.Tombstone(tt.key, tt.rev)
+				} else {
+					wti.Restore(tt.key, tt.created, tt.rev, tt.ver)
+				}
+			}
+		}
+		if !ti.Equal(wti) {
+			t.Errorf("#%d: not equal ti", i)
+		}
+	}
+
+	// Once Compact
+	for i := int64(1); i < maxRev; i++ {
+		ti := newTreeIndex()
+		for _, tt := range tests {
+			if tt.remove {
+				ti.Tombstone(tt.key, tt.rev)
+			} else {
+				ti.Put(tt.key, tt.rev)
+			}
+		}
+		am := ti.Compact(i)
+
+		wti := newTreeIndex()
+		for _, tt := range tests {
+			if _, ok := am[tt.rev]; ok || tt.rev.GreaterThan(revision{main: i}) {
+				if tt.remove {
+					wti.Tombstone(tt.key, tt.rev)
+				} else {
+					wti.Restore(tt.key, tt.created, tt.rev, tt.ver)
+				}
+			}
+		}
+		if !ti.Equal(wti) {
+			t.Errorf("#%d: not equal ti", i)
+		}
+	}
+}
+
+func TestIndexRestore(t *testing.T) {
+	key := []byte("foo")
+
+	tests := []struct {
+		created  revision
+		modified revision
+		ver      int64
+	}{
+		{revision{1, 0}, revision{1, 0}, 1},
+		{revision{1, 0}, revision{1, 1}, 2},
+		{revision{1, 0}, revision{2, 0}, 3},
+	}
+
+	// Continuous Restore
+	ti := newTreeIndex()
+	for i, tt := range tests {
+		ti.Restore(key, tt.created, tt.modified, tt.ver)
+
+		modified, created, ver, err := ti.Get(key, tt.modified.main)
+		if modified != tt.modified {
+			t.Errorf("#%d: modified = %v, want %v", i, modified, tt.modified)
+		}
+		if created != tt.created {
+			t.Errorf("#%d: created = %v, want %v", i, created, tt.created)
+		}
+		if ver != tt.ver {
+			t.Errorf("#%d: ver = %d, want %d", i, ver, tt.ver)
+		}
+		if err != nil {
+			t.Errorf("#%d: err = %v, want nil", i, err)
+		}
+	}
+
+	// Once Restore
+	for i, tt := range tests {
+		ti := newTreeIndex()
+		ti.Restore(key, tt.created, tt.modified, tt.ver)
+
+		modified, created, ver, err := ti.Get(key, tt.modified.main)
+		if modified != tt.modified {
+			t.Errorf("#%d: modified = %v, want %v", i, modified, tt.modified)
+		}
+		if created != tt.created {
+			t.Errorf("#%d: created = %v, want %v", i, created, tt.created)
+		}
+		if ver != tt.ver {
+			t.Errorf("#%d: ver = %d, want %d", i, ver, tt.ver)
+		}
+		if err != nil {
+			t.Errorf("#%d: err = %v, want nil", i, err)
+		}
+	}
+}
diff --git a/mvcc/key_index.go b/mvcc/key_index.go
new file mode 100644
index 0000000..c2b2aee
--- /dev/null
+++ b/mvcc/key_index.go
@@ -0,0 +1,334 @@
+// Copyright 2015 CoreOS, Inc.
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//     http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+package mvcc
+
+import (
+	"bytes"
+	"errors"
+	"fmt"
+	"log"
+
+	"github.com/google/btree"
+)
+
+var (
+	ErrRevisionNotFound = errors.New("stroage: revision not found")
+)
+
+// keyIndex stores the revisions of a key in the backend.
+// Each keyIndex has at least one key generation.
+// Each generation might have several key versions.
+// Tombstone on a key appends an tombstone version at the end
+// of the current generation and creates a new empty generation.
+// Each version of a key has an index pointing to the backend.
+//
+// For example: put(1.0);put(2.0);tombstone(3.0);put(4.0);tombstone(5.0) on key "foo"
+// generate a keyIndex:
+// key:     "foo"
+// rev: 5
+// generations:
+//    {empty}
+//    {4.0, 5.0(t)}
+//    {1.0, 2.0, 3.0(t)}
+//
+// Compact a keyIndex removes the versions with smaller or equal to
+// rev except the largest one. If the generation becomes empty
+// during compaction, it will be removed. if all the generations get
+// removed, the keyIndex should be removed.
+
+// For example:
+// compact(2) on the previous example
+// generations:
+//    {empty}
+//    {4.0, 5.0(t)}
+//    {2.0, 3.0(t)}
+//
+// compact(4)
+// generations:
+//    {empty}
+//    {4.0, 5.0(t)}
+//
+// compact(5):
+// generations:
+//    {empty} -> key SHOULD be removed.
+//
+// compact(6):
+// generations:
+//    {empty} -> key SHOULD be removed.
+type keyIndex struct {
+	key         []byte
+	modified    revision // the main rev of the last modification
+	generations []generation
+}
+
+// put puts a revision to the keyIndex.
+func (ki *keyIndex) put(main int64, sub int64) {
+	rev := revision{main: main, sub: sub}
+
+	if !rev.GreaterThan(ki.modified) {
+		log.Panicf("store.keyindex: put with unexpected smaller revision [%v / %v]", rev, ki.modified)
+	}
+	if len(ki.generations) == 0 {
+		ki.generations = append(ki.generations, generation{})
+	}
+	g := &ki.generations[len(ki.generations)-1]
+	if len(g.revs) == 0 { // create a new key
+		keysGauge.Inc()
+		g.created = rev
+	}
+	g.revs = append(g.revs, rev)
+	g.ver++
+	ki.modified = rev
+}
+
+func (ki *keyIndex) restore(created, modified revision, ver int64) {
+	if len(ki.generations) != 0 {
+		log.Panicf("store.keyindex: cannot restore non-empty keyIndex")
+	}
+
+	ki.modified = modified
+	g := generation{created: created, ver: ver, revs: []revision{modified}}
+	ki.generations = append(ki.generations, g)
+	keysGauge.Inc()
+}
+
+// tombstone puts a revision, pointing to a tombstone, to the keyIndex.
+// It also creates a new empty generation in the keyIndex.
+// It returns ErrRevisionNotFound when tombstone on an empty generation.
+func (ki *keyIndex) tombstone(main int64, sub int64) error {
+	if ki.isEmpty() {
+		log.Panicf("store.keyindex: unexpected tombstone on empty keyIndex %s", string(ki.key))
+	}
+	if ki.generations[len(ki.generations)-1].isEmpty() {
+		return ErrRevisionNotFound
+	}
+	ki.put(main, sub)
+	ki.generations = append(ki.generations, generation{})
+	keysGauge.Dec()
+	return nil
+}
+
+// get gets the modified, created revision and version of the key that satisfies the given atRev.
+// Rev must be higher than or equal to the given atRev.
+func (ki *keyIndex) get(atRev int64) (modified, created revision, ver int64, err error) {
+	if ki.isEmpty() {
+		log.Panicf("store.keyindex: unexpected get on empty keyIndex %s", string(ki.key))
+	}
+	g := ki.findGeneration(atRev)
+	if g.isEmpty() {
+		return revision{}, revision{}, 0, ErrRevisionNotFound
+	}
+
+	n := g.walk(func(rev revision) bool { return rev.main > atRev })
+	if n != -1 {
+		return g.revs[n], g.created, g.ver - int64(len(g.revs)-n-1), nil
+	}
+
+	return revision{}, revision{}, 0, ErrRevisionNotFound
+}
+
+// since returns revisions since the given rev. Only the revision with the
+// largest sub revision will be returned if multiple revisions have the same
+// main revision.
+func (ki *keyIndex) since(rev int64) []revision {
+	if ki.isEmpty() {
+		log.Panicf("store.keyindex: unexpected get on empty keyIndex %s", string(ki.key))
+	}
+	since := revision{rev, 0}
+	var gi int
+	// find the generations to start checking
+	for gi = len(ki.generations) - 1; gi > 0; gi-- {
+		g := ki.generations[gi]
+		if g.isEmpty() {
+			continue
+		}
+		if since.GreaterThan(g.created) {
+			break
+		}
+	}
+
+	var revs []revision
+	var last int64
+	for ; gi < len(ki.generations); gi++ {
+		for _, r := range ki.generations[gi].revs {
+			if since.GreaterThan(r) {
+				continue
+			}
+			if r.main == last {
+				// replace the revision with a new one that has higher sub value,
+				// because the original one should not be seen by external
+				revs[len(revs)-1] = r
+				continue
+			}
+			revs = append(revs, r)
+			last = r.main
+		}
+	}
+	return revs
+}
+
+// compact compacts a keyIndex by removing the versions with smaller or equal
+// revision than the given atRev except the largest one (If the largest one is
+// a tombstone, it will not be kept).
+// If a generation becomes empty during compaction, it will be removed.
+func (ki *keyIndex) compact(atRev int64, available map[revision]struct{}) {
+	if ki.isEmpty() {
+		log.Panicf("store.keyindex: unexpected compact on empty keyIndex %s", string(ki.key))
+	}
+
+	// walk until reaching the first revision that has an revision smaller or equal to
+	// the atRev.
+	// add it to the available map
+	f := func(rev revision) bool {
+		if rev.main <= atRev {
+			available[rev] = struct{}{}
+			return false
+		}
+		return true
+	}
+
+	i, g := 0, &ki.generations[0]
+	// find first generation includes atRev or created after atRev
+	for i < len(ki.generations)-1 {
+		if tomb := g.revs[len(g.revs)-1].main; tomb > atRev {
+			break
+		}
+		i++
+		g = &ki.generations[i]
+	}
+
+	if !g.isEmpty() {
+		n := g.walk(f)
+		// remove the previous contents.
+		if n != -1 {
+			g.revs = g.revs[n:]
+		}
+		// remove any tombstone
+		if len(g.revs) == 1 && i != len(ki.generations)-1 {
+			delete(available, g.revs[0])
+			i++
+		}
+	}
+	// remove the previous generations.
+	ki.generations = ki.generations[i:]
+	return
+}
+
+func (ki *keyIndex) isEmpty() bool {
+	return len(ki.generations) == 1 && ki.generations[0].isEmpty()
+}
+
+// findGeneration finds out the generation of the keyIndex that the
+// given rev belongs to. If the given rev is at the gap of two generations,
+// which means that the key does not exist at the given rev, it returns nil.
+func (ki *keyIndex) findGeneration(rev int64) *generation {
+	lastg := len(ki.generations) - 1
+	cg := lastg
+
+	for cg >= 0 {
+		if len(ki.generations[cg].revs) == 0 {
+			cg--
+			continue
+		}
+		g := ki.generations[cg]
+		if cg != lastg {
+			if tomb := g.revs[len(g.revs)-1].main; tomb <= rev {
+				return nil
+			}
+		}
+		if g.revs[0].main <= rev {
+			return &ki.generations[cg]
+		}
+		cg--
+	}
+	return nil
+}
+
+func (a *keyIndex) Less(b btree.Item) bool {
+	return bytes.Compare(a.key, b.(*keyIndex).key) == -1
+}
+
+func (a *keyIndex) equal(b *keyIndex) bool {
+	if !bytes.Equal(a.key, b.key) {
+		return false
+	}
+	if a.modified != b.modified {
+		return false
+	}
+	if len(a.generations) != len(b.generations) {
+		return false
+	}
+	for i := range a.generations {
+		ag, bg := a.generations[i], b.generations[i]
+		if !ag.equal(bg) {
+			return false
+		}
+	}
+	return true
+}
+
+func (ki *keyIndex) String() string {
+	var s string
+	for _, g := range ki.generations {
+		s += g.String()
+	}
+	return s
+}
+
+// generation contains multiple revisions of a key.
+type generation struct {
+	ver     int64
+	created revision // when the generation is created (put in first revision).
+	revs    []revision
+}
+
+func (g *generation) isEmpty() bool { return g == nil || len(g.revs) == 0 }
+
+// walk walks through the revisions in the generation in descending order.
+// It passes the revision to the given function.
+// walk returns until: 1. it finishes walking all pairs 2. the function returns false.
+// walk returns the position at where it stopped. If it stopped after
+// finishing walking, -1 will be returned.
+func (g *generation) walk(f func(rev revision) bool) int {
+	l := len(g.revs)
+	for i := range g.revs {
+		ok := f(g.revs[l-i-1])
+		if !ok {
+			return l - i - 1
+		}
+	}
+	return -1
+}
+
+func (g *generation) String() string {
+	return fmt.Sprintf("g: created[%d] ver[%d], revs %#v\n", g.created, g.ver, g.revs)
+}
+
+func (a generation) equal(b generation) bool {
+	if a.ver != b.ver {
+		return false
+	}
+	if len(a.revs) != len(b.revs) {
+		return false
+	}
+
+	for i := range a.revs {
+		ar, br := a.revs[i], b.revs[i]
+		if ar != br {
+			return false
+		}
+	}
+	return true
+}
diff --git a/mvcc/key_index_test.go b/mvcc/key_index_test.go
new file mode 100644
index 0000000..7ee4afb
--- /dev/null
+++ b/mvcc/key_index_test.go
@@ -0,0 +1,654 @@
+// Copyright 2015 CoreOS, Inc.
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//     http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+package mvcc
+
+import (
+	"reflect"
+	"testing"
+)
+
+func TestKeyIndexGet(t *testing.T) {
+	// key: "foo"
+	// rev: 16
+	// generations:
+	//    {empty}
+	//    {{14, 0}[1], {14, 1}[2], {16, 0}(t)[3]}
+	//    {{8, 0}[1], {10, 0}[2], {12, 0}(t)[3]}
+	//    {{2, 0}[1], {4, 0}[2], {6, 0}(t)[3]}
+	ki := newTestKeyIndex()
+	ki.compact(4, make(map[revision]struct{}))
+
+	tests := []struct {
+		rev int64
+
+		wmod   revision
+		wcreat revision
+		wver   int64
+		werr   error
+	}{
+		{17, revision{}, revision{}, 0, ErrRevisionNotFound},
+		{16, revision{}, revision{}, 0, ErrRevisionNotFound},
+
+		// get on generation 3
+		{15, revision{14, 1}, revision{14, 0}, 2, nil},
+		{14, revision{14, 1}, revision{14, 0}, 2, nil},
+
+		{13, revision{}, revision{}, 0, ErrRevisionNotFound},
+		{12, revision{}, revision{}, 0, ErrRevisionNotFound},
+
+		// get on generation 2
+		{11, revision{10, 0}, revision{8, 0}, 2, nil},
+		{10, revision{10, 0}, revision{8, 0}, 2, nil},
+		{9, revision{8, 0}, revision{8, 0}, 1, nil},
+		{8, revision{8, 0}, revision{8, 0}, 1, nil},
+
+		{7, revision{}, revision{}, 0, ErrRevisionNotFound},
+		{6, revision{}, revision{}, 0, ErrRevisionNotFound},
+
+		// get on generation 1
+		{5, revision{4, 0}, revision{2, 0}, 2, nil},
+		{4, revision{4, 0}, revision{2, 0}, 2, nil},
+
+		{3, revision{}, revision{}, 0, ErrRevisionNotFound},
+		{2, revision{}, revision{}, 0, ErrRevisionNotFound},
+		{1, revision{}, revision{}, 0, ErrRevisionNotFound},
+		{0, revision{}, revision{}, 0, ErrRevisionNotFound},
+	}
+
+	for i, tt := range tests {
+		mod, creat, ver, err := ki.get(tt.rev)
+		if err != tt.werr {
+			t.Errorf("#%d: err = %v, want %v", i, err, tt.werr)
+		}
+		if mod != tt.wmod {
+			t.Errorf("#%d: modified = %+v, want %+v", i, mod, tt.wmod)
+		}
+		if creat != tt.wcreat {
+			t.Errorf("#%d: created = %+v, want %+v", i, creat, tt.wcreat)
+		}
+		if ver != tt.wver {
+			t.Errorf("#%d: version = %d, want %d", i, ver, tt.wver)
+		}
+	}
+}
+
+func TestKeyIndexSince(t *testing.T) {
+	ki := newTestKeyIndex()
+	ki.compact(4, make(map[revision]struct{}))
+
+	allRevs := []revision{{4, 0}, {6, 0}, {8, 0}, {10, 0}, {12, 0}, {14, 1}, {16, 0}}
+	tests := []struct {
+		rev int64
+
+		wrevs []revision
+	}{
+		{17, nil},
+		{16, allRevs[6:]},
+		{15, allRevs[6:]},
+		{14, allRevs[5:]},
+		{13, allRevs[5:]},
+		{12, allRevs[4:]},
+		{11, allRevs[4:]},
+		{10, allRevs[3:]},
+		{9, allRevs[3:]},
+		{8, allRevs[2:]},
+		{7, allRevs[2:]},
+		{6, allRevs[1:]},
+		{5, allRevs[1:]},
+		{4, allRevs},
+		{3, allRevs},
+		{2, allRevs},
+		{1, allRevs},
+		{0, allRevs},
+	}
+
+	for i, tt := range tests {
+		revs := ki.since(tt.rev)
+		if !reflect.DeepEqual(revs, tt.wrevs) {
+			t.Errorf("#%d: revs = %+v, want %+v", i, revs, tt.wrevs)
+		}
+	}
+}
+
+func TestKeyIndexPut(t *testing.T) {
+	ki := &keyIndex{key: []byte("foo")}
+	ki.put(5, 0)
+
+	wki := &keyIndex{
+		key:         []byte("foo"),
+		modified:    revision{5, 0},
+		generations: []generation{{created: revision{5, 0}, ver: 1, revs: []revision{{main: 5}}}},
+	}
+	if !reflect.DeepEqual(ki, wki) {
+		t.Errorf("ki = %+v, want %+v", ki, wki)
+	}
+
+	ki.put(7, 0)
+
+	wki = &keyIndex{
+		key:         []byte("foo"),
+		modified:    revision{7, 0},
+		generations: []generation{{created: revision{5, 0}, ver: 2, revs: []revision{{main: 5}, {main: 7}}}},
+	}
+	if !reflect.DeepEqual(ki, wki) {
+		t.Errorf("ki = %+v, want %+v", ki, wki)
+	}
+}
+
+func TestKeyIndexRestore(t *testing.T) {
+	ki := &keyIndex{key: []byte("foo")}
+	ki.restore(revision{5, 0}, revision{7, 0}, 2)
+
+	wki := &keyIndex{
+		key:         []byte("foo"),
+		modified:    revision{7, 0},
+		generations: []generation{{created: revision{5, 0}, ver: 2, revs: []revision{{main: 7}}}},
+	}
+	if !reflect.DeepEqual(ki, wki) {
+		t.Errorf("ki = %+v, want %+v", ki, wki)
+	}
+}
+
+func TestKeyIndexTombstone(t *testing.T) {
+	ki := &keyIndex{key: []byte("foo")}
+	ki.put(5, 0)
+
+	err := ki.tombstone(7, 0)
+	if err != nil {
+		t.Errorf("unexpected tombstone error: %v", err)
+	}
+
+	wki := &keyIndex{
+		key:         []byte("foo"),
+		modified:    revision{7, 0},
+		generations: []generation{{created: revision{5, 0}, ver: 2, revs: []revision{{main: 5}, {main: 7}}}, {}},
+	}
+	if !reflect.DeepEqual(ki, wki) {
+		t.Errorf("ki = %+v, want %+v", ki, wki)
+	}
+
+	ki.put(8, 0)
+	ki.put(9, 0)
+	err = ki.tombstone(15, 0)
+	if err != nil {
+		t.Errorf("unexpected tombstone error: %v", err)
+	}
+
+	wki = &keyIndex{
+		key:      []byte("foo"),
+		modified: revision{15, 0},
+		generations: []generation{
+			{created: revision{5, 0}, ver: 2, revs: []revision{{main: 5}, {main: 7}}},
+			{created: revision{8, 0}, ver: 3, revs: []revision{{main: 8}, {main: 9}, {main: 15}}},
+			{},
+		},
+	}
+	if !reflect.DeepEqual(ki, wki) {
+		t.Errorf("ki = %+v, want %+v", ki, wki)
+	}
+
+	err = ki.tombstone(16, 0)
+	if err != ErrRevisionNotFound {
+		t.Errorf("tombstone error = %v, want %v", err, ErrRevisionNotFound)
+	}
+}
+
+func TestKeyIndexCompact(t *testing.T) {
+	tests := []struct {
+		compact int64
+
+		wki *keyIndex
+		wam map[revision]struct{}
+	}{
+		{
+			1,
+			&keyIndex{
+				key:      []byte("foo"),
+				modified: revision{16, 0},
+				generations: []generation{
+					{created: revision{2, 0}, ver: 3, revs: []revision{{main: 2}, {main: 4}, {main: 6}}},
+					{created: revision{8, 0}, ver: 3, revs: []revision{{main: 8}, {main: 10}, {main: 12}}},
+					{created: revision{14, 0}, ver: 3, revs: []revision{{main: 14}, {main: 14, sub: 1}, {main: 16}}},
+					{},
+				},
+			},
+			map[revision]struct{}{},
+		},
+		{
+			2,
+			&keyIndex{
+				key:      []byte("foo"),
+				modified: revision{16, 0},
+				generations: []generation{
+					{created: revision{2, 0}, ver: 3, revs: []revision{{main: 2}, {main: 4}, {main: 6}}},
+					{created: revision{8, 0}, ver: 3, revs: []revision{{main: 8}, {main: 10}, {main: 12}}},
+					{created: revision{14, 0}, ver: 3, revs: []revision{{main: 14}, {main: 14, sub: 1}, {main: 16}}},
+					{},
+				},
+			},
+			map[revision]struct{}{
+				revision{main: 2}: {},
+			},
+		},
+		{
+			3,
+			&keyIndex{
+				key:      []byte("foo"),
+				modified: revision{16, 0},
+				generations: []generation{
+					{created: revision{2, 0}, ver: 3, revs: []revision{{main: 2}, {main: 4}, {main: 6}}},
+					{created: revision{8, 0}, ver: 3, revs: []revision{{main: 8}, {main: 10}, {main: 12}}},
+					{created: revision{14, 0}, ver: 3, revs: []revision{{main: 14}, {main: 14, sub: 1}, {main: 16}}},
+					{},
+				},
+			},
+			map[revision]struct{}{
+				revision{main: 2}: {},
+			},
+		},
+		{
+			4,
+			&keyIndex{
+				key:      []byte("foo"),
+				modified: revision{16, 0},
+				generations: []generation{
+					{created: revision{2, 0}, ver: 3, revs: []revision{{main: 4}, {main: 6}}},
+					{created: revision{8, 0}, ver: 3, revs: []revision{{main: 8}, {main: 10}, {main: 12}}},
+					{created: revision{14, 0}, ver: 3, revs: []revision{{main: 14}, {main: 14, sub: 1}, {main: 16}}},
+					{},
+				},
+			},
+			map[revision]struct{}{
+				revision{main: 4}: {},
+			},
+		},
+		{
+			5,
+			&keyIndex{
+				key:      []byte("foo"),
+				modified: revision{16, 0},
+				generations: []generation{
+					{created: revision{2, 0}, ver: 3, revs: []revision{{main: 4}, {main: 6}}},
+					{created: revision{8, 0}, ver: 3, revs: []revision{{main: 8}, {main: 10}, {main: 12}}},
+					{created: revision{14, 0}, ver: 3, revs: []revision{{main: 14}, {main: 14, sub: 1}, {main: 16}}},
+					{},
+				},
+			},
+			map[revision]struct{}{
+				revision{main: 4}: {},
+			},
+		},
+		{
+			6,
+			&keyIndex{
+				key:      []byte("foo"),
+				modified: revision{16, 0},
+				generations: []generation{
+					{created: revision{8, 0}, ver: 3, revs: []revision{{main: 8}, {main: 10}, {main: 12}}},
+					{created: revision{14, 0}, ver: 3, revs: []revision{{main: 14}, {main: 14, sub: 1}, {main: 16}}},
+					{},
+				},
+			},
+			map[revision]struct{}{},
+		},
+		{
+			7,
+			&keyIndex{
+				key:      []byte("foo"),
+				modified: revision{16, 0},
+				generations: []generation{
+					{created: revision{8, 0}, ver: 3, revs: []revision{{main: 8}, {main: 10}, {main: 12}}},
+					{created: revision{14, 0}, ver: 3, revs: []revision{{main: 14}, {main: 14, sub: 1}, {main: 16}}},
+					{},
+				},
+			},
+			map[revision]struct{}{},
+		},
+		{
+			8,
+			&keyIndex{
+				key:      []byte("foo"),
+				modified: revision{16, 0},
+				generations: []generation{
+					{created: revision{8, 0}, ver: 3, revs: []revision{{main: 8}, {main: 10}, {main: 12}}},
+					{created: revision{14, 0}, ver: 3, revs: []revision{{main: 14}, {main: 14, sub: 1}, {main: 16}}},
+					{},
+				},
+			},
+			map[revision]struct{}{
+				revision{main: 8}: {},
+			},
+		},
+		{
+			9,
+			&keyIndex{
+				key:      []byte("foo"),
+				modified: revision{16, 0},
+				generations: []generation{
+					{created: revision{8, 0}, ver: 3, revs: []revision{{main: 8}, {main: 10}, {main: 12}}},
+					{created: revision{14, 0}, ver: 3, revs: []revision{{main: 14}, {main: 14, sub: 1}, {main: 16}}},
+					{},
+				},
+			},
+			map[revision]struct{}{
+				revision{main: 8}: {},
+			},
+		},
+		{
+			10,
+			&keyIndex{
+				key:      []byte("foo"),
+				modified: revision{16, 0},
+				generations: []generation{
+					{created: revision{8, 0}, ver: 3, revs: []revision{{main: 10}, {main: 12}}},
+					{created: revision{14, 0}, ver: 3, revs: []revision{{main: 14}, {main: 14, sub: 1}, {main: 16}}},
+					{},
+				},
+			},
+			map[revision]struct{}{
+				revision{main: 10}: {},
+			},
+		},
+		{
+			11,
+			&keyIndex{
+				key:      []byte("foo"),
+				modified: revision{16, 0},
+				generations: []generation{
+					{created: revision{8, 0}, ver: 3, revs: []revision{{main: 10}, {main: 12}}},
+					{created: revision{14, 0}, ver: 3, revs: []revision{{main: 14}, {main: 14, sub: 1}, {main: 16}}},
+					{},
+				},
+			},
+			map[revision]struct{}{
+				revision{main: 10}: {},
+			},
+		},
+		{
+			12,
+			&keyIndex{
+				key:      []byte("foo"),
+				modified: revision{16, 0},
+				generations: []generation{
+					{created: revision{14, 0}, ver: 3, revs: []revision{{main: 14}, {main: 14, sub: 1}, {main: 16}}},
+					{},
+				},
+			},
+			map[revision]struct{}{},
+		},
+		{
+			13,
+			&keyIndex{
+				key:      []byte("foo"),
+				modified: revision{16, 0},
+				generations: []generation{
+					{created: revision{14, 0}, ver: 3, revs: []revision{{main: 14}, {main: 14, sub: 1}, {main: 16}}},
+					{},
+				},
+			},
+			map[revision]struct{}{},
+		},
+		{
+			14,
+			&keyIndex{
+				key:      []byte("foo"),
+				modified: revision{16, 0},
+				generations: []generation{
+					{created: revision{14, 0}, ver: 3, revs: []revision{{main: 14, sub: 1}, {main: 16}}},
+					{},
+				},
+			},
+			map[revision]struct{}{
+				revision{main: 14, sub: 1}: {},
+			},
+		},
+		{
+			15,
+			&keyIndex{
+				key:      []byte("foo"),
+				modified: revision{16, 0},
+				generations: []generation{
+					{created: revision{14, 0}, ver: 3, revs: []revision{{main: 14, sub: 1}, {main: 16}}},
+					{},
+				},
+			},
+			map[revision]struct{}{
+				revision{main: 14, sub: 1}: {},
+			},
+		},
+		{
+			16,
+			&keyIndex{
+				key:      []byte("foo"),
+				modified: revision{16, 0},
+				generations: []generation{
+					{},
+				},
+			},
+			map[revision]struct{}{},
+		},
+	}
+
+	// Continuous Compaction
+	ki := newTestKeyIndex()
+	for i, tt := range tests {
+		am := make(map[revision]struct{})
+		ki.compact(tt.compact, am)
+		if !reflect.DeepEqual(ki, tt.wki) {
+			t.Errorf("#%d: ki = %+v, want %+v", i, ki, tt.wki)
+		}
+		if !reflect.DeepEqual(am, tt.wam) {
+			t.Errorf("#%d: am = %+v, want %+v", i, am, tt.wam)
+		}
+	}
+
+	// Jump Compaction
+	ki = newTestKeyIndex()
+	for i, tt := range tests {
+		if (i%2 == 0 && i < 6) || (i%2 == 1 && i > 6) {
+			am := make(map[revision]struct{})
+			ki.compact(tt.compact, am)
+			if !reflect.DeepEqual(ki, tt.wki) {
+				t.Errorf("#%d: ki = %+v, want %+v", i, ki, tt.wki)
+			}
+			if !reflect.DeepEqual(am, tt.wam) {
+				t.Errorf("#%d: am = %+v, want %+v", i, am, tt.wam)
+			}
+		}
+	}
+
+	// Once Compaction
+	for i, tt := range tests {
+		ki := newTestKeyIndex()
+		am := make(map[revision]struct{})
+		ki.compact(tt.compact, am)
+		if !reflect.DeepEqual(ki, tt.wki) {
+			t.Errorf("#%d: ki = %+v, want %+v", i, ki, tt.wki)
+		}
+		if !reflect.DeepEqual(am, tt.wam) {
+			t.Errorf("#%d: am = %+v, want %+v", i, am, tt.wam)
+		}
+	}
+}
+
+// test that compact on version that higher than last modified version works well
+func TestKeyIndexCompactOnFurtherRev(t *testing.T) {
+	ki := &keyIndex{key: []byte("foo")}
+	ki.put(1, 0)
+	ki.put(2, 0)
+	am := make(map[revision]struct{})
+	ki.compact(3, am)
+
+	wki := &keyIndex{
+		key:      []byte("foo"),
+		modified: revision{2, 0},
+		generations: []generation{
+			{created: revision{1, 0}, ver: 2, revs: []revision{{main: 2}}},
+		},
+	}
+	wam := map[revision]struct{}{
+		revision{main: 2}: {},
+	}
+	if !reflect.DeepEqual(ki, wki) {
+		t.Errorf("ki = %+v, want %+v", ki, wki)
+	}
+	if !reflect.DeepEqual(am, wam) {
+		t.Errorf("am = %+v, want %+v", am, wam)
+	}
+}
+
+func TestKeyIndexIsEmpty(t *testing.T) {
+	tests := []struct {
+		ki *keyIndex
+		w  bool
+	}{
+		{
+			&keyIndex{
+				key:         []byte("foo"),
+				generations: []generation{{}},
+			},
+			true,
+		},
+		{
+			&keyIndex{
+				key:      []byte("foo"),
+				modified: revision{2, 0},
+				generations: []generation{
+					{created: revision{1, 0}, ver: 2, revs: []revision{{main: 2}}},
+				},
+			},
+			false,
+		},
+	}
+	for i, tt := range tests {
+		g := tt.ki.isEmpty()
+		if g != tt.w {
+			t.Errorf("#%d: isEmpty = %v, want %v", i, g, tt.w)
+		}
+	}
+}
+
+func TestKeyIndexFindGeneration(t *testing.T) {
+	ki := newTestKeyIndex()
+
+	tests := []struct {
+		rev int64
+		wg  *generation
+	}{
+		{0, nil},
+		{1, nil},
+		{2, &ki.generations[0]},
+		{3, &ki.generations[0]},
+		{4, &ki.generations[0]},
+		{5, &ki.generations[0]},
+		{6, nil},
+		{7, nil},
+		{8, &ki.generations[1]},
+		{9, &ki.generations[1]},
+		{10, &ki.generations[1]},
+		{11, &ki.generations[1]},
+		{12, nil},
+		{13, nil},
+	}
+	for i, tt := range tests {
+		g := ki.findGeneration(tt.rev)
+		if g != tt.wg {
+			t.Errorf("#%d: generation = %+v, want %+v", i, g, tt.wg)
+		}
+	}
+}
+
+func TestKeyIndexLess(t *testing.T) {
+	ki := &keyIndex{key: []byte("foo")}
+
+	tests := []struct {
+		ki *keyIndex
+		w  bool
+	}{
+		{&keyIndex{key: []byte("doo")}, false},
+		{&keyIndex{key: []byte("foo")}, false},
+		{&keyIndex{key: []byte("goo")}, true},
+	}
+	for i, tt := range tests {
+		g := ki.Less(tt.ki)
+		if g != tt.w {
+			t.Errorf("#%d: Less = %v, want %v", i, g, tt.w)
+		}
+	}
+}
+
+func TestGenerationIsEmpty(t *testing.T) {
+	tests := []struct {
+		g *generation
+		w bool
+	}{
+		{nil, true},
+		{&generation{}, true},
+		{&generation{revs: []revision{{main: 1}}}, false},
+	}
+	for i, tt := range tests {
+		g := tt.g.isEmpty()
+		if g != tt.w {
+			t.Errorf("#%d: isEmpty = %v, want %v", i, g, tt.w)
+		}
+	}
+}
+
+func TestGenerationWalk(t *testing.T) {
+	g := &generation{
+		ver:     3,
+		created: revision{2, 0},
+		revs:    []revision{{main: 2}, {main: 4}, {main: 6}},
+	}
+	tests := []struct {
+		f  func(rev revision) bool
+		wi int
+	}{
+		{func(rev revision) bool { return rev.main >= 7 }, 2},
+		{func(rev revision) bool { return rev.main >= 6 }, 1},
+		{func(rev revision) bool { return rev.main >= 5 }, 1},
+		{func(rev revision) bool { return rev.main >= 4 }, 0},
+		{func(rev revision) bool { return rev.main >= 3 }, 0},
+		{func(rev revision) bool { return rev.main >= 2 }, -1},
+	}
+	for i, tt := range tests {
+		idx := g.walk(tt.f)
+		if idx != tt.wi {
+			t.Errorf("#%d: index = %d, want %d", i, idx, tt.wi)
+		}
+	}
+}
+
+func newTestKeyIndex() *keyIndex {
+	// key: "foo"
+	// rev: 16
+	// generations:
+	//    {empty}
+	//    {{14, 0}[1], {14, 1}[2], {16, 0}(t)[3]}
+	//    {{8, 0}[1], {10, 0}[2], {12, 0}(t)[3]}
+	//    {{2, 0}[1], {4, 0}[2], {6, 0}(t)[3]}
+
+	ki := &keyIndex{key: []byte("foo")}
+	ki.put(2, 0)
+	ki.put(4, 0)
+	ki.tombstone(6, 0)
+	ki.put(8, 0)
+	ki.put(10, 0)
+	ki.tombstone(12, 0)
+	ki.put(14, 0)
+	ki.put(14, 1)
+	ki.tombstone(16, 0)
+	return ki
+}
diff --git a/mvcc/kv.go b/mvcc/kv.go
new file mode 100644
index 0000000..a8415a7
--- /dev/null
+++ b/mvcc/kv.go
@@ -0,0 +1,107 @@
+// Copyright 2015 CoreOS, Inc.
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//     http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+package mvcc
+
+import (
+	"github.com/coreos/etcd/lease"
+	"github.com/coreos/etcd/mvcc/backend"
+	"github.com/coreos/etcd/mvcc/mvccpb"
+)
+
+type KV interface {
+	// Rev returns the current revision of the KV.
+	Rev() int64
+
+	// FirstRev returns the first revision of the KV.
+	// After a compaction, the first revision increases to the compaction
+	// revision.
+	FirstRev() int64
+
+	// Range gets the keys in the range at rangeRev.
+	// The returned rev is the current revision of the KV when the operation is executed.
+	// If rangeRev <=0, range gets the keys at currentRev.
+	// If `end` is nil, the request returns the key.
+	// If `end` is not nil and not empty, it gets the keys in range [key, range_end).
+	// If `end` is not nil and empty, it gets the keys greater than or equal to key.
+	// Limit limits the number of keys returned.
+	// If the required rev is compacted, ErrCompacted will be returned.
+	Range(key, end []byte, limit, rangeRev int64) (kvs []mvccpb.KeyValue, rev int64, err error)
+
+	// Put puts the given key, value into the store. Put also takes additional argument lease to
+	// attach a lease to a key-value pair as meta-data. KV implementation does not validate the lease
+	// id.
+	// A put also increases the rev of the store, and generates one event in the event history.
+	// The returned rev is the current revision of the KV when the operation is executed.
+	Put(key, value []byte, lease lease.LeaseID) (rev int64)
+
+	// DeleteRange deletes the given range from the store.
+	// A deleteRange increases the rev of the store if any key in the range exists.
+	// The number of key deleted will be returned.
+	// The returned rev is the current revision of the KV when the operation is executed.
+	// It also generates one event for each key delete in the event history.
+	// if the `end` is nil, deleteRange deletes the key.
+	// if the `end` is not nil, deleteRange deletes the keys in range [key, range_end).
+	DeleteRange(key, end []byte) (n, rev int64)
+
+	// TxnBegin begins a txn. Only Txn prefixed operation can be executed, others will be blocked
+	// until txn ends. Only one on-going txn is allowed.
+	// TxnBegin returns an int64 txn ID.
+	// All txn prefixed operations with same txn ID will be done with the same rev.
+	TxnBegin() int64
+	// TxnEnd ends the on-going txn with txn ID. If the on-going txn ID is not matched, error is returned.
+	TxnEnd(txnID int64) error
+	// TxnRange returns the current revision of the KV when the operation is executed.
+	TxnRange(txnID int64, key, end []byte, limit, rangeRev int64) (kvs []mvccpb.KeyValue, rev int64, err error)
+	TxnPut(txnID int64, key, value []byte, lease lease.LeaseID) (rev int64, err error)
+	TxnDeleteRange(txnID int64, key, end []byte) (n, rev int64, err error)
+
+	// Compact frees all superseded keys with revisions less than rev.
+	Compact(rev int64) (<-chan struct{}, error)
+
+	// Hash retrieves the hash of KV state.
+	// This method is designed for consistency checking purpose.
+	Hash() (uint32, error)
+
+	// Commit commits txns into the underlying backend.
+	Commit()
+
+	// Restore restores the KV store from a backend.
+	Restore(b backend.Backend) error
+	Close() error
+}
+
+// WatchableKV is a KV that can be watched.
+type WatchableKV interface {
+	KV
+	Watchable
+}
+
+// Watchable is the interface that wraps the NewWatchStream function.
+type Watchable interface {
+	// NewWatchStream returns a WatchStream that can be used to
+	// watch events happened or happening on the KV.
+	NewWatchStream() WatchStream
+}
+
+// ConsistentWatchableKV is a WatchableKV that understands the consistency
+// algorithm and consistent index.
+// If the consistent index of executing entry is not larger than the
+// consistent index of ConsistentWatchableKV, all operations in
+// this entry are skipped and return empty response.
+type ConsistentWatchableKV interface {
+	WatchableKV
+	// ConsistentIndex returns the current consistent index of the KV.
+	ConsistentIndex() uint64
+}
diff --git a/mvcc/kv_test.go b/mvcc/kv_test.go
new file mode 100644
index 0000000..6c04584
--- /dev/null
+++ b/mvcc/kv_test.go
@@ -0,0 +1,839 @@
+// Copyright 2015 CoreOS, Inc.
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//     http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+package mvcc
+
+import (
+	"fmt"
+	"os"
+	"reflect"
+	"testing"
+	"time"
+
+	"github.com/coreos/etcd/lease"
+	"github.com/coreos/etcd/mvcc/backend"
+	"github.com/coreos/etcd/mvcc/mvccpb"
+	"github.com/coreos/etcd/pkg/testutil"
+)
+
+// Functional tests for features implemented in v3 store. It treats v3 store
+// as a black box, and tests it by feeding the input and validating the output.
+
+// TODO: add similar tests on operations in one txn/rev
+
+type (
+	rangeFunc       func(kv KV, key, end []byte, limit, rangeRev int64) ([]mvccpb.KeyValue, int64, error)
+	putFunc         func(kv KV, key, value []byte, lease lease.LeaseID) int64
+	deleteRangeFunc func(kv KV, key, end []byte) (n, rev int64)
+)
+
+var (
+	normalRangeFunc = func(kv KV, key, end []byte, limit, rangeRev int64) ([]mvccpb.KeyValue, int64, error) {
+		return kv.Range(key, end, limit, rangeRev)
+	}
+	txnRangeFunc = func(kv KV, key, end []byte, limit, rangeRev int64) ([]mvccpb.KeyValue, int64, error) {
+		id := kv.TxnBegin()
+		defer kv.TxnEnd(id)
+		return kv.TxnRange(id, key, end, limit, rangeRev)
+	}
+
+	normalPutFunc = func(kv KV, key, value []byte, lease lease.LeaseID) int64 {
+		return kv.Put(key, value, lease)
+	}
+	txnPutFunc = func(kv KV, key, value []byte, lease lease.LeaseID) int64 {
+		id := kv.TxnBegin()
+		defer kv.TxnEnd(id)
+		rev, err := kv.TxnPut(id, key, value, lease)
+		if err != nil {
+			panic("txn put error")
+		}
+		return rev
+	}
+
+	normalDeleteRangeFunc = func(kv KV, key, end []byte) (n, rev int64) {
+		return kv.DeleteRange(key, end)
+	}
+	txnDeleteRangeFunc = func(kv KV, key, end []byte) (n, rev int64) {
+		id := kv.TxnBegin()
+		defer kv.TxnEnd(id)
+		n, rev, err := kv.TxnDeleteRange(id, key, end)
+		if err != nil {
+			panic("txn delete error")
+		}
+		return n, rev
+	}
+)
+
+func TestKVRange(t *testing.T)    { testKVRange(t, normalRangeFunc) }
+func TestKVTxnRange(t *testing.T) { testKVRange(t, txnRangeFunc) }
+
+func testKVRange(t *testing.T, f rangeFunc) {
+	b, tmpPath := backend.NewDefaultTmpBackend()
+	s := NewStore(b, &lease.FakeLessor{}, nil)
+	defer cleanup(s, b, tmpPath)
+
+	kvs := put3TestKVs(s)
+
+	wrev := int64(4)
+	tests := []struct {
+		key, end []byte
+		wkvs     []mvccpb.KeyValue
+	}{
+		// get no keys
+		{
+			[]byte("doo"), []byte("foo"),
+			nil,
+		},
+		// get no keys when key == end
+		{
+			[]byte("foo"), []byte("foo"),
+			nil,
+		},
+		// get no keys when ranging single key
+		{
+			[]byte("doo"), nil,
+			nil,
+		},
+		// get all keys
+		{
+			[]byte("foo"), []byte("foo3"),
+			kvs,
+		},
+		// get partial keys
+		{
+			[]byte("foo"), []byte("foo1"),
+			kvs[:1],
+		},
+		// get single key
+		{
+			[]byte("foo"), nil,
+			kvs[:1],
+		},
+		// get entire keyspace
+		{
+			[]byte(""), []byte(""),
+			kvs,
+		},
+	}
+
+	for i, tt := range tests {
+		kvs, rev, err := f(s, tt.key, tt.end, 0, 0)
+		if err != nil {
+			t.Fatal(err)
+		}
+		if rev != wrev {
+			t.Errorf("#%d: rev = %d, want %d", i, rev, wrev)
+		}
+		if !reflect.DeepEqual(kvs, tt.wkvs) {
+			t.Errorf("#%d: kvs = %+v, want %+v", i, kvs, tt.wkvs)
+		}
+	}
+}
+
+func TestKVRangeRev(t *testing.T)    { testKVRangeRev(t, normalRangeFunc) }
+func TestKVTxnRangeRev(t *testing.T) { testKVRangeRev(t, normalRangeFunc) }
+
+func testKVRangeRev(t *testing.T, f rangeFunc) {
+	b, tmpPath := backend.NewDefaultTmpBackend()
+	s := NewStore(b, &lease.FakeLessor{}, nil)
+	defer cleanup(s, b, tmpPath)
+
+	kvs := put3TestKVs(s)
+
+	tests := []struct {
+		rev  int64
+		wrev int64
+		wkvs []mvccpb.KeyValue
+	}{
+		{-1, 4, kvs},
+		{0, 4, kvs},
+		{2, 4, kvs[:1]},
+		{3, 4, kvs[:2]},
+		{4, 4, kvs},
+	}
+
+	for i, tt := range tests {
+		kvs, rev, err := f(s, []byte("foo"), []byte("foo3"), 0, tt.rev)
+		if err != nil {
+			t.Fatal(err)
+		}
+		if rev != tt.wrev {
+			t.Errorf("#%d: rev = %d, want %d", i, rev, tt.wrev)
+		}
+		if !reflect.DeepEqual(kvs, tt.wkvs) {
+			t.Errorf("#%d: kvs = %+v, want %+v", i, kvs, tt.wkvs)
+		}
+	}
+}
+
+func TestKVRangeBadRev(t *testing.T)    { testKVRangeBadRev(t, normalRangeFunc) }
+func TestKVTxnRangeBadRev(t *testing.T) { testKVRangeBadRev(t, normalRangeFunc) }
+
+func testKVRangeBadRev(t *testing.T, f rangeFunc) {
+	b, tmpPath := backend.NewDefaultTmpBackend()
+	s := NewStore(b, &lease.FakeLessor{}, nil)
+	defer cleanup(s, b, tmpPath)
+
+	put3TestKVs(s)
+	if _, err := s.Compact(4); err != nil {
+		t.Fatalf("compact error (%v)", err)
+	}
+
+	tests := []struct {
+		rev  int64
+		werr error
+	}{
+		{-1, nil}, // <= 0 is most recent store
+		{0, nil},
+		{1, ErrCompacted},
+		{2, ErrCompacted},
+		{4, nil},
+		{5, ErrFutureRev},
+		{100, ErrFutureRev},
+	}
+	for i, tt := range tests {
+		_, _, err := f(s, []byte("foo"), []byte("foo3"), 0, tt.rev)
+		if err != tt.werr {
+			t.Errorf("#%d: error = %v, want %v", i, err, tt.werr)
+		}
+	}
+}
+
+func TestKVRangeLimit(t *testing.T)    { testKVRangeLimit(t, normalRangeFunc) }
+func TestKVTxnRangeLimit(t *testing.T) { testKVRangeLimit(t, txnRangeFunc) }
+
+func testKVRangeLimit(t *testing.T, f rangeFunc) {
+	b, tmpPath := backend.NewDefaultTmpBackend()
+	s := NewStore(b, &lease.FakeLessor{}, nil)
+	defer cleanup(s, b, tmpPath)
+
+	kvs := put3TestKVs(s)
+
+	wrev := int64(4)
+	tests := []struct {
+		limit int64
+		wkvs  []mvccpb.KeyValue
+	}{
+		// no limit
+		{-1, kvs},
+		// no limit
+		{0, kvs},
+		{1, kvs[:1]},
+		{2, kvs[:2]},
+		{3, kvs},
+		{100, kvs},
+	}
+	for i, tt := range tests {
+		kvs, rev, err := f(s, []byte("foo"), []byte("foo3"), tt.limit, 0)
+		if err != nil {
+			t.Fatalf("#%d: range error (%v)", i, err)
+		}
+		if !reflect.DeepEqual(kvs, tt.wkvs) {
+			t.Errorf("#%d: kvs = %+v, want %+v", i, kvs, tt.wkvs)
+		}
+		if rev != wrev {
+			t.Errorf("#%d: rev = %d, want %d", i, rev, wrev)
+		}
+	}
+}
+
+func TestKVPutMultipleTimes(t *testing.T)    { testKVPutMultipleTimes(t, normalPutFunc) }
+func TestKVTxnPutMultipleTimes(t *testing.T) { testKVPutMultipleTimes(t, txnPutFunc) }
+
+func testKVPutMultipleTimes(t *testing.T, f putFunc) {
+	b, tmpPath := backend.NewDefaultTmpBackend()
+	s := NewStore(b, &lease.FakeLessor{}, nil)
+	defer cleanup(s, b, tmpPath)
+
+	for i := 0; i < 10; i++ {
+		base := int64(i + 1)
+
+		rev := f(s, []byte("foo"), []byte("bar"), lease.LeaseID(base))
+		if rev != base+1 {
+			t.Errorf("#%d: rev = %d, want %d", i, rev, base+1)
+		}
+
+		kvs, _, err := s.Range([]byte("foo"), nil, 0, 0)
+		if err != nil {
+			t.Fatal(err)
+		}
+		wkvs := []mvccpb.KeyValue{
+			{Key: []byte("foo"), Value: []byte("bar"), CreateRevision: 2, ModRevision: base + 1, Version: base, Lease: base},
+		}
+		if !reflect.DeepEqual(kvs, wkvs) {
+			t.Errorf("#%d: kvs = %+v, want %+v", i, kvs, wkvs)
+		}
+	}
+}
+
+func TestKVDeleteRange(t *testing.T)    { testKVDeleteRange(t, normalDeleteRangeFunc) }
+func TestKVTxnDeleteRange(t *testing.T) { testKVDeleteRange(t, txnDeleteRangeFunc) }
+
+func testKVDeleteRange(t *testing.T, f deleteRangeFunc) {
+	tests := []struct {
+		key, end []byte
+
+		wrev int64
+		wN   int64
+	}{
+		{
+			[]byte("foo"), nil,
+			5, 1,
+		},
+		{
+			[]byte("foo"), []byte("foo1"),
+			5, 1,
+		},
+		{
+			[]byte("foo"), []byte("foo2"),
+			5, 2,
+		},
+		{
+			[]byte("foo"), []byte("foo3"),
+			5, 3,
+		},
+		{
+			[]byte("foo3"), []byte("foo8"),
+			4, 0,
+		},
+		{
+			[]byte("foo3"), nil,
+			4, 0,
+		},
+	}
+
+	for i, tt := range tests {
+		b, tmpPath := backend.NewDefaultTmpBackend()
+		s := NewStore(b, &lease.FakeLessor{}, nil)
+
+		s.Put([]byte("foo"), []byte("bar"), lease.NoLease)
+		s.Put([]byte("foo1"), []byte("bar1"), lease.NoLease)
+		s.Put([]byte("foo2"), []byte("bar2"), lease.NoLease)
+
+		n, rev := f(s, tt.key, tt.end)
+		if n != tt.wN || rev != tt.wrev {
+			t.Errorf("#%d: n = %d, rev = %d, want (%d, %d)", i, n, rev, tt.wN, tt.wrev)
+		}
+
+		cleanup(s, b, tmpPath)
+	}
+}
+
+func TestKVDeleteMultipleTimes(t *testing.T)    { testKVDeleteMultipleTimes(t, normalDeleteRangeFunc) }
+func TestKVTxnDeleteMultipleTimes(t *testing.T) { testKVDeleteMultipleTimes(t, txnDeleteRangeFunc) }
+
+func testKVDeleteMultipleTimes(t *testing.T, f deleteRangeFunc) {
+	b, tmpPath := backend.NewDefaultTmpBackend()
+	s := NewStore(b, &lease.FakeLessor{}, nil)
+	defer cleanup(s, b, tmpPath)
+
+	s.Put([]byte("foo"), []byte("bar"), lease.NoLease)
+
+	n, rev := f(s, []byte("foo"), nil)
+	if n != 1 || rev != 3 {
+		t.Fatalf("n = %d, rev = %d, want (%d, %d)", n, rev, 1, 3)
+	}
+
+	for i := 0; i < 10; i++ {
+		n, rev := f(s, []byte("foo"), nil)
+		if n != 0 || rev != 3 {
+			t.Fatalf("#%d: n = %d, rev = %d, want (%d, %d)", i, n, rev, 0, 3)
+		}
+	}
+}
+
+// test that range, put, delete on single key in sequence repeatedly works correctly.
+func TestKVOperationInSequence(t *testing.T) {
+	b, tmpPath := backend.NewDefaultTmpBackend()
+	s := NewStore(b, &lease.FakeLessor{}, nil)
+	defer cleanup(s, b, tmpPath)
+
+	for i := 0; i < 10; i++ {
+		base := int64(i*2 + 1)
+
+		// put foo
+		rev := s.Put([]byte("foo"), []byte("bar"), lease.NoLease)
+		if rev != base+1 {
+			t.Errorf("#%d: put rev = %d, want %d", i, rev, base+1)
+		}
+
+		kvs, rev, err := s.Range([]byte("foo"), nil, 0, base+1)
+		if err != nil {
+			t.Fatal(err)
+		}
+		wkvs := []mvccpb.KeyValue{
+			{Key: []byte("foo"), Value: []byte("bar"), CreateRevision: base + 1, ModRevision: base + 1, Version: 1, Lease: int64(lease.NoLease)},
+		}
+		if !reflect.DeepEqual(kvs, wkvs) {
+			t.Errorf("#%d: kvs = %+v, want %+v", i, kvs, wkvs)
+		}
+		if rev != base+1 {
+			t.Errorf("#%d: range rev = %d, want %d", i, rev, base+1)
+		}
+
+		// delete foo
+		n, rev := s.DeleteRange([]byte("foo"), nil)
+		if n != 1 || rev != base+2 {
+			t.Errorf("#%d: n = %d, rev = %d, want (%d, %d)", i, n, rev, 1, base+2)
+		}
+
+		kvs, rev, err = s.Range([]byte("foo"), nil, 0, base+2)
+		if err != nil {
+			t.Fatal(err)
+		}
+		if kvs != nil {
+			t.Errorf("#%d: kvs = %+v, want %+v", i, kvs, nil)
+		}
+		if rev != base+2 {
+			t.Errorf("#%d: range rev = %d, want %d", i, rev, base+2)
+		}
+	}
+}
+
+func TestKVTxnBlockNonTxnOperations(t *testing.T) {
+	b, tmpPath := backend.NewDefaultTmpBackend()
+	s := NewStore(b, &lease.FakeLessor{}, nil)
+
+	tests := []func(){
+		func() { s.Range([]byte("foo"), nil, 0, 0) },
+		func() { s.Put([]byte("foo"), nil, lease.NoLease) },
+		func() { s.DeleteRange([]byte("foo"), nil) },
+	}
+	for i, tt := range tests {
+		id := s.TxnBegin()
+		done := make(chan struct{}, 1)
+		go func() {
+			tt()
+			done <- struct{}{}
+		}()
+		select {
+		case <-done:
+			t.Fatalf("#%d: operation failed to be blocked", i)
+		case <-time.After(10 * time.Millisecond):
+		}
+
+		s.TxnEnd(id)
+		select {
+		case <-done:
+		case <-time.After(10 * time.Second):
+			testutil.FatalStack(t, fmt.Sprintf("#%d: operation failed to be unblocked", i))
+		}
+	}
+
+	// only close backend when we know all the tx are finished
+	cleanup(s, b, tmpPath)
+}
+
+func TestKVTxnWrongID(t *testing.T) {
+	b, tmpPath := backend.NewDefaultTmpBackend()
+	s := NewStore(b, &lease.FakeLessor{}, nil)
+	defer cleanup(s, b, tmpPath)
+
+	id := s.TxnBegin()
+	wrongid := id + 1
+
+	tests := []func() error{
+		func() error {
+			_, _, err := s.TxnRange(wrongid, []byte("foo"), nil, 0, 0)
+			return err
+		},
+		func() error {
+			_, err := s.TxnPut(wrongid, []byte("foo"), nil, lease.NoLease)
+			return err
+		},
+		func() error {
+			_, _, err := s.TxnDeleteRange(wrongid, []byte("foo"), nil)
+			return err
+		},
+		func() error { return s.TxnEnd(wrongid) },
+	}
+	for i, tt := range tests {
+		err := tt()
+		if err != ErrTxnIDMismatch {
+			t.Fatalf("#%d: err = %+v, want %+v", i, err, ErrTxnIDMismatch)
+		}
+	}
+
+	err := s.TxnEnd(id)
+	if err != nil {
+		t.Fatalf("end err = %+v, want %+v", err, nil)
+	}
+}
+
+// test that txn range, put, delete on single key in sequence repeatedly works correctly.
+func TestKVTxnOperationInSequence(t *testing.T) {
+	b, tmpPath := backend.NewDefaultTmpBackend()
+	s := NewStore(b, &lease.FakeLessor{}, nil)
+	defer cleanup(s, b, tmpPath)
+
+	for i := 0; i < 10; i++ {
+		id := s.TxnBegin()
+		base := int64(i + 1)
+
+		// put foo
+		rev, err := s.TxnPut(id, []byte("foo"), []byte("bar"), lease.NoLease)
+		if err != nil {
+			t.Fatal(err)
+		}
+		if rev != base+1 {
+			t.Errorf("#%d: put rev = %d, want %d", i, rev, base+1)
+		}
+
+		kvs, rev, err := s.TxnRange(id, []byte("foo"), nil, 0, base+1)
+		if err != nil {
+			t.Fatal(err)
+		}
+		wkvs := []mvccpb.KeyValue{
+			{Key: []byte("foo"), Value: []byte("bar"), CreateRevision: base + 1, ModRevision: base + 1, Version: 1, Lease: int64(lease.NoLease)},
+		}
+		if !reflect.DeepEqual(kvs, wkvs) {
+			t.Errorf("#%d: kvs = %+v, want %+v", i, kvs, wkvs)
+		}
+		if rev != base+1 {
+			t.Errorf("#%d: range rev = %d, want %d", i, rev, base+1)
+		}
+
+		// delete foo
+		n, rev, err := s.TxnDeleteRange(id, []byte("foo"), nil)
+		if err != nil {
+			t.Fatal(err)
+		}
+		if n != 1 || rev != base+1 {
+			t.Errorf("#%d: n = %d, rev = %d, want (%d, %d)", i, n, rev, 1, base+1)
+		}
+
+		kvs, rev, err = s.TxnRange(id, []byte("foo"), nil, 0, base+1)
+		if err != nil {
+			t.Errorf("#%d: range error (%v)", i, err)
+		}
+		if kvs != nil {
+			t.Errorf("#%d: kvs = %+v, want %+v", i, kvs, nil)
+		}
+		if rev != base+1 {
+			t.Errorf("#%d: range rev = %d, want %d", i, rev, base+1)
+		}
+
+		s.TxnEnd(id)
+	}
+}
+
+func TestKVCompactReserveLastValue(t *testing.T) {
+	b, tmpPath := backend.NewDefaultTmpBackend()
+	s := NewStore(b, &lease.FakeLessor{}, nil)
+	defer cleanup(s, b, tmpPath)
+
+	s.Put([]byte("foo"), []byte("bar0"), 1)
+	s.Put([]byte("foo"), []byte("bar1"), 2)
+	s.DeleteRange([]byte("foo"), nil)
+	s.Put([]byte("foo"), []byte("bar2"), 3)
+
+	// rev in tests will be called in Compact() one by one on the same store
+	tests := []struct {
+		rev int64
+		// wanted kvs right after the compacted rev
+		wkvs []mvccpb.KeyValue
+	}{
+		{
+			1,
+			[]mvccpb.KeyValue{
+				{Key: []byte("foo"), Value: []byte("bar0"), CreateRevision: 2, ModRevision: 2, Version: 1, Lease: 1},
+			},
+		},
+		{
+			2,
+			[]mvccpb.KeyValue{
+				{Key: []byte("foo"), Value: []byte("bar1"), CreateRevision: 2, ModRevision: 3, Version: 2, Lease: 2},
+			},
+		},
+		{
+			3,
+			nil,
+		},
+		{
+			4,
+			[]mvccpb.KeyValue{
+				{Key: []byte("foo"), Value: []byte("bar2"), CreateRevision: 5, ModRevision: 5, Version: 1, Lease: 3},
+			},
+		},
+	}
+	for i, tt := range tests {
+		_, err := s.Compact(tt.rev)
+		if err != nil {
+			t.Errorf("#%d: unexpect compact error %v", i, err)
+		}
+		kvs, _, err := s.Range([]byte("foo"), nil, 0, tt.rev+1)
+		if err != nil {
+			t.Errorf("#%d: unexpect range error %v", i, err)
+		}
+		if !reflect.DeepEqual(kvs, tt.wkvs) {
+			t.Errorf("#%d: kvs = %+v, want %+v", i, kvs, tt.wkvs)
+		}
+	}
+}
+
+func TestKVCompactBad(t *testing.T) {
+	b, tmpPath := backend.NewDefaultTmpBackend()
+	s := NewStore(b, &lease.FakeLessor{}, nil)
+	defer cleanup(s, b, tmpPath)
+
+	s.Put([]byte("foo"), []byte("bar0"), lease.NoLease)
+	s.Put([]byte("foo"), []byte("bar1"), lease.NoLease)
+	s.Put([]byte("foo"), []byte("bar2"), lease.NoLease)
+
+	// rev in tests will be called in Compact() one by one on the same store
+	tests := []struct {
+		rev  int64
+		werr error
+	}{
+		{0, nil},
+		{1, nil},
+		{1, ErrCompacted},
+		{4, nil},
+		{5, ErrFutureRev},
+		{100, ErrFutureRev},
+	}
+	for i, tt := range tests {
+		_, err := s.Compact(tt.rev)
+		if err != tt.werr {
+			t.Errorf("#%d: compact error = %v, want %v", i, err, tt.werr)
+		}
+	}
+}
+
+func TestKVHash(t *testing.T) {
+	hashes := make([]uint32, 3)
+
+	for i := 0; i < len(hashes); i++ {
+		var err error
+		b, tmpPath := backend.NewDefaultTmpBackend()
+		kv := NewStore(b, &lease.FakeLessor{}, nil)
+		kv.Put([]byte("foo0"), []byte("bar0"), lease.NoLease)
+		kv.Put([]byte("foo1"), []byte("bar0"), lease.NoLease)
+		hashes[i], err = kv.Hash()
+		if err != nil {
+			t.Fatalf("failed to get hash: %v", err)
+		}
+		cleanup(kv, b, tmpPath)
+	}
+
+	for i := 1; i < len(hashes); i++ {
+		if hashes[i-1] != hashes[i] {
+			t.Errorf("hash[%d](%d) != hash[%d](%d)", i-1, hashes[i-1], i, hashes[i])
+		}
+	}
+}
+
+func TestKVRestore(t *testing.T) {
+	tests := []func(kv KV){
+		func(kv KV) {
+			kv.Put([]byte("foo"), []byte("bar0"), 1)
+			kv.Put([]byte("foo"), []byte("bar1"), 2)
+			kv.Put([]byte("foo"), []byte("bar2"), 3)
+		},
+		func(kv KV) {
+			kv.Put([]byte("foo"), []byte("bar0"), 1)
+			kv.DeleteRange([]byte("foo"), nil)
+			kv.Put([]byte("foo"), []byte("bar1"), 2)
+		},
+		func(kv KV) {
+			kv.Put([]byte("foo"), []byte("bar0"), 1)
+			kv.Put([]byte("foo"), []byte("bar1"), 2)
+			kv.Compact(1)
+		},
+	}
+	for i, tt := range tests {
+		b, tmpPath := backend.NewDefaultTmpBackend()
+		s := NewStore(b, &lease.FakeLessor{}, nil)
+		tt(s)
+		var kvss [][]mvccpb.KeyValue
+		for k := int64(0); k < 10; k++ {
+			kvs, _, _ := s.Range([]byte("a"), []byte("z"), 0, k)
+			kvss = append(kvss, kvs)
+		}
+		s.Close()
+
+		// ns should recover the the previous state from backend.
+		ns := NewStore(b, &lease.FakeLessor{}, nil)
+		// wait for possible compaction to finish
+		testutil.WaitSchedule()
+		var nkvss [][]mvccpb.KeyValue
+		for k := int64(0); k < 10; k++ {
+			nkvs, _, _ := ns.Range([]byte("a"), []byte("z"), 0, k)
+			nkvss = append(nkvss, nkvs)
+		}
+		cleanup(ns, b, tmpPath)
+
+		if !reflect.DeepEqual(nkvss, kvss) {
+			t.Errorf("#%d: kvs history = %+v, want %+v", i, nkvss, kvss)
+		}
+	}
+}
+
+func TestKVSnapshot(t *testing.T) {
+	b, tmpPath := backend.NewDefaultTmpBackend()
+	s := NewStore(b, &lease.FakeLessor{}, nil)
+	defer cleanup(s, b, tmpPath)
+
+	wkvs := put3TestKVs(s)
+
+	newPath := "new_test"
+	f, err := os.Create(newPath)
+	if err != nil {
+		t.Fatal(err)
+	}
+	defer os.Remove(newPath)
+
+	snap := s.b.Snapshot()
+	defer snap.Close()
+	_, err = snap.WriteTo(f)
+	if err != nil {
+		t.Fatal(err)
+	}
+	f.Close()
+
+	ns := NewStore(b, &lease.FakeLessor{}, nil)
+	defer ns.Close()
+	kvs, rev, err := ns.Range([]byte("a"), []byte("z"), 0, 0)
+	if err != nil {
+		t.Errorf("unexpect range error (%v)", err)
+	}
+	if !reflect.DeepEqual(kvs, wkvs) {
+		t.Errorf("kvs = %+v, want %+v", kvs, wkvs)
+	}
+	if rev != 4 {
+		t.Errorf("rev = %d, want %d", rev, 4)
+	}
+}
+
+func TestWatchableKVWatch(t *testing.T) {
+	b, tmpPath := backend.NewDefaultTmpBackend()
+	s := WatchableKV(newWatchableStore(b, &lease.FakeLessor{}, nil))
+	defer cleanup(s, b, tmpPath)
+
+	w := s.NewWatchStream()
+	defer w.Close()
+
+	wid := w.Watch([]byte("foo"), []byte("fop"), 0)
+
+	wev := []mvccpb.Event{
+		{Type: mvccpb.PUT,
+			Kv: &mvccpb.KeyValue{
+				Key:            []byte("foo"),
+				Value:          []byte("bar"),
+				CreateRevision: 2,
+				ModRevision:    2,
+				Version:        1,
+				Lease:          1,
+			},
+		},
+		{
+			Type: mvccpb.PUT,
+			Kv: &mvccpb.KeyValue{
+				Key:            []byte("foo1"),
+				Value:          []byte("bar1"),
+				CreateRevision: 3,
+				ModRevision:    3,
+				Version:        1,
+				Lease:          2,
+			},
+		},
+		{
+			Type: mvccpb.PUT,
+			Kv: &mvccpb.KeyValue{
+				Key:            []byte("foo1"),
+				Value:          []byte("bar11"),
+				CreateRevision: 3,
+				ModRevision:    4,
+				Version:        2,
+				Lease:          3,
+			},
+		},
+	}
+
+	s.Put([]byte("foo"), []byte("bar"), 1)
+	select {
+	case resp := <-w.Chan():
+		if resp.WatchID != wid {
+			t.Errorf("resp.WatchID got = %d, want = %d", resp.WatchID, wid)
+		}
+		ev := resp.Events[0]
+		if !reflect.DeepEqual(ev, wev[0]) {
+			t.Errorf("watched event = %+v, want %+v", ev, wev[0])
+		}
+	case <-time.After(5 * time.Second):
+		// CPU might be too slow, and the routine is not able to switch around
+		testutil.FatalStack(t, "failed to watch the event")
+	}
+
+	s.Put([]byte("foo1"), []byte("bar1"), 2)
+	select {
+	case resp := <-w.Chan():
+		if resp.WatchID != wid {
+			t.Errorf("resp.WatchID got = %d, want = %d", resp.WatchID, wid)
+		}
+		ev := resp.Events[0]
+		if !reflect.DeepEqual(ev, wev[1]) {
+			t.Errorf("watched event = %+v, want %+v", ev, wev[1])
+		}
+	case <-time.After(5 * time.Second):
+		testutil.FatalStack(t, "failed to watch the event")
+	}
+
+	w = s.NewWatchStream()
+	wid = w.Watch([]byte("foo1"), []byte("foo2"), 3)
+
+	select {
+	case resp := <-w.Chan():
+		if resp.WatchID != wid {
+			t.Errorf("resp.WatchID got = %d, want = %d", resp.WatchID, wid)
+		}
+		ev := resp.Events[0]
+		if !reflect.DeepEqual(ev, wev[1]) {
+			t.Errorf("watched event = %+v, want %+v", ev, wev[1])
+		}
+	case <-time.After(5 * time.Second):
+		testutil.FatalStack(t, "failed to watch the event")
+	}
+
+	s.Put([]byte("foo1"), []byte("bar11"), 3)
+	select {
+	case resp := <-w.Chan():
+		if resp.WatchID != wid {
+			t.Errorf("resp.WatchID got = %d, want = %d", resp.WatchID, wid)
+		}
+		ev := resp.Events[0]
+		if !reflect.DeepEqual(ev, wev[2]) {
+			t.Errorf("watched event = %+v, want %+v", ev, wev[2])
+		}
+	case <-time.After(5 * time.Second):
+		testutil.FatalStack(t, "failed to watch the event")
+	}
+}
+
+func cleanup(s KV, b backend.Backend, path string) {
+	s.Close()
+	b.Close()
+	os.Remove(path)
+}
+
+func put3TestKVs(s KV) []mvccpb.KeyValue {
+	s.Put([]byte("foo"), []byte("bar"), 1)
+	s.Put([]byte("foo1"), []byte("bar1"), 2)
+	s.Put([]byte("foo2"), []byte("bar2"), 3)
+	return []mvccpb.KeyValue{
+		{Key: []byte("foo"), Value: []byte("bar"), CreateRevision: 2, ModRevision: 2, Version: 1, Lease: 1},
+		{Key: []byte("foo1"), Value: []byte("bar1"), CreateRevision: 3, ModRevision: 3, Version: 1, Lease: 2},
+		{Key: []byte("foo2"), Value: []byte("bar2"), CreateRevision: 4, ModRevision: 4, Version: 1, Lease: 3},
+	}
+}
diff --git a/mvcc/kvstore.go b/mvcc/kvstore.go
new file mode 100644
index 0000000..d960d0a
--- /dev/null
+++ b/mvcc/kvstore.go
@@ -0,0 +1,636 @@
+// Copyright 2015 CoreOS, Inc.
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//     http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+package mvcc
+
+import (
+	"encoding/binary"
+	"errors"
+	"log"
+	"math"
+	"math/rand"
+	"sync"
+	"time"
+
+	"github.com/coreos/etcd/lease"
+	"github.com/coreos/etcd/mvcc/backend"
+	"github.com/coreos/etcd/mvcc/mvccpb"
+	"github.com/coreos/etcd/pkg/schedule"
+	"golang.org/x/net/context"
+)
+
+var (
+	keyBucketName  = []byte("key")
+	metaBucketName = []byte("meta")
+
+	// markedRevBytesLen is the byte length of marked revision.
+	// The first `revBytesLen` bytes represents a normal revision. The last
+	// one byte is the mark.
+	markedRevBytesLen      = revBytesLen + 1
+	markBytePosition       = markedRevBytesLen - 1
+	markTombstone     byte = 't'
+
+	consistentIndexKeyName  = []byte("consistent_index")
+	scheduledCompactKeyName = []byte("scheduledCompactRev")
+	finishedCompactKeyName  = []byte("finishedCompactRev")
+
+	ErrTxnIDMismatch = errors.New("mvcc: txn id mismatch")
+	ErrCompacted     = errors.New("mvcc: required revision has been compacted")
+	ErrFutureRev     = errors.New("mvcc: required revision is a future revision")
+	ErrCanceled      = errors.New("mvcc: watcher is canceled")
+)
+
+// ConsistentIndexGetter is an interface that wraps the Get method.
+// Consistent index is the offset of an entry in a consistent replicated log.
+type ConsistentIndexGetter interface {
+	// ConsistentIndex returns the consistent index of current executing entry.
+	ConsistentIndex() uint64
+}
+
+type store struct {
+	mu sync.Mutex // guards the following
+
+	ig ConsistentIndexGetter
+
+	b       backend.Backend
+	kvindex index
+
+	le lease.Lessor
+
+	currentRev revision
+	// the main revision of the last compaction
+	compactMainRev int64
+
+	tx    backend.BatchTx
+	txnID int64 // tracks the current txnID to verify txn operations
+
+	changes   []mvccpb.KeyValue
+	fifoSched schedule.Scheduler
+
+	stopc chan struct{}
+}
+
+// NewStore returns a new store. It is useful to create a store inside
+// mvcc pkg. It should only be used for testing externally.
+func NewStore(b backend.Backend, le lease.Lessor, ig ConsistentIndexGetter) *store {
+	s := &store{
+		b:       b,
+		ig:      ig,
+		kvindex: newTreeIndex(),
+
+		le: le,
+
+		currentRev:     revision{main: 1},
+		compactMainRev: -1,
+
+		fifoSched: schedule.NewFIFOScheduler(),
+
+		stopc: make(chan struct{}),
+	}
+
+	if s.le != nil {
+		s.le.SetRangeDeleter(s)
+	}
+
+	tx := s.b.BatchTx()
+	tx.Lock()
+	tx.UnsafeCreateBucket(keyBucketName)
+	tx.UnsafeCreateBucket(metaBucketName)
+	tx.Unlock()
+	s.b.ForceCommit()
+
+	if err := s.restore(); err != nil {
+		// TODO: return the error instead of panic here?
+		panic("failed to recover store from backend")
+	}
+
+	return s
+}
+
+func (s *store) Rev() int64 {
+	s.mu.Lock()
+	defer s.mu.Unlock()
+
+	return s.currentRev.main
+}
+
+func (s *store) FirstRev() int64 {
+	s.mu.Lock()
+	defer s.mu.Unlock()
+
+	return s.compactMainRev
+}
+
+func (s *store) Put(key, value []byte, lease lease.LeaseID) int64 {
+	id := s.TxnBegin()
+	s.put(key, value, lease)
+	s.txnEnd(id)
+
+	putCounter.Inc()
+
+	return int64(s.currentRev.main)
+}
+
+func (s *store) Range(key, end []byte, limit, rangeRev int64) (kvs []mvccpb.KeyValue, rev int64, err error) {
+	id := s.TxnBegin()
+	kvs, rev, err = s.rangeKeys(key, end, limit, rangeRev)
+	s.txnEnd(id)
+
+	rangeCounter.Inc()
+
+	return kvs, rev, err
+}
+
+func (s *store) DeleteRange(key, end []byte) (n, rev int64) {
+	id := s.TxnBegin()
+	n = s.deleteRange(key, end)
+	s.txnEnd(id)
+
+	deleteCounter.Inc()
+
+	return n, int64(s.currentRev.main)
+}
+
+func (s *store) TxnBegin() int64 {
+	s.mu.Lock()
+	s.currentRev.sub = 0
+	s.tx = s.b.BatchTx()
+	s.tx.Lock()
+	s.saveIndex()
+
+	s.txnID = rand.Int63()
+	return s.txnID
+}
+
+func (s *store) TxnEnd(txnID int64) error {
+	err := s.txnEnd(txnID)
+	if err != nil {
+		return err
+	}
+
+	txnCounter.Inc()
+	return nil
+}
+
+// txnEnd is used for unlocking an internal txn. It does
+// not increase the txnCounter.
+func (s *store) txnEnd(txnID int64) error {
+	if txnID != s.txnID {
+		return ErrTxnIDMismatch
+	}
+
+	s.tx.Unlock()
+	if s.currentRev.sub != 0 {
+		s.currentRev.main += 1
+	}
+	s.currentRev.sub = 0
+
+	dbTotalSize.Set(float64(s.b.Size()))
+	s.mu.Unlock()
+	return nil
+}
+
+func (s *store) TxnRange(txnID int64, key, end []byte, limit, rangeRev int64) (kvs []mvccpb.KeyValue, rev int64, err error) {
+	if txnID != s.txnID {
+		return nil, 0, ErrTxnIDMismatch
+	}
+	return s.rangeKeys(key, end, limit, rangeRev)
+}
+
+func (s *store) TxnPut(txnID int64, key, value []byte, lease lease.LeaseID) (rev int64, err error) {
+	if txnID != s.txnID {
+		return 0, ErrTxnIDMismatch
+	}
+
+	s.put(key, value, lease)
+	return int64(s.currentRev.main + 1), nil
+}
+
+func (s *store) TxnDeleteRange(txnID int64, key, end []byte) (n, rev int64, err error) {
+	if txnID != s.txnID {
+		return 0, 0, ErrTxnIDMismatch
+	}
+
+	n = s.deleteRange(key, end)
+	if n != 0 || s.currentRev.sub != 0 {
+		rev = int64(s.currentRev.main + 1)
+	} else {
+		rev = int64(s.currentRev.main)
+	}
+	return n, rev, nil
+}
+
+func (s *store) compactBarrier(ctx context.Context, ch chan struct{}) {
+	if ctx == nil || ctx.Err() != nil {
+		s.mu.Lock()
+		select {
+		case <-s.stopc:
+		default:
+			f := func(ctx context.Context) { s.compactBarrier(ctx, ch) }
+			s.fifoSched.Schedule(f)
+		}
+		s.mu.Unlock()
+		return
+	}
+	close(ch)
+}
+
+func (s *store) Compact(rev int64) (<-chan struct{}, error) {
+	s.mu.Lock()
+	defer s.mu.Unlock()
+	if rev <= s.compactMainRev {
+		ch := make(chan struct{})
+		f := func(ctx context.Context) { s.compactBarrier(ctx, ch) }
+		s.fifoSched.Schedule(f)
+		return ch, ErrCompacted
+	}
+	if rev > s.currentRev.main {
+		return nil, ErrFutureRev
+	}
+
+	start := time.Now()
+
+	s.compactMainRev = rev
+
+	rbytes := newRevBytes()
+	revToBytes(revision{main: rev}, rbytes)
+
+	tx := s.b.BatchTx()
+	tx.Lock()
+	tx.UnsafePut(metaBucketName, scheduledCompactKeyName, rbytes)
+	tx.Unlock()
+	// ensure that desired compaction is persisted
+	s.b.ForceCommit()
+
+	keep := s.kvindex.Compact(rev)
+	ch := make(chan struct{})
+	var j = func(ctx context.Context) {
+		if ctx.Err() != nil {
+			s.compactBarrier(ctx, ch)
+			return
+		}
+		if !s.scheduleCompaction(rev, keep) {
+			s.compactBarrier(nil, ch)
+			return
+		}
+		close(ch)
+	}
+
+	s.fifoSched.Schedule(j)
+
+	indexCompactionPauseDurations.Observe(float64(time.Since(start) / time.Millisecond))
+	return ch, nil
+}
+
+func (s *store) Hash() (uint32, error) {
+	s.b.ForceCommit()
+	return s.b.Hash()
+}
+
+func (s *store) Commit() { s.b.ForceCommit() }
+
+func (s *store) Restore(b backend.Backend) error {
+	s.mu.Lock()
+	defer s.mu.Unlock()
+
+	close(s.stopc)
+	s.fifoSched.Stop()
+
+	s.b = b
+	s.kvindex = newTreeIndex()
+	s.currentRev = revision{main: 1}
+	s.compactMainRev = -1
+	s.tx = b.BatchTx()
+	s.txnID = -1
+	s.fifoSched = schedule.NewFIFOScheduler()
+	s.stopc = make(chan struct{})
+
+	return s.restore()
+}
+
+func (s *store) restore() error {
+	min, max := newRevBytes(), newRevBytes()
+	revToBytes(revision{main: 1}, min)
+	revToBytes(revision{main: math.MaxInt64, sub: math.MaxInt64}, max)
+
+	// restore index
+	tx := s.b.BatchTx()
+	tx.Lock()
+	_, finishedCompactBytes := tx.UnsafeRange(metaBucketName, finishedCompactKeyName, nil, 0)
+	if len(finishedCompactBytes) != 0 {
+		s.compactMainRev = bytesToRev(finishedCompactBytes[0]).main
+		log.Printf("mvcc: restore compact to %d", s.compactMainRev)
+	}
+
+	// TODO: limit N to reduce max memory usage
+	keys, vals := tx.UnsafeRange(keyBucketName, min, max, 0)
+	for i, key := range keys {
+		var kv mvccpb.KeyValue
+		if err := kv.Unmarshal(vals[i]); err != nil {
+			log.Fatalf("mvcc: cannot unmarshal event: %v", err)
+		}
+
+		rev := bytesToRev(key[:revBytesLen])
+
+		// restore index
+		switch {
+		case isTombstone(key):
+			s.kvindex.Tombstone(kv.Key, rev)
+			if lease.LeaseID(kv.Lease) != lease.NoLease {
+				err := s.le.Detach(lease.LeaseID(kv.Lease), []lease.LeaseItem{{Key: string(kv.Key)}})
+				if err != nil && err != lease.ErrLeaseNotFound {
+					log.Fatalf("mvcc: unexpected Detach error %v", err)
+				}
+			}
+		default:
+			s.kvindex.Restore(kv.Key, revision{kv.CreateRevision, 0}, rev, kv.Version)
+			if lease.LeaseID(kv.Lease) != lease.NoLease {
+				if s.le == nil {
+					panic("no lessor to attach lease")
+				}
+				err := s.le.Attach(lease.LeaseID(kv.Lease), []lease.LeaseItem{{Key: string(kv.Key)}})
+				// We are walking through the kv history here. It is possible that we attached a key to
+				// the lease and the lease was revoked later.
+				// Thus attaching an old version of key to a none existing lease is possible here, and
+				// we should just ignore the error.
+				if err != nil && err != lease.ErrLeaseNotFound {
+					panic("unexpected Attach error")
+				}
+			}
+		}
+
+		// update revision
+		s.currentRev = rev
+	}
+
+	_, scheduledCompactBytes := tx.UnsafeRange(metaBucketName, scheduledCompactKeyName, nil, 0)
+	scheduledCompact := int64(0)
+	if len(scheduledCompactBytes) != 0 {
+		scheduledCompact = bytesToRev(scheduledCompactBytes[0]).main
+		if scheduledCompact <= s.compactMainRev {
+			scheduledCompact = 0
+		}
+	}
+
+	tx.Unlock()
+
+	if scheduledCompact != 0 {
+		s.Compact(scheduledCompact)
+		log.Printf("mvcc: resume scheduled compaction at %d", scheduledCompact)
+	}
+
+	return nil
+}
+
+func (s *store) Close() error {
+	close(s.stopc)
+	s.fifoSched.Stop()
+	return nil
+}
+
+func (a *store) Equal(b *store) bool {
+	if a.currentRev != b.currentRev {
+		return false
+	}
+	if a.compactMainRev != b.compactMainRev {
+		return false
+	}
+	return a.kvindex.Equal(b.kvindex)
+}
+
+// range is a keyword in Go, add Keys suffix.
+func (s *store) rangeKeys(key, end []byte, limit, rangeRev int64) (kvs []mvccpb.KeyValue, curRev int64, err error) {
+	curRev = int64(s.currentRev.main)
+	if s.currentRev.sub > 0 {
+		curRev += 1
+	}
+
+	if rangeRev > curRev {
+		return nil, s.currentRev.main, ErrFutureRev
+	}
+	var rev int64
+	if rangeRev <= 0 {
+		rev = curRev
+	} else {
+		rev = rangeRev
+	}
+	if rev < s.compactMainRev {
+		return nil, 0, ErrCompacted
+	}
+
+	_, revpairs := s.kvindex.Range(key, end, int64(rev))
+	if len(revpairs) == 0 {
+		return nil, curRev, nil
+	}
+
+	for _, revpair := range revpairs {
+		start, end := revBytesRange(revpair)
+
+		_, vs := s.tx.UnsafeRange(keyBucketName, start, end, 0)
+		if len(vs) != 1 {
+			log.Fatalf("mvcc: range cannot find rev (%d,%d)", revpair.main, revpair.sub)
+		}
+
+		var kv mvccpb.KeyValue
+		if err := kv.Unmarshal(vs[0]); err != nil {
+			log.Fatalf("mvcc: cannot unmarshal event: %v", err)
+		}
+		kvs = append(kvs, kv)
+		if limit > 0 && len(kvs) >= int(limit) {
+			break
+		}
+	}
+	return kvs, curRev, nil
+}
+
+func (s *store) put(key, value []byte, leaseID lease.LeaseID) {
+	rev := s.currentRev.main + 1
+	c := rev
+	oldLease := lease.NoLease
+
+	// if the key exists before, use its previous created and
+	// get its previous leaseID
+	grev, created, ver, err := s.kvindex.Get(key, rev)
+	if err == nil {
+		c = created.main
+		ibytes := newRevBytes()
+		revToBytes(grev, ibytes)
+		_, vs := s.tx.UnsafeRange(keyBucketName, ibytes, nil, 0)
+		var kv mvccpb.KeyValue
+		if err = kv.Unmarshal(vs[0]); err != nil {
+			log.Fatalf("mvcc: cannot unmarshal value: %v", err)
+		}
+		oldLease = lease.LeaseID(kv.Lease)
+	}
+
+	ibytes := newRevBytes()
+	revToBytes(revision{main: rev, sub: s.currentRev.sub}, ibytes)
+
+	ver = ver + 1
+	kv := mvccpb.KeyValue{
+		Key:            key,
+		Value:          value,
+		CreateRevision: c,
+		ModRevision:    rev,
+		Version:        ver,
+		Lease:          int64(leaseID),
+	}
+
+	d, err := kv.Marshal()
+	if err != nil {
+		log.Fatalf("mvcc: cannot marshal event: %v", err)
+	}
+
+	s.tx.UnsafeSeqPut(keyBucketName, ibytes, d)
+	s.kvindex.Put(key, revision{main: rev, sub: s.currentRev.sub})
+	s.changes = append(s.changes, kv)
+	s.currentRev.sub += 1
+
+	if oldLease != lease.NoLease {
+		if s.le == nil {
+			panic("no lessor to detach lease")
+		}
+
+		err = s.le.Detach(oldLease, []lease.LeaseItem{{Key: string(key)}})
+		if err != nil {
+			panic("unexpected error from lease detach")
+		}
+	}
+
+	if leaseID != lease.NoLease {
+		if s.le == nil {
+			panic("no lessor to attach lease")
+		}
+
+		err = s.le.Attach(leaseID, []lease.LeaseItem{{Key: string(key)}})
+		if err != nil {
+			panic("unexpected error from lease Attach")
+		}
+	}
+}
+
+func (s *store) deleteRange(key, end []byte) int64 {
+	rrev := s.currentRev.main
+	if s.currentRev.sub > 0 {
+		rrev += 1
+	}
+	keys, revs := s.kvindex.Range(key, end, rrev)
+
+	if len(keys) == 0 {
+		return 0
+	}
+
+	for i, key := range keys {
+		s.delete(key, revs[i])
+	}
+	return int64(len(keys))
+}
+
+func (s *store) delete(key []byte, rev revision) {
+	mainrev := s.currentRev.main + 1
+
+	ibytes := newRevBytes()
+	revToBytes(revision{main: mainrev, sub: s.currentRev.sub}, ibytes)
+	ibytes = appendMarkTombstone(ibytes)
+
+	kv := mvccpb.KeyValue{
+		Key: key,
+	}
+
+	d, err := kv.Marshal()
+	if err != nil {
+		log.Fatalf("mvcc: cannot marshal event: %v", err)
+	}
+
+	s.tx.UnsafeSeqPut(keyBucketName, ibytes, d)
+	err = s.kvindex.Tombstone(key, revision{main: mainrev, sub: s.currentRev.sub})
+	if err != nil {
+		log.Fatalf("mvcc: cannot tombstone an existing key (%s): %v", string(key), err)
+	}
+	s.changes = append(s.changes, kv)
+	s.currentRev.sub += 1
+
+	ibytes = newRevBytes()
+	revToBytes(rev, ibytes)
+	_, vs := s.tx.UnsafeRange(keyBucketName, ibytes, nil, 0)
+
+	kv.Reset()
+	if err = kv.Unmarshal(vs[0]); err != nil {
+		log.Fatalf("mvcc: cannot unmarshal value: %v", err)
+	}
+
+	if lease.LeaseID(kv.Lease) != lease.NoLease {
+		err = s.le.Detach(lease.LeaseID(kv.Lease), []lease.LeaseItem{{Key: string(kv.Key)}})
+		if err != nil {
+			log.Fatalf("mvcc: cannot detach %v", err)
+		}
+	}
+}
+
+func (s *store) getChanges() []mvccpb.KeyValue {
+	changes := s.changes
+	s.changes = make([]mvccpb.KeyValue, 0, 128)
+	return changes
+}
+
+func (s *store) saveIndex() {
+	if s.ig == nil {
+		return
+	}
+	tx := s.tx
+	// TODO: avoid this unnecessary allocation
+	bs := make([]byte, 8)
+	binary.BigEndian.PutUint64(bs, s.ig.ConsistentIndex())
+	// put the index into the underlying backend
+	// tx has been locked in TxnBegin, so there is no need to lock it again
+	tx.UnsafePut(metaBucketName, consistentIndexKeyName, bs)
+}
+
+func (s *store) ConsistentIndex() uint64 {
+	// TODO: cache index in a uint64 field?
+	tx := s.b.BatchTx()
+	tx.Lock()
+	defer tx.Unlock()
+	_, vs := tx.UnsafeRange(metaBucketName, consistentIndexKeyName, nil, 0)
+	if len(vs) == 0 {
+		return 0
+	}
+	return binary.BigEndian.Uint64(vs[0])
+}
+
+// appendMarkTombstone appends tombstone mark to normal revision bytes.
+func appendMarkTombstone(b []byte) []byte {
+	if len(b) != revBytesLen {
+		log.Panicf("cannot append mark to non normal revision bytes")
+	}
+	return append(b, markTombstone)
+}
+
+// isTombstone checks whether the revision bytes is a tombstone.
+func isTombstone(b []byte) bool {
+	return len(b) == markedRevBytesLen && b[markBytePosition] == markTombstone
+}
+
+// revBytesRange returns the range of revision bytes at
+// the given revision.
+func revBytesRange(rev revision) (start, end []byte) {
+	start = newRevBytes()
+	revToBytes(rev, start)
+
+	end = newRevBytes()
+	endRev := revision{main: rev.main, sub: rev.sub + 1}
+	revToBytes(endRev, end)
+
+	return start, end
+}
diff --git a/mvcc/kvstore_bench_test.go b/mvcc/kvstore_bench_test.go
new file mode 100644
index 0000000..6906caf
--- /dev/null
+++ b/mvcc/kvstore_bench_test.go
@@ -0,0 +1,62 @@
+// Copyright 2015 CoreOS, Inc.
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//     http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+package mvcc
+
+import (
+	"log"
+	"testing"
+
+	"github.com/coreos/etcd/lease"
+	"github.com/coreos/etcd/mvcc/backend"
+)
+
+func BenchmarkStorePut(b *testing.B) {
+	be, tmpPath := backend.NewDefaultTmpBackend()
+	s := NewStore(be, &lease.FakeLessor{}, nil)
+	defer cleanup(s, be, tmpPath)
+
+	// arbitrary number of bytes
+	bytesN := 64
+	keys := createBytesSlice(bytesN, b.N)
+	vals := createBytesSlice(bytesN, b.N)
+
+	b.ResetTimer()
+	for i := 0; i < b.N; i++ {
+		s.Put(keys[i], vals[i], lease.NoLease)
+	}
+}
+
+// BenchmarkStoreTxnPut benchmarks the Put operation
+// with transaction begin and end, where transaction involves
+// some synchronization operations, such as mutex locking.
+func BenchmarkStoreTxnPut(b *testing.B) {
+	be, tmpPath := backend.NewDefaultTmpBackend()
+	s := NewStore(be, &lease.FakeLessor{}, nil)
+	defer cleanup(s, be, tmpPath)
+
+	// arbitrary number of bytes
+	bytesN := 64
+	keys := createBytesSlice(bytesN, b.N)
+	vals := createBytesSlice(bytesN, b.N)
+
+	b.ResetTimer()
+	for i := 0; i < b.N; i++ {
+		id := s.TxnBegin()
+		if _, err := s.TxnPut(id, keys[i], vals[i], lease.NoLease); err != nil {
+			log.Fatalf("txn put error: %v", err)
+		}
+		s.TxnEnd(id)
+	}
+}
diff --git a/mvcc/kvstore_compaction.go b/mvcc/kvstore_compaction.go
new file mode 100644
index 0000000..8c5618a
--- /dev/null
+++ b/mvcc/kvstore_compaction.go
@@ -0,0 +1,65 @@
+// Copyright 2015 CoreOS, Inc.
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//     http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+package mvcc
+
+import (
+	"encoding/binary"
+	"time"
+)
+
+func (s *store) scheduleCompaction(compactMainRev int64, keep map[revision]struct{}) bool {
+	totalStart := time.Now()
+	defer dbCompactionTotalDurations.Observe(float64(time.Since(totalStart) / time.Millisecond))
+
+	end := make([]byte, 8)
+	binary.BigEndian.PutUint64(end, uint64(compactMainRev+1))
+
+	batchsize := int64(10000)
+	last := make([]byte, 8+1+8)
+	for {
+		var rev revision
+
+		start := time.Now()
+		tx := s.b.BatchTx()
+		tx.Lock()
+
+		keys, _ := tx.UnsafeRange(keyBucketName, last, end, batchsize)
+		for _, key := range keys {
+			rev = bytesToRev(key)
+			if _, ok := keep[rev]; !ok {
+				tx.UnsafeDelete(keyBucketName, key)
+			}
+		}
+
+		if len(keys) < int(batchsize) {
+			rbytes := make([]byte, 8+1+8)
+			revToBytes(revision{main: compactMainRev}, rbytes)
+			tx.UnsafePut(metaBucketName, finishedCompactKeyName, rbytes)
+			tx.Unlock()
+			return true
+		}
+
+		// update last
+		revToBytes(revision{main: rev.main, sub: rev.sub + 1}, last)
+		tx.Unlock()
+		dbCompactionPauseDurations.Observe(float64(time.Since(start) / time.Millisecond))
+
+		select {
+		case <-time.After(100 * time.Millisecond):
+		case <-s.stopc:
+			return false
+		}
+	}
+}
diff --git a/mvcc/kvstore_compaction_test.go b/mvcc/kvstore_compaction_test.go
new file mode 100644
index 0000000..3ef64b2
--- /dev/null
+++ b/mvcc/kvstore_compaction_test.go
@@ -0,0 +1,95 @@
+// Copyright 2015 CoreOS, Inc.
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//     http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+package mvcc
+
+import (
+	"reflect"
+	"testing"
+
+	"github.com/coreos/etcd/lease"
+	"github.com/coreos/etcd/mvcc/backend"
+)
+
+func TestScheduleCompaction(t *testing.T) {
+	revs := []revision{{1, 0}, {2, 0}, {3, 0}}
+
+	tests := []struct {
+		rev   int64
+		keep  map[revision]struct{}
+		wrevs []revision
+	}{
+		// compact at 1 and discard all history
+		{
+			1,
+			nil,
+			revs[1:],
+		},
+		// compact at 3 and discard all history
+		{
+			3,
+			nil,
+			nil,
+		},
+		// compact at 1 and keeps history one step earlier
+		{
+			1,
+			map[revision]struct{}{
+				revision{main: 1}: {},
+			},
+			revs,
+		},
+		// compact at 1 and keeps history two steps earlier
+		{
+			3,
+			map[revision]struct{}{
+				revision{main: 2}: {},
+				revision{main: 3}: {},
+			},
+			revs[1:],
+		},
+	}
+	for i, tt := range tests {
+		b, tmpPath := backend.NewDefaultTmpBackend()
+		s := NewStore(b, &lease.FakeLessor{}, nil)
+		tx := s.b.BatchTx()
+
+		tx.Lock()
+		ibytes := newRevBytes()
+		for _, rev := range revs {
+			revToBytes(rev, ibytes)
+			tx.UnsafePut(keyBucketName, ibytes, []byte("bar"))
+		}
+		tx.Unlock()
+
+		s.scheduleCompaction(tt.rev, tt.keep)
+
+		tx.Lock()
+		for _, rev := range tt.wrevs {
+			revToBytes(rev, ibytes)
+			keys, _ := tx.UnsafeRange(keyBucketName, ibytes, nil, 0)
+			if len(keys) != 1 {
+				t.Errorf("#%d: range on %v = %d, want 1", i, rev, len(keys))
+			}
+		}
+		_, vals := tx.UnsafeRange(metaBucketName, finishedCompactKeyName, nil, 0)
+		revToBytes(revision{main: tt.rev}, ibytes)
+		if w := [][]byte{ibytes}; !reflect.DeepEqual(vals, w) {
+			t.Errorf("#%d: vals on %v = %+v, want %+v", i, finishedCompactKeyName, vals, w)
+		}
+		tx.Unlock()
+
+		cleanup(s, b, tmpPath)
+	}
+}
diff --git a/mvcc/kvstore_test.go b/mvcc/kvstore_test.go
new file mode 100644
index 0000000..a03dc00
--- /dev/null
+++ b/mvcc/kvstore_test.go
@@ -0,0 +1,669 @@
+// Copyright 2015 CoreOS, Inc.
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//     http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+package mvcc
+
+import (
+	"crypto/rand"
+	"encoding/binary"
+	"math"
+	"os"
+	"reflect"
+	"testing"
+	"time"
+
+	"github.com/coreos/etcd/lease"
+	"github.com/coreos/etcd/mvcc/backend"
+	"github.com/coreos/etcd/mvcc/mvccpb"
+	"github.com/coreos/etcd/pkg/schedule"
+	"github.com/coreos/etcd/pkg/testutil"
+)
+
+func TestStoreRev(t *testing.T) {
+	b, tmpPath := backend.NewDefaultTmpBackend()
+	s := NewStore(b, &lease.FakeLessor{}, nil)
+	defer s.Close()
+	defer os.Remove(tmpPath)
+
+	for i := 1; i <= 3; i++ {
+		s.Put([]byte("foo"), []byte("bar"), lease.NoLease)
+		if r := s.Rev(); r != int64(i+1) {
+			t.Errorf("#%d: rev = %d, want %d", i, r, i+1)
+		}
+	}
+}
+
+func TestStorePut(t *testing.T) {
+	kv := mvccpb.KeyValue{
+		Key:            []byte("foo"),
+		Value:          []byte("bar"),
+		CreateRevision: 1,
+		ModRevision:    2,
+		Version:        1,
+	}
+	kvb, err := kv.Marshal()
+	if err != nil {
+		t.Fatal(err)
+	}
+
+	tests := []struct {
+		rev revision
+		r   indexGetResp
+		rr  *rangeResp
+
+		wrev    revision
+		wkey    []byte
+		wkv     mvccpb.KeyValue
+		wputrev revision
+	}{
+		{
+			revision{1, 0},
+			indexGetResp{revision{}, revision{}, 0, ErrRevisionNotFound},
+			nil,
+
+			revision{1, 1},
+			newTestKeyBytes(revision{2, 0}, false),
+			mvccpb.KeyValue{
+				Key:            []byte("foo"),
+				Value:          []byte("bar"),
+				CreateRevision: 2,
+				ModRevision:    2,
+				Version:        1,
+				Lease:          1,
+			},
+			revision{2, 0},
+		},
+		{
+			revision{1, 1},
+			indexGetResp{revision{2, 0}, revision{2, 0}, 1, nil},
+			&rangeResp{[][]byte{newTestKeyBytes(revision{2, 1}, false)}, [][]byte{kvb}},
+
+			revision{1, 2},
+			newTestKeyBytes(revision{2, 1}, false),
+			mvccpb.KeyValue{
+				Key:            []byte("foo"),
+				Value:          []byte("bar"),
+				CreateRevision: 2,
+				ModRevision:    2,
+				Version:        2,
+				Lease:          2,
+			},
+			revision{2, 1},
+		},
+		{
+			revision{2, 0},
+			indexGetResp{revision{2, 1}, revision{2, 0}, 2, nil},
+			&rangeResp{[][]byte{newTestKeyBytes(revision{2, 1}, false)}, [][]byte{kvb}},
+
+			revision{2, 1},
+			newTestKeyBytes(revision{3, 0}, false),
+			mvccpb.KeyValue{
+				Key:            []byte("foo"),
+				Value:          []byte("bar"),
+				CreateRevision: 2,
+				ModRevision:    3,
+				Version:        3,
+				Lease:          3,
+			},
+			revision{3, 0},
+		},
+	}
+	for i, tt := range tests {
+		s := newFakeStore()
+		b := s.b.(*fakeBackend)
+		fi := s.kvindex.(*fakeIndex)
+
+		s.currentRev = tt.rev
+		s.tx = b.BatchTx()
+		fi.indexGetRespc <- tt.r
+		if tt.rr != nil {
+			b.tx.rangeRespc <- *tt.rr
+		}
+
+		s.put([]byte("foo"), []byte("bar"), lease.LeaseID(i+1))
+
+		data, err := tt.wkv.Marshal()
+		if err != nil {
+			t.Errorf("#%d: marshal err = %v, want nil", i, err)
+		}
+
+		wact := []testutil.Action{
+			{"seqput", []interface{}{keyBucketName, tt.wkey, data}},
+		}
+
+		if tt.rr != nil {
+			wact = []testutil.Action{
+				{"range", []interface{}{keyBucketName, newTestKeyBytes(tt.r.rev, false), []byte(nil), int64(0)}},
+				{"seqput", []interface{}{keyBucketName, tt.wkey, data}},
+			}
+		}
+
+		if g := b.tx.Action(); !reflect.DeepEqual(g, wact) {
+			t.Errorf("#%d: tx action = %+v, want %+v", i, g, wact)
+		}
+		wact = []testutil.Action{
+			{"get", []interface{}{[]byte("foo"), tt.wputrev.main}},
+			{"put", []interface{}{[]byte("foo"), tt.wputrev}},
+		}
+		if g := fi.Action(); !reflect.DeepEqual(g, wact) {
+			t.Errorf("#%d: index action = %+v, want %+v", i, g, wact)
+		}
+		if s.currentRev != tt.wrev {
+			t.Errorf("#%d: rev = %+v, want %+v", i, s.currentRev, tt.wrev)
+		}
+
+		s.Close()
+	}
+}
+
+func TestStoreRange(t *testing.T) {
+	key := newTestKeyBytes(revision{2, 0}, false)
+	kv := mvccpb.KeyValue{
+		Key:            []byte("foo"),
+		Value:          []byte("bar"),
+		CreateRevision: 1,
+		ModRevision:    2,
+		Version:        1,
+	}
+	kvb, err := kv.Marshal()
+	if err != nil {
+		t.Fatal(err)
+	}
+	currev := revision{1, 1}
+	wrev := int64(2)
+
+	tests := []struct {
+		idxr indexRangeResp
+		r    rangeResp
+	}{
+		{
+			indexRangeResp{[][]byte{[]byte("foo")}, []revision{{2, 0}}},
+			rangeResp{[][]byte{key}, [][]byte{kvb}},
+		},
+		{
+			indexRangeResp{[][]byte{[]byte("foo"), []byte("foo1")}, []revision{{2, 0}, {3, 0}}},
+			rangeResp{[][]byte{key}, [][]byte{kvb}},
+		},
+	}
+	for i, tt := range tests {
+		s := newFakeStore()
+		b := s.b.(*fakeBackend)
+		fi := s.kvindex.(*fakeIndex)
+
+		s.currentRev = currev
+		s.tx = b.BatchTx()
+		b.tx.rangeRespc <- tt.r
+		fi.indexRangeRespc <- tt.idxr
+
+		kvs, rev, err := s.rangeKeys([]byte("foo"), []byte("goo"), 1, 0)
+		if err != nil {
+			t.Errorf("#%d: err = %v, want nil", i, err)
+		}
+		if w := []mvccpb.KeyValue{kv}; !reflect.DeepEqual(kvs, w) {
+			t.Errorf("#%d: kvs = %+v, want %+v", i, kvs, w)
+		}
+		if rev != wrev {
+			t.Errorf("#%d: rev = %d, want %d", i, rev, wrev)
+		}
+
+		wstart, wend := revBytesRange(tt.idxr.revs[0])
+		wact := []testutil.Action{
+			{"range", []interface{}{keyBucketName, wstart, wend, int64(0)}},
+		}
+		if g := b.tx.Action(); !reflect.DeepEqual(g, wact) {
+			t.Errorf("#%d: tx action = %+v, want %+v", i, g, wact)
+		}
+		wact = []testutil.Action{
+			{"range", []interface{}{[]byte("foo"), []byte("goo"), wrev}},
+		}
+		if g := fi.Action(); !reflect.DeepEqual(g, wact) {
+			t.Errorf("#%d: index action = %+v, want %+v", i, g, wact)
+		}
+		if s.currentRev != currev {
+			t.Errorf("#%d: current rev = %+v, want %+v", i, s.currentRev, currev)
+		}
+
+		s.Close()
+	}
+}
+
+func TestStoreDeleteRange(t *testing.T) {
+	key := newTestKeyBytes(revision{2, 0}, false)
+	kv := mvccpb.KeyValue{
+		Key:            []byte("foo"),
+		Value:          []byte("bar"),
+		CreateRevision: 1,
+		ModRevision:    2,
+		Version:        1,
+	}
+	kvb, err := kv.Marshal()
+	if err != nil {
+		t.Fatal(err)
+	}
+
+	tests := []struct {
+		rev revision
+		r   indexRangeResp
+		rr  rangeResp
+
+		wkey    []byte
+		wrev    revision
+		wrrev   int64
+		wdelrev revision
+	}{
+		{
+			revision{2, 0},
+			indexRangeResp{[][]byte{[]byte("foo")}, []revision{{2, 0}}},
+			rangeResp{[][]byte{key}, [][]byte{kvb}},
+
+			newTestKeyBytes(revision{3, 0}, true),
+			revision{2, 1},
+			2,
+			revision{3, 0},
+		},
+		{
+			revision{2, 1},
+			indexRangeResp{[][]byte{[]byte("foo")}, []revision{{2, 0}}},
+			rangeResp{[][]byte{key}, [][]byte{kvb}},
+
+			newTestKeyBytes(revision{3, 1}, true),
+			revision{2, 2},
+			3,
+			revision{3, 1},
+		},
+	}
+	for i, tt := range tests {
+		s := newFakeStore()
+		b := s.b.(*fakeBackend)
+		fi := s.kvindex.(*fakeIndex)
+
+		s.currentRev = tt.rev
+		s.tx = b.BatchTx()
+		fi.indexRangeRespc <- tt.r
+		b.tx.rangeRespc <- tt.rr
+
+		n := s.deleteRange([]byte("foo"), []byte("goo"))
+		if n != 1 {
+			t.Errorf("#%d: n = %d, want 1", i, n)
+		}
+
+		data, err := (&mvccpb.KeyValue{
+			Key: []byte("foo"),
+		}).Marshal()
+		if err != nil {
+			t.Errorf("#%d: marshal err = %v, want nil", i, err)
+		}
+		wact := []testutil.Action{
+			{"seqput", []interface{}{keyBucketName, tt.wkey, data}},
+			{"range", []interface{}{keyBucketName, newTestKeyBytes(revision{2, 0}, false), []byte(nil), int64(0)}},
+		}
+		if g := b.tx.Action(); !reflect.DeepEqual(g, wact) {
+			t.Errorf("#%d: tx action = %+v, want %+v", i, g, wact)
+		}
+		wact = []testutil.Action{
+			{"range", []interface{}{[]byte("foo"), []byte("goo"), tt.wrrev}},
+			{"tombstone", []interface{}{[]byte("foo"), tt.wdelrev}},
+		}
+		if g := fi.Action(); !reflect.DeepEqual(g, wact) {
+			t.Errorf("#%d: index action = %+v, want %+v", i, g, wact)
+		}
+		if s.currentRev != tt.wrev {
+			t.Errorf("#%d: rev = %+v, want %+v", i, s.currentRev, tt.wrev)
+		}
+	}
+}
+
+func TestStoreCompact(t *testing.T) {
+	s := newFakeStore()
+	defer s.Close()
+	b := s.b.(*fakeBackend)
+	fi := s.kvindex.(*fakeIndex)
+
+	s.currentRev = revision{3, 0}
+	fi.indexCompactRespc <- map[revision]struct{}{revision{1, 0}: {}}
+	key1 := newTestKeyBytes(revision{1, 0}, false)
+	key2 := newTestKeyBytes(revision{2, 0}, false)
+	b.tx.rangeRespc <- rangeResp{[][]byte{key1, key2}, nil}
+
+	s.Compact(3)
+	s.fifoSched.WaitFinish(1)
+
+	if s.compactMainRev != 3 {
+		t.Errorf("compact main rev = %d, want 3", s.compactMainRev)
+	}
+	end := make([]byte, 8)
+	binary.BigEndian.PutUint64(end, uint64(4))
+	wact := []testutil.Action{
+		{"put", []interface{}{metaBucketName, scheduledCompactKeyName, newTestRevBytes(revision{3, 0})}},
+		{"range", []interface{}{keyBucketName, make([]byte, 17), end, int64(10000)}},
+		{"delete", []interface{}{keyBucketName, key2}},
+		{"put", []interface{}{metaBucketName, finishedCompactKeyName, newTestRevBytes(revision{3, 0})}},
+	}
+	if g := b.tx.Action(); !reflect.DeepEqual(g, wact) {
+		t.Errorf("tx actions = %+v, want %+v", g, wact)
+	}
+	wact = []testutil.Action{
+		{"compact", []interface{}{int64(3)}},
+	}
+	if g := fi.Action(); !reflect.DeepEqual(g, wact) {
+		t.Errorf("index action = %+v, want %+v", g, wact)
+	}
+}
+
+func TestStoreRestore(t *testing.T) {
+	s := newFakeStore()
+	b := s.b.(*fakeBackend)
+	fi := s.kvindex.(*fakeIndex)
+
+	putkey := newTestKeyBytes(revision{3, 0}, false)
+	putkv := mvccpb.KeyValue{
+		Key:            []byte("foo"),
+		Value:          []byte("bar"),
+		CreateRevision: 4,
+		ModRevision:    4,
+		Version:        1,
+	}
+	putkvb, err := putkv.Marshal()
+	if err != nil {
+		t.Fatal(err)
+	}
+	delkey := newTestKeyBytes(revision{5, 0}, true)
+	delkv := mvccpb.KeyValue{
+		Key: []byte("foo"),
+	}
+	delkvb, err := delkv.Marshal()
+	if err != nil {
+		t.Fatal(err)
+	}
+	b.tx.rangeRespc <- rangeResp{[][]byte{finishedCompactKeyName}, [][]byte{newTestRevBytes(revision{3, 0})}}
+	b.tx.rangeRespc <- rangeResp{[][]byte{putkey, delkey}, [][]byte{putkvb, delkvb}}
+	b.tx.rangeRespc <- rangeResp{[][]byte{scheduledCompactKeyName}, [][]byte{newTestRevBytes(revision{3, 0})}}
+
+	s.restore()
+
+	if s.compactMainRev != 3 {
+		t.Errorf("compact rev = %d, want 5", s.compactMainRev)
+	}
+	wrev := revision{5, 0}
+	if !reflect.DeepEqual(s.currentRev, wrev) {
+		t.Errorf("current rev = %v, want %v", s.currentRev, wrev)
+	}
+	wact := []testutil.Action{
+		{"range", []interface{}{metaBucketName, finishedCompactKeyName, []byte(nil), int64(0)}},
+		{"range", []interface{}{keyBucketName, newTestRevBytes(revision{1, 0}), newTestRevBytes(revision{math.MaxInt64, math.MaxInt64}), int64(0)}},
+		{"range", []interface{}{metaBucketName, scheduledCompactKeyName, []byte(nil), int64(0)}},
+	}
+	if g := b.tx.Action(); !reflect.DeepEqual(g, wact) {
+		t.Errorf("tx actions = %+v, want %+v", g, wact)
+	}
+	wact = []testutil.Action{
+		{"restore", []interface{}{[]byte("foo"), revision{4, 0}, revision{3, 0}, int64(1)}},
+		{"tombstone", []interface{}{[]byte("foo"), revision{5, 0}}},
+	}
+	if g := fi.Action(); !reflect.DeepEqual(g, wact) {
+		t.Errorf("index action = %+v, want %+v", g, wact)
+	}
+}
+
+func TestRestoreContinueUnfinishedCompaction(t *testing.T) {
+	b, tmpPath := backend.NewDefaultTmpBackend()
+	s0 := NewStore(b, &lease.FakeLessor{}, nil)
+	defer os.Remove(tmpPath)
+
+	s0.Put([]byte("foo"), []byte("bar"), lease.NoLease)
+	s0.Put([]byte("foo"), []byte("bar1"), lease.NoLease)
+	s0.Put([]byte("foo"), []byte("bar2"), lease.NoLease)
+
+	// write scheduled compaction, but not do compaction
+	rbytes := newRevBytes()
+	revToBytes(revision{main: 2}, rbytes)
+	tx := s0.b.BatchTx()
+	tx.Lock()
+	tx.UnsafePut(metaBucketName, scheduledCompactKeyName, rbytes)
+	tx.Unlock()
+
+	s0.Close()
+
+	s1 := NewStore(b, &lease.FakeLessor{}, nil)
+
+	// wait for scheduled compaction to be finished
+	time.Sleep(100 * time.Millisecond)
+
+	if _, _, err := s1.Range([]byte("foo"), nil, 0, 1); err != ErrCompacted {
+		t.Errorf("range on compacted rev error = %v, want %v", err, ErrCompacted)
+	}
+	// check the key in backend is deleted
+	revbytes := newRevBytes()
+	revToBytes(revision{main: 1}, revbytes)
+
+	// The disk compaction is done asynchronously and requires more time on slow disk.
+	// try 5 times for CI with slow IO.
+	for i := 0; i < 5; i++ {
+		tx = s1.b.BatchTx()
+		tx.Lock()
+		ks, _ := tx.UnsafeRange(keyBucketName, revbytes, nil, 0)
+		tx.Unlock()
+		if len(ks) != 0 {
+			time.Sleep(100 * time.Millisecond)
+			continue
+		}
+		return
+	}
+
+	t.Errorf("key for rev %+v still exists, want deleted", bytesToRev(revbytes))
+}
+
+func TestTxnPut(t *testing.T) {
+	// assign arbitrary size
+	bytesN := 30
+	sliceN := 100
+	keys := createBytesSlice(bytesN, sliceN)
+	vals := createBytesSlice(bytesN, sliceN)
+
+	b, tmpPath := backend.NewDefaultTmpBackend()
+	s := NewStore(b, &lease.FakeLessor{}, nil)
+	defer cleanup(s, b, tmpPath)
+
+	for i := 0; i < sliceN; i++ {
+		id := s.TxnBegin()
+		base := int64(i + 2)
+
+		rev, err := s.TxnPut(id, keys[i], vals[i], lease.NoLease)
+		if err != nil {
+			t.Error("txn put error")
+		}
+		if rev != base {
+			t.Errorf("#%d: rev = %d, want %d", i, rev, base)
+		}
+
+		s.TxnEnd(id)
+	}
+}
+
+func TestTxnBlockBackendForceCommit(t *testing.T) {
+	b, tmpPath := backend.NewDefaultTmpBackend()
+	s := NewStore(b, &lease.FakeLessor{}, nil)
+	defer os.Remove(tmpPath)
+
+	id := s.TxnBegin()
+
+	done := make(chan struct{})
+	go func() {
+		s.b.ForceCommit()
+		done <- struct{}{}
+	}()
+	select {
+	case <-done:
+		t.Fatalf("failed to block ForceCommit")
+	case <-time.After(100 * time.Millisecond):
+	}
+
+	s.TxnEnd(id)
+	select {
+	case <-done:
+	case <-time.After(5 * time.Second): // wait 5 seconds for CI with slow IO
+		testutil.FatalStack(t, "failed to execute ForceCommit")
+	}
+}
+
+// TODO: test attach key to lessor
+
+func newTestRevBytes(rev revision) []byte {
+	bytes := newRevBytes()
+	revToBytes(rev, bytes)
+	return bytes
+}
+
+func newTestKeyBytes(rev revision, tombstone bool) []byte {
+	bytes := newRevBytes()
+	revToBytes(rev, bytes)
+	if tombstone {
+		bytes = appendMarkTombstone(bytes)
+	}
+	return bytes
+}
+
+func newFakeStore() *store {
+	b := &fakeBackend{&fakeBatchTx{
+		Recorder:   &testutil.RecorderBuffered{},
+		rangeRespc: make(chan rangeResp, 5)}}
+	fi := &fakeIndex{
+		Recorder:              &testutil.RecorderBuffered{},
+		indexGetRespc:         make(chan indexGetResp, 1),
+		indexRangeRespc:       make(chan indexRangeResp, 1),
+		indexRangeEventsRespc: make(chan indexRangeEventsResp, 1),
+		indexCompactRespc:     make(chan map[revision]struct{}, 1),
+	}
+	return &store{
+		b:              b,
+		le:             &lease.FakeLessor{},
+		kvindex:        fi,
+		currentRev:     revision{},
+		compactMainRev: -1,
+		fifoSched:      schedule.NewFIFOScheduler(),
+		stopc:          make(chan struct{}),
+	}
+}
+
+type rangeResp struct {
+	keys [][]byte
+	vals [][]byte
+}
+
+type fakeBatchTx struct {
+	testutil.Recorder
+	rangeRespc chan rangeResp
+}
+
+func (b *fakeBatchTx) Lock()                          {}
+func (b *fakeBatchTx) Unlock()                        {}
+func (b *fakeBatchTx) UnsafeCreateBucket(name []byte) {}
+func (b *fakeBatchTx) UnsafePut(bucketName []byte, key []byte, value []byte) {
+	b.Recorder.Record(testutil.Action{Name: "put", Params: []interface{}{bucketName, key, value}})
+}
+func (b *fakeBatchTx) UnsafeSeqPut(bucketName []byte, key []byte, value []byte) {
+	b.Recorder.Record(testutil.Action{Name: "seqput", Params: []interface{}{bucketName, key, value}})
+}
+func (b *fakeBatchTx) UnsafeRange(bucketName []byte, key, endKey []byte, limit int64) (keys [][]byte, vals [][]byte) {
+	b.Recorder.Record(testutil.Action{Name: "range", Params: []interface{}{bucketName, key, endKey, limit}})
+	r := <-b.rangeRespc
+	return r.keys, r.vals
+}
+func (b *fakeBatchTx) UnsafeDelete(bucketName []byte, key []byte) {
+	b.Recorder.Record(testutil.Action{Name: "delete", Params: []interface{}{bucketName, key}})
+}
+func (b *fakeBatchTx) UnsafeForEach(bucketName []byte, visitor func(k, v []byte) error) error {
+	return nil
+}
+func (b *fakeBatchTx) Commit()        {}
+func (b *fakeBatchTx) CommitAndStop() {}
+
+type fakeBackend struct {
+	tx *fakeBatchTx
+}
+
+func (b *fakeBackend) BatchTx() backend.BatchTx   { return b.tx }
+func (b *fakeBackend) Hash() (uint32, error)      { return 0, nil }
+func (b *fakeBackend) Size() int64                { return 0 }
+func (b *fakeBackend) Snapshot() backend.Snapshot { return nil }
+func (b *fakeBackend) ForceCommit()               {}
+func (b *fakeBackend) Defrag() error              { return nil }
+func (b *fakeBackend) Close() error               { return nil }
+
+type indexGetResp struct {
+	rev     revision
+	created revision
+	ver     int64
+	err     error
+}
+
+type indexRangeResp struct {
+	keys [][]byte
+	revs []revision
+}
+
+type indexRangeEventsResp struct {
+	revs []revision
+}
+
+type fakeIndex struct {
+	testutil.Recorder
+	indexGetRespc         chan indexGetResp
+	indexRangeRespc       chan indexRangeResp
+	indexRangeEventsRespc chan indexRangeEventsResp
+	indexCompactRespc     chan map[revision]struct{}
+}
+
+func (i *fakeIndex) Get(key []byte, atRev int64) (rev, created revision, ver int64, err error) {
+	i.Recorder.Record(testutil.Action{Name: "get", Params: []interface{}{key, atRev}})
+	r := <-i.indexGetRespc
+	return r.rev, r.created, r.ver, r.err
+}
+func (i *fakeIndex) Range(key, end []byte, atRev int64) ([][]byte, []revision) {
+	i.Recorder.Record(testutil.Action{Name: "range", Params: []interface{}{key, end, atRev}})
+	r := <-i.indexRangeRespc
+	return r.keys, r.revs
+}
+func (i *fakeIndex) Put(key []byte, rev revision) {
+	i.Recorder.Record(testutil.Action{Name: "put", Params: []interface{}{key, rev}})
+}
+func (i *fakeIndex) Restore(key []byte, created, modified revision, ver int64) {
+	i.Recorder.Record(testutil.Action{Name: "restore", Params: []interface{}{key, created, modified, ver}})
+}
+func (i *fakeIndex) Tombstone(key []byte, rev revision) error {
+	i.Recorder.Record(testutil.Action{Name: "tombstone", Params: []interface{}{key, rev}})
+	return nil
+}
+func (i *fakeIndex) RangeSince(key, end []byte, rev int64) []revision {
+	i.Recorder.Record(testutil.Action{Name: "rangeEvents", Params: []interface{}{key, end, rev}})
+	r := <-i.indexRangeEventsRespc
+	return r.revs
+}
+func (i *fakeIndex) Compact(rev int64) map[revision]struct{} {
+	i.Recorder.Record(testutil.Action{Name: "compact", Params: []interface{}{rev}})
+	return <-i.indexCompactRespc
+}
+func (i *fakeIndex) Equal(b index) bool { return false }
+
+func createBytesSlice(bytesN, sliceN int) [][]byte {
+	rs := [][]byte{}
+	for len(rs) != sliceN {
+		v := make([]byte, bytesN)
+		if _, err := rand.Read(v); err != nil {
+			panic(err)
+		}
+		rs = append(rs, v)
+	}
+	return rs
+}
diff --git a/mvcc/metrics.go b/mvcc/metrics.go
new file mode 100644
index 0000000..f264601
--- /dev/null
+++ b/mvcc/metrics.go
@@ -0,0 +1,163 @@
+// Copyright 2015 CoreOS, Inc.
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//     http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+package mvcc
+
+import (
+	"github.com/prometheus/client_golang/prometheus"
+)
+
+var (
+	rangeCounter = prometheus.NewCounter(
+		prometheus.CounterOpts{
+			Namespace: "etcd",
+			Subsystem: "mvcc",
+			Name:      "range_total",
+			Help:      "Total number of ranges seen by this member.",
+		})
+
+	putCounter = prometheus.NewCounter(
+		prometheus.CounterOpts{
+			Namespace: "etcd",
+			Subsystem: "mvcc",
+			Name:      "put_total",
+			Help:      "Total number of puts seen by this member.",
+		})
+
+	deleteCounter = prometheus.NewCounter(
+		prometheus.CounterOpts{
+			Namespace: "etcd",
+			Subsystem: "mvcc",
+			Name:      "delete_total",
+			Help:      "Total number of deletes seen by this member.",
+		})
+
+	txnCounter = prometheus.NewCounter(
+		prometheus.CounterOpts{
+			Namespace: "etcd",
+			Subsystem: "mvcc",
+			Name:      "txn_total",
+			Help:      "Total number of txns seen by this member.",
+		})
+
+	keysGauge = prometheus.NewGauge(
+		prometheus.GaugeOpts{
+			Namespace: "etcd",
+			Subsystem: "mvcc",
+			Name:      "keys_total",
+			Help:      "Total number of keys.",
+		})
+
+	watchStreamGauge = prometheus.NewGauge(
+		prometheus.GaugeOpts{
+			Namespace: "etcd",
+			Subsystem: "mvcc",
+			Name:      "watch_stream_total",
+			Help:      "Total number of watch streams.",
+		})
+
+	watcherGauge = prometheus.NewGauge(
+		prometheus.GaugeOpts{
+			Namespace: "etcd",
+			Subsystem: "mvcc",
+			Name:      "watcher_total",
+			Help:      "Total number of watchers.",
+		})
+
+	slowWatcherGauge = prometheus.NewGauge(
+		prometheus.GaugeOpts{
+			Namespace: "etcd",
+			Subsystem: "mvcc",
+			Name:      "slow_watcher_total",
+			Help:      "Total number of unsynced slow watchers.",
+		})
+
+	totalEventsCounter = prometheus.NewCounter(
+		prometheus.CounterOpts{
+			Namespace: "etcd",
+			Subsystem: "mvcc",
+			Name:      "events_total",
+			Help:      "Total number of events sent by this member.",
+		})
+
+	pendingEventsGauge = prometheus.NewGauge(
+		prometheus.GaugeOpts{
+			Namespace: "etcd",
+			Subsystem: "mvcc",
+			Name:      "pending_events_total",
+			Help:      "Total number of pending events to be sent.",
+		})
+
+	indexCompactionPauseDurations = prometheus.NewHistogram(
+		prometheus.HistogramOpts{
+			Namespace: "etcd",
+			Subsystem: "mvcc",
+			Name:      "index_compaction_pause_duration_milliseconds",
+			Help:      "Bucketed histogram of index compaction pause duration.",
+			// 0.5ms -> 1second
+			Buckets: prometheus.ExponentialBuckets(0.5, 2, 12),
+		})
+
+	dbCompactionPauseDurations = prometheus.NewHistogram(
+		prometheus.HistogramOpts{
+			Namespace: "etcd",
+			Subsystem: "mvcc",
+			Name:      "db_compaction_pause_duration_milliseconds",
+			Help:      "Bucketed histogram of db compaction pause duration.",
+			// 1ms -> 4second
+			Buckets: prometheus.ExponentialBuckets(1, 2, 13),
+		})
+
+	dbCompactionTotalDurations = prometheus.NewHistogram(
+		prometheus.HistogramOpts{
+			Namespace: "etcd",
+			Subsystem: "mvcc",
+			Name:      "db_compaction_total_duration_milliseconds",
+			Help:      "Bucketed histogram of db compaction total duration.",
+			// 100ms -> 800second
+			Buckets: prometheus.ExponentialBuckets(100, 2, 14),
+		})
+
+	dbTotalSize = prometheus.NewGauge(prometheus.GaugeOpts{
+		Namespace: "etcd",
+		Subsystem: "mvcc",
+		Name:      "db_total_size_in_bytes",
+		Help:      "Total size of the underlying database in bytes.",
+	})
+)
+
+func init() {
+	prometheus.MustRegister(rangeCounter)
+	prometheus.MustRegister(putCounter)
+	prometheus.MustRegister(deleteCounter)
+	prometheus.MustRegister(txnCounter)
+	prometheus.MustRegister(keysGauge)
+	prometheus.MustRegister(watchStreamGauge)
+	prometheus.MustRegister(watcherGauge)
+	prometheus.MustRegister(slowWatcherGauge)
+	prometheus.MustRegister(totalEventsCounter)
+	prometheus.MustRegister(pendingEventsGauge)
+	prometheus.MustRegister(indexCompactionPauseDurations)
+	prometheus.MustRegister(dbCompactionPauseDurations)
+	prometheus.MustRegister(dbCompactionTotalDurations)
+	prometheus.MustRegister(dbTotalSize)
+}
+
+// ReportEventReceived reports that an event is received.
+// This function should be called when the external systems received an
+// event from mvcc.Watcher.
+func ReportEventReceived() {
+	pendingEventsGauge.Dec()
+	totalEventsCounter.Inc()
+}
diff --git a/mvcc/mvccpb/kv.pb.go b/mvcc/mvccpb/kv.pb.go
new file mode 100644
index 0000000..dd3b1d8
--- /dev/null
+++ b/mvcc/mvccpb/kv.pb.go
@@ -0,0 +1,682 @@
+// Code generated by protoc-gen-gogo.
+// source: kv.proto
+// DO NOT EDIT!
+
+/*
+	Package mvccpb is a generated protocol buffer package.
+
+	It is generated from these files:
+		kv.proto
+
+	It has these top-level messages:
+		KeyValue
+		Event
+*/
+package mvccpb
+
+import (
+	"fmt"
+
+	proto "github.com/gogo/protobuf/proto"
+
+	math "math"
+)
+
+import io "io"
+
+// Reference imports to suppress errors if they are not otherwise used.
+var _ = proto.Marshal
+var _ = fmt.Errorf
+var _ = math.Inf
+
+// This is a compile-time assertion to ensure that this generated file
+// is compatible with the proto package it is being compiled against.
+const _ = proto.GoGoProtoPackageIsVersion1
+
+type Event_EventType int32
+
+const (
+	PUT    Event_EventType = 0
+	DELETE Event_EventType = 1
+	EXPIRE Event_EventType = 2
+)
+
+var Event_EventType_name = map[int32]string{
+	0: "PUT",
+	1: "DELETE",
+	2: "EXPIRE",
+}
+var Event_EventType_value = map[string]int32{
+	"PUT":    0,
+	"DELETE": 1,
+	"EXPIRE": 2,
+}
+
+func (x Event_EventType) String() string {
+	return proto.EnumName(Event_EventType_name, int32(x))
+}
+func (Event_EventType) EnumDescriptor() ([]byte, []int) { return fileDescriptorKv, []int{1, 0} }
+
+type KeyValue struct {
+	// key is the key in bytes. An empty key is not allowed.
+	Key []byte `protobuf:"bytes,1,opt,name=key,proto3" json:"key,omitempty"`
+	// create_revision is the revision of last creation on this key.
+	CreateRevision int64 `protobuf:"varint,2,opt,name=create_revision,proto3" json:"create_revision,omitempty"`
+	// mod_revision is the revision of last modification on this key.
+	ModRevision int64 `protobuf:"varint,3,opt,name=mod_revision,proto3" json:"mod_revision,omitempty"`
+	// version is the version of the key. A deletion resets
+	// the version to zero and any modification of the key
+	// increases its version.
+	Version int64 `protobuf:"varint,4,opt,name=version,proto3" json:"version,omitempty"`
+	// value is the value held by the key, in bytes.
+	Value []byte `protobuf:"bytes,5,opt,name=value,proto3" json:"value,omitempty"`
+	// lease is the ID of the lease that attached to key.
+	// When the attached lease expires, the key will be deleted.
+	// If lease is 0, then no lease is attached to the key.
+	Lease int64 `protobuf:"varint,6,opt,name=lease,proto3" json:"lease,omitempty"`
+}
+
+func (m *KeyValue) Reset()                    { *m = KeyValue{} }
+func (m *KeyValue) String() string            { return proto.CompactTextString(m) }
+func (*KeyValue) ProtoMessage()               {}
+func (*KeyValue) Descriptor() ([]byte, []int) { return fileDescriptorKv, []int{0} }
+
+type Event struct {
+	// type is the kind of event. If type is a PUT, it indicates
+	// new data has been stored to the key. If type is a DELETE,
+	// it indicates the key was deleted.
+	Type Event_EventType `protobuf:"varint,1,opt,name=type,proto3,enum=mvccpb.Event_EventType" json:"type,omitempty"`
+	// kv holds the KeyValue for the event.
+	// A PUT event contains current kv pair.
+	// A PUT event with kv.Version=1 indicates the creation of a key.
+	// A DELETE/EXPIRE event contains the deleted key with
+	// its modification revision set to the revision of deletion.
+	Kv *KeyValue `protobuf:"bytes,2,opt,name=kv" json:"kv,omitempty"`
+}
+
+func (m *Event) Reset()                    { *m = Event{} }
+func (m *Event) String() string            { return proto.CompactTextString(m) }
+func (*Event) ProtoMessage()               {}
+func (*Event) Descriptor() ([]byte, []int) { return fileDescriptorKv, []int{1} }
+
+func init() {
+	proto.RegisterType((*KeyValue)(nil), "mvccpb.KeyValue")
+	proto.RegisterType((*Event)(nil), "mvccpb.Event")
+	proto.RegisterEnum("mvccpb.Event_EventType", Event_EventType_name, Event_EventType_value)
+}
+func (m *KeyValue) Marshal() (data []byte, err error) {
+	size := m.Size()
+	data = make([]byte, size)
+	n, err := m.MarshalTo(data)
+	if err != nil {
+		return nil, err
+	}
+	return data[:n], nil
+}
+
+func (m *KeyValue) MarshalTo(data []byte) (int, error) {
+	var i int
+	_ = i
+	var l int
+	_ = l
+	if len(m.Key) > 0 {
+		data[i] = 0xa
+		i++
+		i = encodeVarintKv(data, i, uint64(len(m.Key)))
+		i += copy(data[i:], m.Key)
+	}
+	if m.CreateRevision != 0 {
+		data[i] = 0x10
+		i++
+		i = encodeVarintKv(data, i, uint64(m.CreateRevision))
+	}
+	if m.ModRevision != 0 {
+		data[i] = 0x18
+		i++
+		i = encodeVarintKv(data, i, uint64(m.ModRevision))
+	}
+	if m.Version != 0 {
+		data[i] = 0x20
+		i++
+		i = encodeVarintKv(data, i, uint64(m.Version))
+	}
+	if len(m.Value) > 0 {
+		data[i] = 0x2a
+		i++
+		i = encodeVarintKv(data, i, uint64(len(m.Value)))
+		i += copy(data[i:], m.Value)
+	}
+	if m.Lease != 0 {
+		data[i] = 0x30
+		i++
+		i = encodeVarintKv(data, i, uint64(m.Lease))
+	}
+	return i, nil
+}
+
+func (m *Event) Marshal() (data []byte, err error) {
+	size := m.Size()
+	data = make([]byte, size)
+	n, err := m.MarshalTo(data)
+	if err != nil {
+		return nil, err
+	}
+	return data[:n], nil
+}
+
+func (m *Event) MarshalTo(data []byte) (int, error) {
+	var i int
+	_ = i
+	var l int
+	_ = l
+	if m.Type != 0 {
+		data[i] = 0x8
+		i++
+		i = encodeVarintKv(data, i, uint64(m.Type))
+	}
+	if m.Kv != nil {
+		data[i] = 0x12
+		i++
+		i = encodeVarintKv(data, i, uint64(m.Kv.Size()))
+		n1, err := m.Kv.MarshalTo(data[i:])
+		if err != nil {
+			return 0, err
+		}
+		i += n1
+	}
+	return i, nil
+}
+
+func encodeFixed64Kv(data []byte, offset int, v uint64) int {
+	data[offset] = uint8(v)
+	data[offset+1] = uint8(v >> 8)
+	data[offset+2] = uint8(v >> 16)
+	data[offset+3] = uint8(v >> 24)
+	data[offset+4] = uint8(v >> 32)
+	data[offset+5] = uint8(v >> 40)
+	data[offset+6] = uint8(v >> 48)
+	data[offset+7] = uint8(v >> 56)
+	return offset + 8
+}
+func encodeFixed32Kv(data []byte, offset int, v uint32) int {
+	data[offset] = uint8(v)
+	data[offset+1] = uint8(v >> 8)
+	data[offset+2] = uint8(v >> 16)
+	data[offset+3] = uint8(v >> 24)
+	return offset + 4
+}
+func encodeVarintKv(data []byte, offset int, v uint64) int {
+	for v >= 1<<7 {
+		data[offset] = uint8(v&0x7f | 0x80)
+		v >>= 7
+		offset++
+	}
+	data[offset] = uint8(v)
+	return offset + 1
+}
+func (m *KeyValue) Size() (n int) {
+	var l int
+	_ = l
+	l = len(m.Key)
+	if l > 0 {
+		n += 1 + l + sovKv(uint64(l))
+	}
+	if m.CreateRevision != 0 {
+		n += 1 + sovKv(uint64(m.CreateRevision))
+	}
+	if m.ModRevision != 0 {
+		n += 1 + sovKv(uint64(m.ModRevision))
+	}
+	if m.Version != 0 {
+		n += 1 + sovKv(uint64(m.Version))
+	}
+	l = len(m.Value)
+	if l > 0 {
+		n += 1 + l + sovKv(uint64(l))
+	}
+	if m.Lease != 0 {
+		n += 1 + sovKv(uint64(m.Lease))
+	}
+	return n
+}
+
+func (m *Event) Size() (n int) {
+	var l int
+	_ = l
+	if m.Type != 0 {
+		n += 1 + sovKv(uint64(m.Type))
+	}
+	if m.Kv != nil {
+		l = m.Kv.Size()
+		n += 1 + l + sovKv(uint64(l))
+	}
+	return n
+}
+
+func sovKv(x uint64) (n int) {
+	for {
+		n++
+		x >>= 7
+		if x == 0 {
+			break
+		}
+	}
+	return n
+}
+func sozKv(x uint64) (n int) {
+	return sovKv(uint64((x << 1) ^ uint64((int64(x) >> 63))))
+}
+func (m *KeyValue) Unmarshal(data []byte) error {
+	l := len(data)
+	iNdEx := 0
+	for iNdEx < l {
+		preIndex := iNdEx
+		var wire uint64
+		for shift := uint(0); ; shift += 7 {
+			if shift >= 64 {
+				return ErrIntOverflowKv
+			}
+			if iNdEx >= l {
+				return io.ErrUnexpectedEOF
+			}
+			b := data[iNdEx]
+			iNdEx++
+			wire |= (uint64(b) & 0x7F) << shift
+			if b < 0x80 {
+				break
+			}
+		}
+		fieldNum := int32(wire >> 3)
+		wireType := int(wire & 0x7)
+		if wireType == 4 {
+			return fmt.Errorf("proto: KeyValue: wiretype end group for non-group")
+		}
+		if fieldNum <= 0 {
+			return fmt.Errorf("proto: KeyValue: illegal tag %d (wire type %d)", fieldNum, wire)
+		}
+		switch fieldNum {
+		case 1:
+			if wireType != 2 {
+				return fmt.Errorf("proto: wrong wireType = %d for field Key", wireType)
+			}
+			var byteLen int
+			for shift := uint(0); ; shift += 7 {
+				if shift >= 64 {
+					return ErrIntOverflowKv
+				}
+				if iNdEx >= l {
+					return io.ErrUnexpectedEOF
+				}
+				b := data[iNdEx]
+				iNdEx++
+				byteLen |= (int(b) & 0x7F) << shift
+				if b < 0x80 {
+					break
+				}
+			}
+			if byteLen < 0 {
+				return ErrInvalidLengthKv
+			}
+			postIndex := iNdEx + byteLen
+			if postIndex > l {
+				return io.ErrUnexpectedEOF
+			}
+			m.Key = append(m.Key[:0], data[iNdEx:postIndex]...)
+			if m.Key == nil {
+				m.Key = []byte{}
+			}
+			iNdEx = postIndex
+		case 2:
+			if wireType != 0 {
+				return fmt.Errorf("proto: wrong wireType = %d for field CreateRevision", wireType)
+			}
+			m.CreateRevision = 0
+			for shift := uint(0); ; shift += 7 {
+				if shift >= 64 {
+					return ErrIntOverflowKv
+				}
+				if iNdEx >= l {
+					return io.ErrUnexpectedEOF
+				}
+				b := data[iNdEx]
+				iNdEx++
+				m.CreateRevision |= (int64(b) & 0x7F) << shift
+				if b < 0x80 {
+					break
+				}
+			}
+		case 3:
+			if wireType != 0 {
+				return fmt.Errorf("proto: wrong wireType = %d for field ModRevision", wireType)
+			}
+			m.ModRevision = 0
+			for shift := uint(0); ; shift += 7 {
+				if shift >= 64 {
+					return ErrIntOverflowKv
+				}
+				if iNdEx >= l {
+					return io.ErrUnexpectedEOF
+				}
+				b := data[iNdEx]
+				iNdEx++
+				m.ModRevision |= (int64(b) & 0x7F) << shift
+				if b < 0x80 {
+					break
+				}
+			}
+		case 4:
+			if wireType != 0 {
+				return fmt.Errorf("proto: wrong wireType = %d for field Version", wireType)
+			}
+			m.Version = 0
+			for shift := uint(0); ; shift += 7 {
+				if shift >= 64 {
+					return ErrIntOverflowKv
+				}
+				if iNdEx >= l {
+					return io.ErrUnexpectedEOF
+				}
+				b := data[iNdEx]
+				iNdEx++
+				m.Version |= (int64(b) & 0x7F) << shift
+				if b < 0x80 {
+					break
+				}
+			}
+		case 5:
+			if wireType != 2 {
+				return fmt.Errorf("proto: wrong wireType = %d for field Value", wireType)
+			}
+			var byteLen int
+			for shift := uint(0); ; shift += 7 {
+				if shift >= 64 {
+					return ErrIntOverflowKv
+				}
+				if iNdEx >= l {
+					return io.ErrUnexpectedEOF
+				}
+				b := data[iNdEx]
+				iNdEx++
+				byteLen |= (int(b) & 0x7F) << shift
+				if b < 0x80 {
+					break
+				}
+			}
+			if byteLen < 0 {
+				return ErrInvalidLengthKv
+			}
+			postIndex := iNdEx + byteLen
+			if postIndex > l {
+				return io.ErrUnexpectedEOF
+			}
+			m.Value = append(m.Value[:0], data[iNdEx:postIndex]...)
+			if m.Value == nil {
+				m.Value = []byte{}
+			}
+			iNdEx = postIndex
+		case 6:
+			if wireType != 0 {
+				return fmt.Errorf("proto: wrong wireType = %d for field Lease", wireType)
+			}
+			m.Lease = 0
+			for shift := uint(0); ; shift += 7 {
+				if shift >= 64 {
+					return ErrIntOverflowKv
+				}
+				if iNdEx >= l {
+					return io.ErrUnexpectedEOF
+				}
+				b := data[iNdEx]
+				iNdEx++
+				m.Lease |= (int64(b) & 0x7F) << shift
+				if b < 0x80 {
+					break
+				}
+			}
+		default:
+			iNdEx = preIndex
+			skippy, err := skipKv(data[iNdEx:])
+			if err != nil {
+				return err
+			}
+			if skippy < 0 {
+				return ErrInvalidLengthKv
+			}
+			if (iNdEx + skippy) > l {
+				return io.ErrUnexpectedEOF
+			}
+			iNdEx += skippy
+		}
+	}
+
+	if iNdEx > l {
+		return io.ErrUnexpectedEOF
+	}
+	return nil
+}
+func (m *Event) Unmarshal(data []byte) error {
+	l := len(data)
+	iNdEx := 0
+	for iNdEx < l {
+		preIndex := iNdEx
+		var wire uint64
+		for shift := uint(0); ; shift += 7 {
+			if shift >= 64 {
+				return ErrIntOverflowKv
+			}
+			if iNdEx >= l {
+				return io.ErrUnexpectedEOF
+			}
+			b := data[iNdEx]
+			iNdEx++
+			wire |= (uint64(b) & 0x7F) << shift
+			if b < 0x80 {
+				break
+			}
+		}
+		fieldNum := int32(wire >> 3)
+		wireType := int(wire & 0x7)
+		if wireType == 4 {
+			return fmt.Errorf("proto: Event: wiretype end group for non-group")
+		}
+		if fieldNum <= 0 {
+			return fmt.Errorf("proto: Event: illegal tag %d (wire type %d)", fieldNum, wire)
+		}
+		switch fieldNum {
+		case 1:
+			if wireType != 0 {
+				return fmt.Errorf("proto: wrong wireType = %d for field Type", wireType)
+			}
+			m.Type = 0
+			for shift := uint(0); ; shift += 7 {
+				if shift >= 64 {
+					return ErrIntOverflowKv
+				}
+				if iNdEx >= l {
+					return io.ErrUnexpectedEOF
+				}
+				b := data[iNdEx]
+				iNdEx++
+				m.Type |= (Event_EventType(b) & 0x7F) << shift
+				if b < 0x80 {
+					break
+				}
+			}
+		case 2:
+			if wireType != 2 {
+				return fmt.Errorf("proto: wrong wireType = %d for field Kv", wireType)
+			}
+			var msglen int
+			for shift := uint(0); ; shift += 7 {
+				if shift >= 64 {
+					return ErrIntOverflowKv
+				}
+				if iNdEx >= l {
+					return io.ErrUnexpectedEOF
+				}
+				b := data[iNdEx]
+				iNdEx++
+				msglen |= (int(b) & 0x7F) << shift
+				if b < 0x80 {
+					break
+				}
+			}
+			if msglen < 0 {
+				return ErrInvalidLengthKv
+			}
+			postIndex := iNdEx + msglen
+			if postIndex > l {
+				return io.ErrUnexpectedEOF
+			}
+			if m.Kv == nil {
+				m.Kv = &KeyValue{}
+			}
+			if err := m.Kv.Unmarshal(data[iNdEx:postIndex]); err != nil {
+				return err
+			}
+			iNdEx = postIndex
+		default:
+			iNdEx = preIndex
+			skippy, err := skipKv(data[iNdEx:])
+			if err != nil {
+				return err
+			}
+			if skippy < 0 {
+				return ErrInvalidLengthKv
+			}
+			if (iNdEx + skippy) > l {
+				return io.ErrUnexpectedEOF
+			}
+			iNdEx += skippy
+		}
+	}
+
+	if iNdEx > l {
+		return io.ErrUnexpectedEOF
+	}
+	return nil
+}
+func skipKv(data []byte) (n int, err error) {
+	l := len(data)
+	iNdEx := 0
+	for iNdEx < l {
+		var wire uint64
+		for shift := uint(0); ; shift += 7 {
+			if shift >= 64 {
+				return 0, ErrIntOverflowKv
+			}
+			if iNdEx >= l {
+				return 0, io.ErrUnexpectedEOF
+			}
+			b := data[iNdEx]
+			iNdEx++
+			wire |= (uint64(b) & 0x7F) << shift
+			if b < 0x80 {
+				break
+			}
+		}
+		wireType := int(wire & 0x7)
+		switch wireType {
+		case 0:
+			for shift := uint(0); ; shift += 7 {
+				if shift >= 64 {
+					return 0, ErrIntOverflowKv
+				}
+				if iNdEx >= l {
+					return 0, io.ErrUnexpectedEOF
+				}
+				iNdEx++
+				if data[iNdEx-1] < 0x80 {
+					break
+				}
+			}
+			return iNdEx, nil
+		case 1:
+			iNdEx += 8
+			return iNdEx, nil
+		case 2:
+			var length int
+			for shift := uint(0); ; shift += 7 {
+				if shift >= 64 {
+					return 0, ErrIntOverflowKv
+				}
+				if iNdEx >= l {
+					return 0, io.ErrUnexpectedEOF
+				}
+				b := data[iNdEx]
+				iNdEx++
+				length |= (int(b) & 0x7F) << shift
+				if b < 0x80 {
+					break
+				}
+			}
+			iNdEx += length
+			if length < 0 {
+				return 0, ErrInvalidLengthKv
+			}
+			return iNdEx, nil
+		case 3:
+			for {
+				var innerWire uint64
+				var start int = iNdEx
+				for shift := uint(0); ; shift += 7 {
+					if shift >= 64 {
+						return 0, ErrIntOverflowKv
+					}
+					if iNdEx >= l {
+						return 0, io.ErrUnexpectedEOF
+					}
+					b := data[iNdEx]
+					iNdEx++
+					innerWire |= (uint64(b) & 0x7F) << shift
+					if b < 0x80 {
+						break
+					}
+				}
+				innerWireType := int(innerWire & 0x7)
+				if innerWireType == 4 {
+					break
+				}
+				next, err := skipKv(data[start:])
+				if err != nil {
+					return 0, err
+				}
+				iNdEx = start + next
+			}
+			return iNdEx, nil
+		case 4:
+			return iNdEx, nil
+		case 5:
+			iNdEx += 4
+			return iNdEx, nil
+		default:
+			return 0, fmt.Errorf("proto: illegal wireType %d", wireType)
+		}
+	}
+	panic("unreachable")
+}
+
+var (
+	ErrInvalidLengthKv = fmt.Errorf("proto: negative length found during unmarshaling")
+	ErrIntOverflowKv   = fmt.Errorf("proto: integer overflow")
+)
+
+var fileDescriptorKv = []byte{
+	// 255 bytes of a gzipped FileDescriptorProto
+	0x1f, 0x8b, 0x08, 0x00, 0x00, 0x09, 0x6e, 0x88, 0x02, 0xff, 0xe2, 0xe2, 0xc8, 0x2e, 0xd3, 0x2b,
+	0x28, 0xca, 0x2f, 0xc9, 0x17, 0x62, 0xcb, 0x2d, 0x4b, 0x4e, 0x2e, 0x48, 0x92, 0x12, 0x49, 0xcf,
+	0x4f, 0xcf, 0x07, 0x0b, 0xe9, 0x83, 0x58, 0x10, 0x59, 0xa5, 0x52, 0x2e, 0x0e, 0xef, 0xd4, 0xca,
+	0xb0, 0xc4, 0x9c, 0xd2, 0x54, 0x21, 0x6e, 0x2e, 0xe6, 0xec, 0xd4, 0x4a, 0x09, 0x46, 0x05, 0x46,
+	0x0d, 0x1e, 0x21, 0x71, 0x2e, 0xfe, 0xe4, 0xa2, 0xd4, 0xc4, 0x92, 0xd4, 0xf8, 0xa2, 0xd4, 0xb2,
+	0xcc, 0xe2, 0xcc, 0xfc, 0x3c, 0x09, 0x26, 0xa0, 0x04, 0xb3, 0x90, 0x08, 0x17, 0x4f, 0x6e, 0x7e,
+	0x0a, 0x42, 0x94, 0x19, 0x2c, 0xca, 0xcf, 0xc5, 0x5e, 0x96, 0x5a, 0x04, 0x16, 0x60, 0x01, 0x0b,
+	0xf0, 0x72, 0xb1, 0x96, 0x81, 0x4c, 0x95, 0x60, 0x05, 0x1b, 0x07, 0xe4, 0xe6, 0xa4, 0x26, 0x16,
+	0xa7, 0x4a, 0xb0, 0x81, 0x64, 0x95, 0xaa, 0xb8, 0x58, 0x5d, 0xcb, 0x52, 0xf3, 0x4a, 0x84, 0x54,
+	0xb9, 0x58, 0x4a, 0x2a, 0x0b, 0x52, 0xc1, 0x96, 0xf2, 0x19, 0x89, 0xeb, 0x41, 0x1c, 0xab, 0x07,
+	0x96, 0x84, 0x90, 0x21, 0x40, 0x69, 0x21, 0x19, 0x2e, 0xa6, 0xec, 0x32, 0xb0, 0x03, 0xb8, 0x8d,
+	0x04, 0x60, 0x8a, 0x60, 0x0e, 0x57, 0xd2, 0xe1, 0xe2, 0x44, 0x28, 0x65, 0xe7, 0x62, 0x0e, 0x08,
+	0x0d, 0x11, 0x60, 0x10, 0xe2, 0xe2, 0x62, 0x73, 0x71, 0xf5, 0x71, 0x0d, 0x71, 0x15, 0x60, 0x04,
+	0xb1, 0x5d, 0x23, 0x02, 0x3c, 0x83, 0x5c, 0x05, 0x98, 0x9c, 0x44, 0x4e, 0x3c, 0x94, 0x63, 0xb8,
+	0x00, 0xc4, 0x27, 0x1e, 0xc9, 0x31, 0x5e, 0x00, 0xe2, 0x07, 0x40, 0x9c, 0xc4, 0x06, 0x0e, 0x0f,
+	0x63, 0x40, 0x00, 0x00, 0x00, 0xff, 0xff, 0x4a, 0x2d, 0xc3, 0x10, 0x39, 0x01, 0x00, 0x00,
+}
diff --git a/mvcc/mvccpb/kv.proto b/mvcc/mvccpb/kv.proto
new file mode 100644
index 0000000..49b590f
--- /dev/null
+++ b/mvcc/mvccpb/kv.proto
@@ -0,0 +1,47 @@
+syntax = "proto3";
+package mvccpb;
+
+import "gogoproto/gogo.proto";
+
+option (gogoproto.marshaler_all) = true;
+option (gogoproto.sizer_all) = true;
+option (gogoproto.unmarshaler_all) = true;
+option (gogoproto.goproto_getters_all) = false;
+option (gogoproto.goproto_enum_prefix_all) = false;
+
+message KeyValue {
+  // key is the key in bytes. An empty key is not allowed.
+  bytes key = 1;
+  // create_revision is the revision of last creation on this key.
+  int64 create_revision = 2;
+  // mod_revision is the revision of last modification on this key.
+  int64 mod_revision = 3;
+  // version is the version of the key. A deletion resets
+  // the version to zero and any modification of the key
+  // increases its version.
+  int64 version = 4;
+  // value is the value held by the key, in bytes.
+  bytes value = 5;
+  // lease is the ID of the lease that attached to key.
+  // When the attached lease expires, the key will be deleted.
+  // If lease is 0, then no lease is attached to the key.
+  int64 lease = 6;
+}
+
+message Event {
+  enum EventType {
+    PUT = 0;
+    DELETE = 1;
+    EXPIRE = 2;
+  }
+  // type is the kind of event. If type is a PUT, it indicates
+  // new data has been stored to the key. If type is a DELETE,
+  // it indicates the key was deleted.
+  EventType type = 1;
+  // kv holds the KeyValue for the event.
+  // A PUT event contains current kv pair.
+  // A PUT event with kv.Version=1 indicates the creation of a key.
+  // A DELETE/EXPIRE event contains the deleted key with
+  // its modification revision set to the revision of deletion.
+  KeyValue kv = 2;
+}
diff --git a/mvcc/revision.go b/mvcc/revision.go
new file mode 100644
index 0000000..acca89b
--- /dev/null
+++ b/mvcc/revision.go
@@ -0,0 +1,67 @@
+// Copyright 2015 CoreOS, Inc.
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//     http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+package mvcc
+
+import "encoding/binary"
+
+// revBytesLen is the byte length of a normal revision.
+// First 8 bytes is the revision.main in big-endian format. The 9th byte
+// is a '_'. The last 8 bytes is the revision.sub in big-endian format.
+const revBytesLen = 8 + 1 + 8
+
+// A revision indicates modification of the key-value space.
+// The set of changes that share same main revision changes the key-value space atomically.
+type revision struct {
+	// main is the main revision of a set of changes that happen atomically.
+	main int64
+
+	// sub is the the sub revision of a change in a set of changes that happen
+	// atomically. Each change has different increasing sub revision in that
+	// set.
+	sub int64
+}
+
+func (a revision) GreaterThan(b revision) bool {
+	if a.main > b.main {
+		return true
+	}
+	if a.main < b.main {
+		return false
+	}
+	return a.sub > b.sub
+}
+
+func newRevBytes() []byte {
+	return make([]byte, revBytesLen, markedRevBytesLen)
+}
+
+func revToBytes(rev revision, bytes []byte) {
+	binary.BigEndian.PutUint64(bytes, uint64(rev.main))
+	bytes[8] = '_'
+	binary.BigEndian.PutUint64(bytes[9:], uint64(rev.sub))
+}
+
+func bytesToRev(bytes []byte) revision {
+	return revision{
+		main: int64(binary.BigEndian.Uint64(bytes[0:8])),
+		sub:  int64(binary.BigEndian.Uint64(bytes[9:])),
+	}
+}
+
+type revisions []revision
+
+func (a revisions) Len() int           { return len(a) }
+func (a revisions) Less(i, j int) bool { return a[j].GreaterThan(a[i]) }
+func (a revisions) Swap(i, j int)      { a[i], a[j] = a[j], a[i] }
diff --git a/mvcc/revision_test.go b/mvcc/revision_test.go
new file mode 100644
index 0000000..4bc1677
--- /dev/null
+++ b/mvcc/revision_test.go
@@ -0,0 +1,53 @@
+// Copyright 2015 CoreOS, Inc.
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//     http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+package mvcc
+
+import (
+	"bytes"
+	"math"
+	"reflect"
+	"testing"
+)
+
+// TestRevision tests that revision could be encoded to and decoded from
+// bytes slice. Moreover, the lexicographical order of its byte slice representation
+// follows the order of (main, sub).
+func TestRevision(t *testing.T) {
+	tests := []revision{
+		// order in (main, sub)
+		{},
+		{main: 1, sub: 0},
+		{main: 1, sub: 1},
+		{main: 2, sub: 0},
+		{main: math.MaxInt64, sub: math.MaxInt64},
+	}
+
+	bs := make([][]byte, len(tests))
+	for i, tt := range tests {
+		b := newRevBytes()
+		revToBytes(tt, b)
+		bs[i] = b
+
+		if grev := bytesToRev(b); !reflect.DeepEqual(grev, tt) {
+			t.Errorf("#%d: revision = %+v, want %+v", i, grev, tt)
+		}
+	}
+
+	for i := 0; i < len(tests)-1; i++ {
+		if bytes.Compare(bs[i], bs[i+1]) >= 0 {
+			t.Errorf("#%d: %v (%+v) should be smaller than %v (%+v)", i, bs[i], tests[i], bs[i+1], tests[i+1])
+		}
+	}
+}
diff --git a/mvcc/watchable_store.go b/mvcc/watchable_store.go
new file mode 100644
index 0000000..2815e7c
--- /dev/null
+++ b/mvcc/watchable_store.go
@@ -0,0 +1,386 @@
+// Copyright 2015 CoreOS, Inc.
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//     http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+package mvcc
+
+import (
+	"log"
+	"sync"
+	"time"
+
+	"github.com/coreos/etcd/lease"
+	"github.com/coreos/etcd/mvcc/backend"
+	"github.com/coreos/etcd/mvcc/mvccpb"
+)
+
+const (
+	// chanBufLen is the length of the buffered chan
+	// for sending out watched events.
+	// TODO: find a good buf value. 1024 is just a random one that
+	// seems to be reasonable.
+	chanBufLen = 1024
+)
+
+type watchable interface {
+	watch(key, end []byte, startRev int64, id WatchID, ch chan<- WatchResponse) (*watcher, cancelFunc)
+	progress(w *watcher)
+	rev() int64
+}
+
+type watchableStore struct {
+	mu sync.Mutex
+
+	*store
+
+	// contains all unsynced watchers that needs to sync with events that have happened
+	unsynced watcherGroup
+
+	// contains all synced watchers that are in sync with the progress of the store.
+	// The key of the map is the key that the watcher watches on.
+	synced watcherGroup
+
+	stopc chan struct{}
+	wg    sync.WaitGroup
+}
+
+// cancelFunc updates unsynced and synced maps when running
+// cancel operations.
+type cancelFunc func()
+
+func New(b backend.Backend, le lease.Lessor, ig ConsistentIndexGetter) ConsistentWatchableKV {
+	return newWatchableStore(b, le, ig)
+}
+
+func newWatchableStore(b backend.Backend, le lease.Lessor, ig ConsistentIndexGetter) *watchableStore {
+	s := &watchableStore{
+		store:    NewStore(b, le, ig),
+		unsynced: newWatcherGroup(),
+		synced:   newWatcherGroup(),
+		stopc:    make(chan struct{}),
+	}
+	if s.le != nil {
+		// use this store as the deleter so revokes trigger watch events
+		s.le.SetRangeDeleter(s)
+	}
+	s.wg.Add(1)
+	go s.syncWatchersLoop()
+	return s
+}
+
+func (s *watchableStore) Put(key, value []byte, lease lease.LeaseID) (rev int64) {
+	s.mu.Lock()
+	defer s.mu.Unlock()
+
+	rev = s.store.Put(key, value, lease)
+	changes := s.store.getChanges()
+	if len(changes) != 1 {
+		log.Panicf("unexpected len(changes) != 1 after put")
+	}
+
+	ev := mvccpb.Event{
+		Type: mvccpb.PUT,
+		Kv:   &changes[0],
+	}
+	s.notify(rev, []mvccpb.Event{ev})
+	return rev
+}
+
+func (s *watchableStore) DeleteRange(key, end []byte) (n, rev int64) {
+	s.mu.Lock()
+	defer s.mu.Unlock()
+
+	n, rev = s.store.DeleteRange(key, end)
+	changes := s.store.getChanges()
+
+	if len(changes) != int(n) {
+		log.Panicf("unexpected len(changes) != n after deleteRange")
+	}
+
+	if n == 0 {
+		return n, rev
+	}
+
+	evs := make([]mvccpb.Event, n)
+	for i, change := range changes {
+		evs[i] = mvccpb.Event{
+			Type: mvccpb.DELETE,
+			Kv:   &change}
+		evs[i].Kv.ModRevision = rev
+	}
+	s.notify(rev, evs)
+	return n, rev
+}
+
+func (s *watchableStore) TxnBegin() int64 {
+	s.mu.Lock()
+	return s.store.TxnBegin()
+}
+
+func (s *watchableStore) TxnEnd(txnID int64) error {
+	err := s.store.TxnEnd(txnID)
+	if err != nil {
+		return err
+	}
+
+	changes := s.getChanges()
+	if len(changes) == 0 {
+		s.mu.Unlock()
+		return nil
+	}
+
+	rev := s.store.Rev()
+	evs := make([]mvccpb.Event, len(changes))
+	for i, change := range changes {
+		switch change.CreateRevision {
+		case 0:
+			evs[i] = mvccpb.Event{
+				Type: mvccpb.DELETE,
+				Kv:   &changes[i]}
+			evs[i].Kv.ModRevision = rev
+		default:
+			evs[i] = mvccpb.Event{
+				Type: mvccpb.PUT,
+				Kv:   &changes[i]}
+		}
+	}
+
+	s.notify(rev, evs)
+	s.mu.Unlock()
+
+	return nil
+}
+
+func (s *watchableStore) Close() error {
+	close(s.stopc)
+	s.wg.Wait()
+	return s.store.Close()
+}
+
+func (s *watchableStore) NewWatchStream() WatchStream {
+	watchStreamGauge.Inc()
+	return &watchStream{
+		watchable: s,
+		ch:        make(chan WatchResponse, chanBufLen),
+		cancels:   make(map[WatchID]cancelFunc),
+		watchers:  make(map[WatchID]*watcher),
+	}
+}
+
+func (s *watchableStore) watch(key, end []byte, startRev int64, id WatchID, ch chan<- WatchResponse) (*watcher, cancelFunc) {
+	s.mu.Lock()
+	defer s.mu.Unlock()
+
+	wa := &watcher{
+		key: key,
+		end: end,
+		cur: startRev,
+		id:  id,
+		ch:  ch,
+	}
+
+	s.store.mu.Lock()
+	synced := startRev > s.store.currentRev.main || startRev == 0
+	if synced {
+		wa.cur = s.store.currentRev.main + 1
+		if startRev > wa.cur {
+			wa.cur = startRev
+		}
+	}
+	s.store.mu.Unlock()
+	if synced {
+		s.synced.add(wa)
+	} else {
+		slowWatcherGauge.Inc()
+		s.unsynced.add(wa)
+	}
+	watcherGauge.Inc()
+
+	cancel := cancelFunc(func() {
+		s.mu.Lock()
+		defer s.mu.Unlock()
+		// remove references of the watcher
+		if s.unsynced.delete(wa) {
+			slowWatcherGauge.Dec()
+			watcherGauge.Dec()
+			return
+		}
+
+		if s.synced.delete(wa) {
+			watcherGauge.Dec()
+		}
+		// If we cannot find it, it should have finished watch.
+	})
+
+	return wa, cancel
+}
+
+// syncWatchersLoop syncs the watcher in the unsynced map every 100ms.
+func (s *watchableStore) syncWatchersLoop() {
+	defer s.wg.Done()
+
+	for {
+		s.mu.Lock()
+		s.syncWatchers()
+		s.mu.Unlock()
+
+		select {
+		case <-time.After(100 * time.Millisecond):
+		case <-s.stopc:
+			return
+		}
+	}
+}
+
+// syncWatchers periodically syncs unsynced watchers by: Iterate all unsynced
+// watchers to get the minimum revision within its range, skipping the
+// watcher if its current revision is behind the compact revision of the
+// store. And use this minimum revision to get all key-value pairs. Then send
+// those events to watchers.
+func (s *watchableStore) syncWatchers() {
+	s.store.mu.Lock()
+	defer s.store.mu.Unlock()
+
+	if s.unsynced.size() == 0 {
+		return
+	}
+
+	// in order to find key-value pairs from unsynced watchers, we need to
+	// find min revision index, and these revisions can be used to
+	// query the backend store of key-value pairs
+	curRev := s.store.currentRev.main
+	compactionRev := s.store.compactMainRev
+	minRev := s.unsynced.scanMinRev(curRev, compactionRev)
+	minBytes, maxBytes := newRevBytes(), newRevBytes()
+	revToBytes(revision{main: minRev}, minBytes)
+	revToBytes(revision{main: curRev + 1}, maxBytes)
+
+	// UnsafeRange returns keys and values. And in boltdb, keys are revisions.
+	// values are actual key-value pairs in backend.
+	tx := s.store.b.BatchTx()
+	tx.Lock()
+	revs, vs := tx.UnsafeRange(keyBucketName, minBytes, maxBytes, 0)
+	evs := kvsToEvents(&s.unsynced, revs, vs)
+	tx.Unlock()
+
+	wb := newWatcherBatch(&s.unsynced, evs)
+
+	for w, eb := range wb {
+		select {
+		// s.store.Rev also uses Lock, so just return directly
+		case w.ch <- WatchResponse{WatchID: w.id, Events: eb.evs, Revision: s.store.currentRev.main}:
+			pendingEventsGauge.Add(float64(len(eb.evs)))
+		default:
+			// TODO: handle the full unsynced watchers.
+			// continue to process other watchers for now, the full ones
+			// will be processed next time and hopefully it will not be full.
+			continue
+		}
+		if eb.moreRev != 0 {
+			w.cur = eb.moreRev
+			continue
+		}
+		w.cur = curRev
+		s.synced.add(w)
+		s.unsynced.delete(w)
+	}
+
+	// bring all un-notified watchers to synced.
+	for w := range s.unsynced.watchers {
+		if !wb.contains(w) {
+			w.cur = curRev
+			s.synced.add(w)
+			s.unsynced.delete(w)
+		}
+	}
+
+	slowWatcherGauge.Set(float64(s.unsynced.size()))
+}
+
+// kvsToEvents gets all events for the watchers from all key-value pairs
+func kvsToEvents(wg *watcherGroup, revs, vals [][]byte) (evs []mvccpb.Event) {
+	for i, v := range vals {
+		var kv mvccpb.KeyValue
+		if err := kv.Unmarshal(v); err != nil {
+			log.Panicf("mvcc: cannot unmarshal event: %v", err)
+		}
+
+		if !wg.contains(string(kv.Key)) {
+			continue
+		}
+
+		ty := mvccpb.PUT
+		if isTombstone(revs[i]) {
+			ty = mvccpb.DELETE
+			// patch in mod revision so watchers won't skip
+			kv.ModRevision = bytesToRev(revs[i]).main
+		}
+		evs = append(evs, mvccpb.Event{Kv: &kv, Type: ty})
+	}
+	return evs
+}
+
+// notify notifies the fact that given event at the given rev just happened to
+// watchers that watch on the key of the event.
+func (s *watchableStore) notify(rev int64, evs []mvccpb.Event) {
+	for w, eb := range newWatcherBatch(&s.synced, evs) {
+		if eb.revs != 1 {
+			log.Panicf("mvcc: unexpected multiple revisions in notification")
+		}
+		select {
+		case w.ch <- WatchResponse{WatchID: w.id, Events: eb.evs, Revision: s.Rev()}:
+			pendingEventsGauge.Add(float64(len(eb.evs)))
+		default:
+			// move slow watcher to unsynced
+			w.cur = rev
+			s.unsynced.add(w)
+			s.synced.delete(w)
+			slowWatcherGauge.Inc()
+		}
+	}
+}
+
+func (s *watchableStore) rev() int64 { return s.store.Rev() }
+
+func (s *watchableStore) progress(w *watcher) {
+	s.mu.Lock()
+	defer s.mu.Unlock()
+
+	if _, ok := s.synced.watchers[w]; ok {
+		select {
+		case w.ch <- WatchResponse{WatchID: w.id, Revision: s.rev()}:
+		default:
+			// If the ch is full, this watcher is receiving events.
+			// We do not need to send progress at all.
+		}
+	}
+}
+
+type watcher struct {
+	// the watcher key
+	key []byte
+	// end indicates the end of the range to watch.
+	// If end is set, the watcher is on a range.
+	end []byte
+
+	// cur is the current watcher revision of a unsynced watcher.
+	// cur will be updated for unsynced watcher while it is catching up.
+	// cur is startRev of a synced watcher.
+	// cur will not be updated for synced watcher.
+	cur int64
+	id  WatchID
+
+	// a chan to send out the watch response.
+	// The chan might be shared with other watchers.
+	ch chan<- WatchResponse
+}
diff --git a/mvcc/watchable_store_bench_test.go b/mvcc/watchable_store_bench_test.go
new file mode 100644
index 0000000..605e01d
--- /dev/null
+++ b/mvcc/watchable_store_bench_test.go
@@ -0,0 +1,127 @@
+// Copyright 2015 CoreOS, Inc.
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//     http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+package mvcc
+
+import (
+	"math/rand"
+	"os"
+	"testing"
+
+	"github.com/coreos/etcd/lease"
+	"github.com/coreos/etcd/mvcc/backend"
+)
+
+// Benchmarks on cancel function performance for unsynced watchers
+// in a WatchableStore. It creates k*N watchers to populate unsynced
+// with a reasonably large number of watchers. And measures the time it
+// takes to cancel N watchers out of k*N watchers. The performance is
+// expected to differ depending on the unsynced member implementation.
+// TODO: k is an arbitrary constant. We need to figure out what factor
+// we should put to simulate the real-world use cases.
+func BenchmarkWatchableStoreUnsyncedCancel(b *testing.B) {
+	be, tmpPath := backend.NewDefaultTmpBackend()
+	s := NewStore(be, &lease.FakeLessor{}, nil)
+
+	// manually create watchableStore instead of newWatchableStore
+	// because newWatchableStore periodically calls syncWatchersLoop
+	// method to sync watchers in unsynced map. We want to keep watchers
+	// in unsynced for this benchmark.
+	ws := &watchableStore{
+		store:    s,
+		unsynced: newWatcherGroup(),
+
+		// to make the test not crash from assigning to nil map.
+		// 'synced' doesn't get populated in this test.
+		synced: newWatcherGroup(),
+	}
+
+	defer func() {
+		ws.store.Close()
+		os.Remove(tmpPath)
+	}()
+
+	// Put a key so that we can spawn watchers on that key
+	// (testKey in this test). This increases the rev to 1,
+	// and later we can we set the watcher's startRev to 1,
+	// and force watchers to be in unsynced.
+	testKey := []byte("foo")
+	testValue := []byte("bar")
+	s.Put(testKey, testValue, lease.NoLease)
+
+	w := ws.NewWatchStream()
+
+	const k int = 2
+	benchSampleN := b.N
+	watcherN := k * benchSampleN
+
+	watchIDs := make([]WatchID, watcherN)
+	for i := 0; i < watcherN; i++ {
+		// non-0 value to keep watchers in unsynced
+		watchIDs[i] = w.Watch(testKey, nil, 1)
+	}
+
+	// random-cancel N watchers to make it not biased towards
+	// data structures with an order, such as slice.
+	ix := rand.Perm(watcherN)
+
+	b.ResetTimer()
+	b.ReportAllocs()
+
+	// cancel N watchers
+	for _, idx := range ix[:benchSampleN] {
+		if err := w.Cancel(watchIDs[idx]); err != nil {
+			b.Error(err)
+		}
+	}
+}
+
+func BenchmarkWatchableStoreSyncedCancel(b *testing.B) {
+	be, tmpPath := backend.NewDefaultTmpBackend()
+	s := newWatchableStore(be, &lease.FakeLessor{}, nil)
+
+	defer func() {
+		s.store.Close()
+		os.Remove(tmpPath)
+	}()
+
+	// Put a key so that we can spawn watchers on that key
+	testKey := []byte("foo")
+	testValue := []byte("bar")
+	s.Put(testKey, testValue, lease.NoLease)
+
+	w := s.NewWatchStream()
+
+	// put 1 million watchers on the same key
+	const watcherN = 1000000
+
+	watchIDs := make([]WatchID, watcherN)
+	for i := 0; i < watcherN; i++ {
+		// 0 for startRev to keep watchers in synced
+		watchIDs[i] = w.Watch(testKey, nil, 0)
+	}
+
+	// randomly cancel watchers to make it not biased towards
+	// data structures with an order, such as slice.
+	ix := rand.Perm(watcherN)
+
+	b.ResetTimer()
+	b.ReportAllocs()
+
+	for _, idx := range ix {
+		if err := w.Cancel(watchIDs[idx]); err != nil {
+			b.Error(err)
+		}
+	}
+}
diff --git a/mvcc/watchable_store_test.go b/mvcc/watchable_store_test.go
new file mode 100644
index 0000000..2d6fe9d
--- /dev/null
+++ b/mvcc/watchable_store_test.go
@@ -0,0 +1,426 @@
+// Copyright 2015 CoreOS, Inc.
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//     http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+package mvcc
+
+import (
+	"bytes"
+	"os"
+	"reflect"
+	"testing"
+	"time"
+
+	"github.com/coreos/etcd/lease"
+	"github.com/coreos/etcd/mvcc/backend"
+	"github.com/coreos/etcd/mvcc/mvccpb"
+)
+
+func TestWatch(t *testing.T) {
+	b, tmpPath := backend.NewDefaultTmpBackend()
+	s := newWatchableStore(b, &lease.FakeLessor{}, nil)
+
+	defer func() {
+		s.store.Close()
+		os.Remove(tmpPath)
+	}()
+
+	testKey := []byte("foo")
+	testValue := []byte("bar")
+	s.Put(testKey, testValue, lease.NoLease)
+
+	w := s.NewWatchStream()
+	w.Watch(testKey, nil, 0)
+
+	if !s.synced.contains(string(testKey)) {
+		// the key must have had an entry in synced
+		t.Errorf("existence = false, want true")
+	}
+}
+
+func TestNewWatcherCancel(t *testing.T) {
+	b, tmpPath := backend.NewDefaultTmpBackend()
+	s := newWatchableStore(b, &lease.FakeLessor{}, nil)
+
+	defer func() {
+		s.store.Close()
+		os.Remove(tmpPath)
+	}()
+	testKey := []byte("foo")
+	testValue := []byte("bar")
+	s.Put(testKey, testValue, lease.NoLease)
+
+	w := s.NewWatchStream()
+	wt := w.Watch(testKey, nil, 0)
+
+	if err := w.Cancel(wt); err != nil {
+		t.Error(err)
+	}
+
+	if s.synced.contains(string(testKey)) {
+		// the key shoud have been deleted
+		t.Errorf("existence = true, want false")
+	}
+}
+
+// TestCancelUnsynced tests if running CancelFunc removes watchers from unsynced.
+func TestCancelUnsynced(t *testing.T) {
+	b, tmpPath := backend.NewDefaultTmpBackend()
+
+	// manually create watchableStore instead of newWatchableStore
+	// because newWatchableStore automatically calls syncWatchers
+	// method to sync watchers in unsynced map. We want to keep watchers
+	// in unsynced to test if syncWatchers works as expected.
+	s := &watchableStore{
+		store:    NewStore(b, &lease.FakeLessor{}, nil),
+		unsynced: newWatcherGroup(),
+
+		// to make the test not crash from assigning to nil map.
+		// 'synced' doesn't get populated in this test.
+		synced: newWatcherGroup(),
+	}
+
+	defer func() {
+		s.store.Close()
+		os.Remove(tmpPath)
+	}()
+
+	// Put a key so that we can spawn watchers on that key.
+	// (testKey in this test). This increases the rev to 1,
+	// and later we can we set the watcher's startRev to 1,
+	// and force watchers to be in unsynced.
+	testKey := []byte("foo")
+	testValue := []byte("bar")
+	s.Put(testKey, testValue, lease.NoLease)
+
+	w := s.NewWatchStream()
+
+	// arbitrary number for watchers
+	watcherN := 100
+
+	// create watcherN of watch ids to cancel
+	watchIDs := make([]WatchID, watcherN)
+	for i := 0; i < watcherN; i++ {
+		// use 1 to keep watchers in unsynced
+		watchIDs[i] = w.Watch(testKey, nil, 1)
+	}
+
+	for _, idx := range watchIDs {
+		if err := w.Cancel(idx); err != nil {
+			t.Error(err)
+		}
+	}
+
+	// After running CancelFunc
+	//
+	// unsynced should be empty
+	// because cancel removes watcher from unsynced
+	if size := s.unsynced.size(); size != 0 {
+		t.Errorf("unsynced size = %d, want 0", size)
+	}
+}
+
+// TestSyncWatchers populates unsynced watcher map and tests syncWatchers
+// method to see if it correctly sends events to channel of unsynced watchers
+// and moves these watchers to synced.
+func TestSyncWatchers(t *testing.T) {
+	b, tmpPath := backend.NewDefaultTmpBackend()
+
+	s := &watchableStore{
+		store:    NewStore(b, &lease.FakeLessor{}, nil),
+		unsynced: newWatcherGroup(),
+		synced:   newWatcherGroup(),
+	}
+
+	defer func() {
+		s.store.Close()
+		os.Remove(tmpPath)
+	}()
+
+	testKey := []byte("foo")
+	testValue := []byte("bar")
+	s.Put(testKey, testValue, lease.NoLease)
+
+	w := s.NewWatchStream()
+
+	// arbitrary number for watchers
+	watcherN := 100
+
+	for i := 0; i < watcherN; i++ {
+		// specify rev as 1 to keep watchers in unsynced
+		w.Watch(testKey, nil, 1)
+	}
+
+	// Before running s.syncWatchers() synced should be empty because we manually
+	// populate unsynced only
+	sws := s.synced.watcherSetByKey(string(testKey))
+	uws := s.unsynced.watcherSetByKey(string(testKey))
+
+	if len(sws) != 0 {
+		t.Fatalf("synced[string(testKey)] size = %d, want 0", len(sws))
+	}
+	// unsynced should not be empty because we manually populated unsynced only
+	if len(uws) != watcherN {
+		t.Errorf("unsynced size = %d, want %d", len(uws), watcherN)
+	}
+
+	// this should move all unsynced watchers to synced ones
+	s.syncWatchers()
+
+	sws = s.synced.watcherSetByKey(string(testKey))
+	uws = s.unsynced.watcherSetByKey(string(testKey))
+
+	// After running s.syncWatchers(), synced should not be empty because syncwatchers
+	// populates synced in this test case
+	if len(sws) != watcherN {
+		t.Errorf("synced[string(testKey)] size = %d, want %d", len(sws), watcherN)
+	}
+
+	// unsynced should be empty because syncwatchers is expected to move all watchers
+	// from unsynced to synced in this test case
+	if len(uws) != 0 {
+		t.Errorf("unsynced size = %d, want 0", len(uws))
+	}
+
+	for w := range sws {
+		if w.cur != s.Rev() {
+			t.Errorf("w.cur = %d, want %d", w.cur, s.Rev())
+		}
+	}
+
+	if len(w.(*watchStream).ch) != watcherN {
+		t.Errorf("watched event size = %d, want %d", len(w.(*watchStream).ch), watcherN)
+	}
+
+	evs := (<-w.(*watchStream).ch).Events
+	if len(evs) != 1 {
+		t.Errorf("len(evs) got = %d, want = 1", len(evs))
+	}
+	if evs[0].Type != mvccpb.PUT {
+		t.Errorf("got = %v, want = %v", evs[0].Type, mvccpb.PUT)
+	}
+	if !bytes.Equal(evs[0].Kv.Key, testKey) {
+		t.Errorf("got = %s, want = %s", evs[0].Kv.Key, testKey)
+	}
+	if !bytes.Equal(evs[0].Kv.Value, testValue) {
+		t.Errorf("got = %s, want = %s", evs[0].Kv.Value, testValue)
+	}
+}
+
+// TestWatchCompacted tests a watcher that watches on a compacted revision.
+func TestWatchCompacted(t *testing.T) {
+	b, tmpPath := backend.NewDefaultTmpBackend()
+	s := newWatchableStore(b, &lease.FakeLessor{}, nil)
+
+	defer func() {
+		s.store.Close()
+		os.Remove(tmpPath)
+	}()
+	testKey := []byte("foo")
+	testValue := []byte("bar")
+
+	maxRev := 10
+	compactRev := int64(5)
+	for i := 0; i < maxRev; i++ {
+		s.Put(testKey, testValue, lease.NoLease)
+	}
+	_, err := s.Compact(compactRev)
+	if err != nil {
+		t.Fatalf("failed to compact kv (%v)", err)
+	}
+
+	w := s.NewWatchStream()
+	wt := w.Watch(testKey, nil, compactRev-1)
+
+	select {
+	case resp := <-w.Chan():
+		if resp.WatchID != wt {
+			t.Errorf("resp.WatchID = %x, want %x", resp.WatchID, wt)
+		}
+		if resp.CompactRevision == 0 {
+			t.Errorf("resp.Compacted = %v, want %v", resp.CompactRevision, compactRev)
+		}
+	case <-time.After(1 * time.Second):
+		t.Fatalf("failed to receive response (timeout)")
+	}
+}
+
+func TestWatchFutureRev(t *testing.T) {
+	b, tmpPath := backend.NewDefaultTmpBackend()
+	s := newWatchableStore(b, &lease.FakeLessor{}, nil)
+
+	defer func() {
+		s.store.Close()
+		os.Remove(tmpPath)
+	}()
+
+	testKey := []byte("foo")
+	testValue := []byte("bar")
+
+	w := s.NewWatchStream()
+	wrev := int64(10)
+	w.Watch(testKey, nil, wrev)
+
+	for i := 0; i < 10; i++ {
+		rev := s.Put(testKey, testValue, lease.NoLease)
+		if rev >= wrev {
+			break
+		}
+	}
+
+	select {
+	case resp := <-w.Chan():
+		if resp.Revision != wrev {
+			t.Fatalf("rev = %d, want %d", resp.Revision, wrev)
+		}
+		if len(resp.Events) != 1 {
+			t.Fatalf("failed to get events from the response")
+		}
+		if resp.Events[0].Kv.ModRevision != wrev {
+			t.Fatalf("kv.rev = %d, want %d", resp.Events[0].Kv.ModRevision, wrev)
+		}
+	case <-time.After(time.Second):
+		t.Fatal("failed to receive event in 1 second.")
+	}
+}
+
+// TestWatchBatchUnsynced tests batching on unsynced watchers
+func TestWatchBatchUnsynced(t *testing.T) {
+	b, tmpPath := backend.NewDefaultTmpBackend()
+	s := newWatchableStore(b, &lease.FakeLessor{}, nil)
+
+	oldMaxRevs := watchBatchMaxRevs
+	defer func() {
+		watchBatchMaxRevs = oldMaxRevs
+		s.store.Close()
+		os.Remove(tmpPath)
+	}()
+	batches := 3
+	watchBatchMaxRevs = 4
+
+	v := []byte("foo")
+	for i := 0; i < watchBatchMaxRevs*batches; i++ {
+		s.Put(v, v, lease.NoLease)
+	}
+
+	w := s.NewWatchStream()
+	w.Watch(v, nil, 1)
+	for i := 0; i < batches; i++ {
+		if resp := <-w.Chan(); len(resp.Events) != watchBatchMaxRevs {
+			t.Fatalf("len(events) = %d, want %d", len(resp.Events), watchBatchMaxRevs)
+		}
+	}
+
+	s.store.mu.Lock()
+	defer s.store.mu.Unlock()
+	if size := s.synced.size(); size != 1 {
+		t.Errorf("synced size = %d, want 1", size)
+	}
+}
+
+func TestNewMapwatcherToEventMap(t *testing.T) {
+	k0, k1, k2 := []byte("foo0"), []byte("foo1"), []byte("foo2")
+	v0, v1, v2 := []byte("bar0"), []byte("bar1"), []byte("bar2")
+
+	ws := []*watcher{{key: k0}, {key: k1}, {key: k2}}
+
+	evs := []mvccpb.Event{
+		{
+			Type: mvccpb.PUT,
+			Kv:   &mvccpb.KeyValue{Key: k0, Value: v0},
+		},
+		{
+			Type: mvccpb.PUT,
+			Kv:   &mvccpb.KeyValue{Key: k1, Value: v1},
+		},
+		{
+			Type: mvccpb.PUT,
+			Kv:   &mvccpb.KeyValue{Key: k2, Value: v2},
+		},
+	}
+
+	tests := []struct {
+		sync []*watcher
+		evs  []mvccpb.Event
+
+		wwe map[*watcher][]mvccpb.Event
+	}{
+		// no watcher in sync, some events should return empty wwe
+		{
+			nil,
+			evs,
+			map[*watcher][]mvccpb.Event{},
+		},
+
+		// one watcher in sync, one event that does not match the key of that
+		// watcher should return empty wwe
+		{
+			[]*watcher{ws[2]},
+			evs[:1],
+			map[*watcher][]mvccpb.Event{},
+		},
+
+		// one watcher in sync, one event that matches the key of that
+		// watcher should return wwe with that matching watcher
+		{
+			[]*watcher{ws[1]},
+			evs[1:2],
+			map[*watcher][]mvccpb.Event{
+				ws[1]: evs[1:2],
+			},
+		},
+
+		// two watchers in sync that watches two different keys, one event
+		// that matches the key of only one of the watcher should return wwe
+		// with the matching watcher
+		{
+			[]*watcher{ws[0], ws[2]},
+			evs[2:],
+			map[*watcher][]mvccpb.Event{
+				ws[2]: evs[2:],
+			},
+		},
+
+		// two watchers in sync that watches the same key, two events that
+		// match the keys should return wwe with those two watchers
+		{
+			[]*watcher{ws[0], ws[1]},
+			evs[:2],
+			map[*watcher][]mvccpb.Event{
+				ws[0]: evs[:1],
+				ws[1]: evs[1:2],
+			},
+		},
+	}
+
+	for i, tt := range tests {
+		wg := newWatcherGroup()
+		for _, w := range tt.sync {
+			wg.add(w)
+		}
+
+		gwe := newWatcherBatch(&wg, tt.evs)
+		if len(gwe) != len(tt.wwe) {
+			t.Errorf("#%d: len(gwe) got = %d, want = %d", i, len(gwe), len(tt.wwe))
+		}
+		// compare gwe and tt.wwe
+		for w, eb := range gwe {
+			if len(eb.evs) != len(tt.wwe[w]) {
+				t.Errorf("#%d: len(eb.evs) got = %d, want = %d", i, len(eb.evs), len(tt.wwe[w]))
+			}
+			if !reflect.DeepEqual(eb.evs, tt.wwe[w]) {
+				t.Errorf("#%d: reflect.DeepEqual events got = %v, want = true", i, false)
+			}
+		}
+	}
+}
diff --git a/mvcc/watcher.go b/mvcc/watcher.go
new file mode 100644
index 0000000..2b68d96
--- /dev/null
+++ b/mvcc/watcher.go
@@ -0,0 +1,156 @@
+// Copyright 2015 CoreOS, Inc.
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//     http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+package mvcc
+
+import (
+	"errors"
+	"sync"
+
+	"github.com/coreos/etcd/mvcc/mvccpb"
+)
+
+var (
+	ErrWatcherNotExist = errors.New("mvcc: watcher does not exist")
+)
+
+type WatchID int64
+
+type WatchStream interface {
+	// Watch creates a watcher. The watcher watches the events happening or
+	// happened on the given key or range [key, end) from the given startRev.
+	//
+	// The whole event history can be watched unless compacted.
+	// If `startRev` <=0, watch observes events after currentRev.
+	//
+	// The returned `id` is the ID of this watcher. It appears as WatchID
+	// in events that are sent to the created watcher through stream channel.
+	//
+	Watch(key, end []byte, startRev int64) WatchID
+
+	// Chan returns a chan. All watch response will be sent to the returned chan.
+	Chan() <-chan WatchResponse
+
+	// RequestProgress requests the progress of the watcher with given ID. The response
+	// will only be sent if the watcher is currently synced.
+	// The responses will be sent through the WatchRespone Chan attached
+	// with this stream to ensure correct ordering.
+	// The responses contains no events. The revision in the response is the progress
+	// of the watchers since the watcher is currently synced.
+	RequestProgress(id WatchID)
+
+	// Cancel cancels a watcher by giving its ID. If watcher does not exist, an error will be
+	// returned.
+	Cancel(id WatchID) error
+
+	// Close closes Chan and release all related resources.
+	Close()
+
+	// Rev returns the current revision of the KV the stream watches on.
+	Rev() int64
+}
+
+type WatchResponse struct {
+	// WatchID is the WatchID of the watcher this response sent to.
+	WatchID WatchID
+
+	// Events contains all the events that needs to send.
+	Events []mvccpb.Event
+
+	// Revision is the revision of the KV when the watchResponse is created.
+	// For a normal response, the revision should be the same as the last
+	// modified revision inside Events. For a delayed response to a unsynced
+	// watcher, the revision is greater than the last modified revision
+	// inside Events.
+	Revision int64
+
+	// CompactRevision is set when the watcher is cancelled due to compaction.
+	CompactRevision int64
+}
+
+// watchStream contains a collection of watchers that share
+// one streaming chan to send out watched events and other control events.
+type watchStream struct {
+	watchable watchable
+	ch        chan WatchResponse
+
+	mu sync.Mutex // guards fields below it
+	// nextID is the ID pre-allocated for next new watcher in this stream
+	nextID   WatchID
+	closed   bool
+	cancels  map[WatchID]cancelFunc
+	watchers map[WatchID]*watcher
+}
+
+// Watch creates a new watcher in the stream and returns its WatchID.
+// TODO: return error if ws is closed?
+func (ws *watchStream) Watch(key, end []byte, startRev int64) WatchID {
+	ws.mu.Lock()
+	defer ws.mu.Unlock()
+	if ws.closed {
+		return -1
+	}
+
+	id := ws.nextID
+	ws.nextID++
+
+	w, c := ws.watchable.watch(key, end, startRev, id, ws.ch)
+
+	ws.cancels[id] = c
+	ws.watchers[id] = w
+	return id
+}
+
+func (ws *watchStream) Chan() <-chan WatchResponse {
+	return ws.ch
+}
+
+func (ws *watchStream) Cancel(id WatchID) error {
+	cancel, ok := ws.cancels[id]
+	if !ok {
+		return ErrWatcherNotExist
+	}
+	cancel()
+	delete(ws.cancels, id)
+	delete(ws.watchers, id)
+	return nil
+}
+
+func (ws *watchStream) Close() {
+	ws.mu.Lock()
+	defer ws.mu.Unlock()
+
+	for _, cancel := range ws.cancels {
+		cancel()
+	}
+	ws.closed = true
+	close(ws.ch)
+	watchStreamGauge.Dec()
+}
+
+func (ws *watchStream) Rev() int64 {
+	ws.mu.Lock()
+	defer ws.mu.Unlock()
+	return ws.watchable.rev()
+}
+
+func (ws *watchStream) RequestProgress(id WatchID) {
+	ws.mu.Lock()
+	w, ok := ws.watchers[id]
+	ws.mu.Unlock()
+	if !ok {
+		return
+	}
+	ws.watchable.progress(w)
+}
diff --git a/mvcc/watcher_bench_test.go b/mvcc/watcher_bench_test.go
new file mode 100644
index 0000000..c5d82a3
--- /dev/null
+++ b/mvcc/watcher_bench_test.go
@@ -0,0 +1,38 @@
+// Copyright 2015 CoreOS, Inc.
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//     http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+package mvcc
+
+import (
+	"fmt"
+	"testing"
+
+	"github.com/coreos/etcd/lease"
+	"github.com/coreos/etcd/mvcc/backend"
+)
+
+func BenchmarkKVWatcherMemoryUsage(b *testing.B) {
+	be, tmpPath := backend.NewDefaultTmpBackend()
+	watchable := newWatchableStore(be, &lease.FakeLessor{}, nil)
+
+	defer cleanup(watchable, be, tmpPath)
+
+	w := watchable.NewWatchStream()
+
+	b.ReportAllocs()
+	b.StartTimer()
+	for i := 0; i < b.N; i++ {
+		w.Watch([]byte(fmt.Sprint("foo", i)), nil, 0)
+	}
+}
diff --git a/mvcc/watcher_group.go b/mvcc/watcher_group.go
new file mode 100644
index 0000000..4b654b7
--- /dev/null
+++ b/mvcc/watcher_group.go
@@ -0,0 +1,267 @@
+// Copyright 2016 CoreOS, Inc.
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//     http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+package mvcc
+
+import (
+	"math"
+
+	"github.com/coreos/etcd/mvcc/mvccpb"
+	"github.com/coreos/etcd/pkg/adt"
+)
+
+var (
+	// watchBatchMaxRevs is the maximum distinct revisions that
+	// may be sent to an unsynced watcher at a time. Declared as
+	// var instead of const for testing purposes.
+	watchBatchMaxRevs = 1000
+)
+
+type eventBatch struct {
+	// evs is a batch of revision-ordered events
+	evs []mvccpb.Event
+	// revs is the minimum unique revisions observed for this batch
+	revs int
+	// moreRev is first revision with more events following this batch
+	moreRev int64
+}
+
+func (eb *eventBatch) add(ev mvccpb.Event) {
+	if eb.revs > watchBatchMaxRevs {
+		// maxed out batch size
+		return
+	}
+
+	if len(eb.evs) == 0 {
+		// base case
+		eb.revs = 1
+		eb.evs = append(eb.evs, ev)
+		return
+	}
+
+	// revision accounting
+	ebRev := eb.evs[len(eb.evs)-1].Kv.ModRevision
+	evRev := ev.Kv.ModRevision
+	if evRev > ebRev {
+		eb.revs++
+		if eb.revs > watchBatchMaxRevs {
+			eb.moreRev = evRev
+			return
+		}
+	}
+
+	eb.evs = append(eb.evs, ev)
+}
+
+type watcherBatch map[*watcher]*eventBatch
+
+func (wb watcherBatch) add(w *watcher, ev mvccpb.Event) {
+	eb := wb[w]
+	if eb == nil {
+		eb = &eventBatch{}
+		wb[w] = eb
+	}
+	eb.add(ev)
+}
+
+func (wb watcherBatch) contains(w *watcher) bool {
+	_, ok := wb[w]
+	return ok
+}
+
+// newWatcherBatch maps watchers to their matched events. It enables quick
+// events look up by watcher.
+func newWatcherBatch(wg *watcherGroup, evs []mvccpb.Event) watcherBatch {
+	wb := make(watcherBatch)
+	for _, ev := range evs {
+		for w := range wg.watcherSetByKey(string(ev.Kv.Key)) {
+			if ev.Kv.ModRevision >= w.cur {
+				// don't double notify
+				wb.add(w, ev)
+			}
+		}
+	}
+	return wb
+}
+
+type watcherSet map[*watcher]struct{}
+
+func (w watcherSet) add(wa *watcher) {
+	if _, ok := w[wa]; ok {
+		panic("add watcher twice!")
+	}
+	w[wa] = struct{}{}
+}
+
+func (w watcherSet) union(ws watcherSet) {
+	for wa := range ws {
+		w.add(wa)
+	}
+}
+
+func (w watcherSet) delete(wa *watcher) {
+	if _, ok := w[wa]; !ok {
+		panic("removing missing watcher!")
+	}
+	delete(w, wa)
+}
+
+type watcherSetByKey map[string]watcherSet
+
+func (w watcherSetByKey) add(wa *watcher) {
+	set := w[string(wa.key)]
+	if set == nil {
+		set = make(watcherSet)
+		w[string(wa.key)] = set
+	}
+	set.add(wa)
+}
+
+func (w watcherSetByKey) delete(wa *watcher) bool {
+	k := string(wa.key)
+	if v, ok := w[k]; ok {
+		if _, ok := v[wa]; ok {
+			delete(v, wa)
+			if len(v) == 0 {
+				// remove the set; nothing left
+				delete(w, k)
+			}
+			return true
+		}
+	}
+	return false
+}
+
+// watcherGroup is a collection of watchers organized by their ranges
+type watcherGroup struct {
+	// keyWatchers has the watchers that watch on a single key
+	keyWatchers watcherSetByKey
+	// ranges has the watchers that watch a range; it is sorted by interval
+	ranges adt.IntervalTree
+	// watchers is the set of all watchers
+	watchers watcherSet
+}
+
+func newWatcherGroup() watcherGroup {
+	return watcherGroup{
+		keyWatchers: make(watcherSetByKey),
+		watchers:    make(watcherSet),
+	}
+}
+
+// add puts a watcher in the group.
+func (wg *watcherGroup) add(wa *watcher) {
+	wg.watchers.add(wa)
+	if wa.end == nil {
+		wg.keyWatchers.add(wa)
+		return
+	}
+
+	// interval already registered?
+	ivl := adt.NewStringAffineInterval(string(wa.key), string(wa.end))
+	if iv := wg.ranges.Find(ivl); iv != nil {
+		iv.Val.(watcherSet).add(wa)
+		return
+	}
+
+	// not registered, put in interval tree
+	ws := make(watcherSet)
+	ws.add(wa)
+	wg.ranges.Insert(ivl, ws)
+}
+
+// contains is whether the given key has a watcher in the group.
+func (wg *watcherGroup) contains(key string) bool {
+	_, ok := wg.keyWatchers[key]
+	return ok || wg.ranges.Contains(adt.NewStringAffinePoint(key))
+}
+
+// size gives the number of unique watchers in the group.
+func (wg *watcherGroup) size() int { return len(wg.watchers) }
+
+// delete removes a watcher from the group.
+func (wg *watcherGroup) delete(wa *watcher) bool {
+	if _, ok := wg.watchers[wa]; !ok {
+		return false
+	}
+	wg.watchers.delete(wa)
+	if wa.end == nil {
+		wg.keyWatchers.delete(wa)
+		return true
+	}
+
+	ivl := adt.NewStringAffineInterval(string(wa.key), string(wa.end))
+	iv := wg.ranges.Find(ivl)
+	if iv == nil {
+		return false
+	}
+
+	ws := iv.Val.(watcherSet)
+	delete(ws, wa)
+	if len(ws) == 0 {
+		// remove interval missing watchers
+		if ok := wg.ranges.Delete(ivl); !ok {
+			panic("could not remove watcher from interval tree")
+		}
+	}
+
+	return true
+}
+
+func (wg *watcherGroup) scanMinRev(curRev int64, compactRev int64) int64 {
+	minRev := int64(math.MaxInt64)
+	for w := range wg.watchers {
+		if w.cur > curRev {
+			panic("watcher current revision should not exceed current revision")
+		}
+		if w.cur < compactRev {
+			select {
+			case w.ch <- WatchResponse{WatchID: w.id, CompactRevision: compactRev}:
+				wg.delete(w)
+			default:
+				// retry next time
+			}
+			continue
+		}
+		if minRev > w.cur {
+			minRev = w.cur
+		}
+	}
+	return minRev
+}
+
+// watcherSetByKey gets the set of watchers that receive events on the given key.
+func (wg *watcherGroup) watcherSetByKey(key string) watcherSet {
+	wkeys := wg.keyWatchers[key]
+	wranges := wg.ranges.Stab(adt.NewStringAffinePoint(key))
+
+	// zero-copy cases
+	switch {
+	case len(wranges) == 0:
+		// no need to merge ranges or copy; reuse single-key set
+		return wkeys
+	case len(wranges) == 0 && len(wkeys) == 0:
+		return nil
+	case len(wranges) == 1 && len(wkeys) == 0:
+		return wranges[0].Val.(watcherSet)
+	}
+
+	// copy case
+	ret := make(watcherSet)
+	ret.union(wg.keyWatchers[key])
+	for _, item := range wranges {
+		ret.union(item.Val.(watcherSet))
+	}
+	return ret
+}
diff --git a/mvcc/watcher_test.go b/mvcc/watcher_test.go
new file mode 100644
index 0000000..ad5bac7
--- /dev/null
+++ b/mvcc/watcher_test.go
@@ -0,0 +1,246 @@
+// Copyright 2015 CoreOS, Inc.
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//     http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+package mvcc
+
+import (
+	"bytes"
+	"os"
+	"reflect"
+	"testing"
+	"time"
+
+	"github.com/coreos/etcd/lease"
+	"github.com/coreos/etcd/mvcc/backend"
+)
+
+// TestWatcherWatchID tests that each watcher provides unique watchID,
+// and the watched event attaches the correct watchID.
+func TestWatcherWatchID(t *testing.T) {
+	b, tmpPath := backend.NewDefaultTmpBackend()
+	s := WatchableKV(newWatchableStore(b, &lease.FakeLessor{}, nil))
+	defer cleanup(s, b, tmpPath)
+
+	w := s.NewWatchStream()
+	defer w.Close()
+
+	idm := make(map[WatchID]struct{})
+
+	for i := 0; i < 10; i++ {
+		id := w.Watch([]byte("foo"), nil, 0)
+		if _, ok := idm[id]; ok {
+			t.Errorf("#%d: id %d exists", i, id)
+		}
+		idm[id] = struct{}{}
+
+		s.Put([]byte("foo"), []byte("bar"), lease.NoLease)
+
+		resp := <-w.Chan()
+		if resp.WatchID != id {
+			t.Errorf("#%d: watch id in event = %d, want %d", i, resp.WatchID, id)
+		}
+
+		if err := w.Cancel(id); err != nil {
+			t.Error(err)
+		}
+	}
+
+	s.Put([]byte("foo2"), []byte("bar"), lease.NoLease)
+
+	// unsynced watchers
+	for i := 10; i < 20; i++ {
+		id := w.Watch([]byte("foo2"), nil, 1)
+		if _, ok := idm[id]; ok {
+			t.Errorf("#%d: id %d exists", i, id)
+		}
+		idm[id] = struct{}{}
+
+		resp := <-w.Chan()
+		if resp.WatchID != id {
+			t.Errorf("#%d: watch id in event = %d, want %d", i, resp.WatchID, id)
+		}
+
+		if err := w.Cancel(id); err != nil {
+			t.Error(err)
+		}
+	}
+}
+
+// TestWatcherWatchPrefix tests if Watch operation correctly watches
+// and returns events with matching prefixes.
+func TestWatcherWatchPrefix(t *testing.T) {
+	b, tmpPath := backend.NewDefaultTmpBackend()
+	s := WatchableKV(newWatchableStore(b, &lease.FakeLessor{}, nil))
+	defer cleanup(s, b, tmpPath)
+
+	w := s.NewWatchStream()
+	defer w.Close()
+
+	idm := make(map[WatchID]struct{})
+
+	val := []byte("bar")
+	keyWatch, keyEnd, keyPut := []byte("foo"), []byte("fop"), []byte("foobar")
+
+	for i := 0; i < 10; i++ {
+		id := w.Watch(keyWatch, keyEnd, 0)
+		if _, ok := idm[id]; ok {
+			t.Errorf("#%d: unexpected duplicated id %x", i, id)
+		}
+		idm[id] = struct{}{}
+
+		s.Put(keyPut, val, lease.NoLease)
+
+		resp := <-w.Chan()
+		if resp.WatchID != id {
+			t.Errorf("#%d: watch id in event = %d, want %d", i, resp.WatchID, id)
+		}
+
+		if err := w.Cancel(id); err != nil {
+			t.Errorf("#%d: unexpected cancel error %v", i, err)
+		}
+
+		if len(resp.Events) != 1 {
+			t.Errorf("#%d: len(resp.Events) got = %d, want = 1", i, len(resp.Events))
+		}
+		if len(resp.Events) == 1 {
+			if !bytes.Equal(resp.Events[0].Kv.Key, keyPut) {
+				t.Errorf("#%d: resp.Events got = %s, want = %s", i, resp.Events[0].Kv.Key, keyPut)
+			}
+		}
+	}
+
+	keyWatch1, keyEnd1, keyPut1 := []byte("foo1"), []byte("foo2"), []byte("foo1bar")
+	s.Put(keyPut1, val, lease.NoLease)
+
+	// unsynced watchers
+	for i := 10; i < 15; i++ {
+		id := w.Watch(keyWatch1, keyEnd1, 1)
+		if _, ok := idm[id]; ok {
+			t.Errorf("#%d: id %d exists", i, id)
+		}
+		idm[id] = struct{}{}
+
+		resp := <-w.Chan()
+		if resp.WatchID != id {
+			t.Errorf("#%d: watch id in event = %d, want %d", i, resp.WatchID, id)
+		}
+
+		if err := w.Cancel(id); err != nil {
+			t.Error(err)
+		}
+
+		if len(resp.Events) != 1 {
+			t.Errorf("#%d: len(resp.Events) got = %d, want = 1", i, len(resp.Events))
+		}
+		if len(resp.Events) == 1 {
+			if !bytes.Equal(resp.Events[0].Kv.Key, keyPut1) {
+				t.Errorf("#%d: resp.Events got = %s, want = %s", i, resp.Events[0].Kv.Key, keyPut1)
+			}
+		}
+	}
+}
+
+// TestWatchStreamCancelWatcherByID ensures cancel calls the cancel func of the watcher
+// with given id inside watchStream.
+func TestWatchStreamCancelWatcherByID(t *testing.T) {
+	b, tmpPath := backend.NewDefaultTmpBackend()
+	s := WatchableKV(newWatchableStore(b, &lease.FakeLessor{}, nil))
+	defer cleanup(s, b, tmpPath)
+
+	w := s.NewWatchStream()
+	defer w.Close()
+
+	id := w.Watch([]byte("foo"), nil, 0)
+
+	tests := []struct {
+		cancelID WatchID
+		werr     error
+	}{
+		// no error should be returned when cancel the created watcher.
+		{id, nil},
+		// not exist error should be returned when cancel again.
+		{id, ErrWatcherNotExist},
+		// not exist error should be returned when cancel a bad id.
+		{id + 1, ErrWatcherNotExist},
+	}
+
+	for i, tt := range tests {
+		gerr := w.Cancel(tt.cancelID)
+
+		if gerr != tt.werr {
+			t.Errorf("#%d: err = %v, want %v", i, gerr, tt.werr)
+		}
+	}
+
+	if l := len(w.(*watchStream).cancels); l != 0 {
+		t.Errorf("cancels = %d, want 0", l)
+	}
+}
+
+// TestWatcherRequestProgress ensures synced watcher can correctly
+// report its correct progress.
+func TestWatcherRequestProgress(t *testing.T) {
+	b, tmpPath := backend.NewDefaultTmpBackend()
+
+	// manually create watchableStore instead of newWatchableStore
+	// because newWatchableStore automatically calls syncWatchers
+	// method to sync watchers in unsynced map. We want to keep watchers
+	// in unsynced to test if syncWatchers works as expected.
+	s := &watchableStore{
+		store:    NewStore(b, &lease.FakeLessor{}, nil),
+		unsynced: newWatcherGroup(),
+		synced:   newWatcherGroup(),
+	}
+
+	defer func() {
+		s.store.Close()
+		os.Remove(tmpPath)
+	}()
+
+	testKey := []byte("foo")
+	notTestKey := []byte("bad")
+	testValue := []byte("bar")
+	s.Put(testKey, testValue, lease.NoLease)
+
+	w := s.NewWatchStream()
+
+	badID := WatchID(1000)
+	w.RequestProgress(badID)
+	select {
+	case resp := <-w.Chan():
+		t.Fatalf("unexpected %+v", resp)
+	default:
+	}
+
+	id := w.Watch(notTestKey, nil, 1)
+	w.RequestProgress(id)
+	select {
+	case resp := <-w.Chan():
+		t.Fatalf("unexpected %+v", resp)
+	default:
+	}
+
+	s.syncWatchers()
+
+	w.RequestProgress(id)
+	wrs := WatchResponse{WatchID: 0, Revision: 2}
+	select {
+	case resp := <-w.Chan():
+		if !reflect.DeepEqual(resp, wrs) {
+			t.Fatalf("got %+v, expect %+v", resp, wrs)
+		}
+	case <-time.After(time.Second):
+		t.Fatal("failed to receive progress")
+	}
+}
diff --git a/raft/raftpb/raft.pb.go b/raft/raftpb/raft.pb.go
index ca5d3f8..e9abec7 100644
--- a/raft/raftpb/raft.pb.go
+++ b/raft/raftpb/raft.pb.go
@@ -183,10 +183,10 @@ func (x *ConfChangeType) UnmarshalJSON(data []byte) error {
 func (ConfChangeType) EnumDescriptor() ([]byte, []int) { return fileDescriptorRaft, []int{2} }
 
 type Entry struct {
-	Type             EntryType `protobuf:"varint,1,opt,name=Type,json=type,enum=raftpb.EntryType" json:"Type"`
-	Term             uint64    `protobuf:"varint,2,opt,name=Term,json=term" json:"Term"`
-	Index            uint64    `protobuf:"varint,3,opt,name=Index,json=index" json:"Index"`
-	Data             []byte    `protobuf:"bytes,4,opt,name=Data,json=data" json:"Data,omitempty"`
+	Type             EntryType `protobuf:"varint,1,opt,name=Type,enum=raftpb.EntryType" json:"Type"`
+	Term             uint64    `protobuf:"varint,2,opt,name=Term" json:"Term"`
+	Index            uint64    `protobuf:"varint,3,opt,name=Index" json:"Index"`
+	Data             []byte    `protobuf:"bytes,4,opt,name=Data" json:"Data,omitempty"`
 	XXX_unrecognized []byte    `json:"-"`
 }
 
@@ -196,7 +196,7 @@ func (*Entry) ProtoMessage()               {}
 func (*Entry) Descriptor() ([]byte, []int) { return fileDescriptorRaft, []int{0} }
 
 type SnapshotMetadata struct {
-	ConfState        ConfState `protobuf:"bytes,1,opt,name=conf_state,json=confState" json:"conf_state"`
+	ConfState        ConfState `protobuf:"bytes,1,opt,name=conf_state" json:"conf_state"`
 	Index            uint64    `protobuf:"varint,2,opt,name=index" json:"index"`
 	Term             uint64    `protobuf:"varint,3,opt,name=term" json:"term"`
 	XXX_unrecognized []byte    `json:"-"`
@@ -261,10 +261,10 @@ func (*ConfState) ProtoMessage()               {}
 func (*ConfState) Descriptor() ([]byte, []int) { return fileDescriptorRaft, []int{5} }
 
 type ConfChange struct {
-	ID               uint64         `protobuf:"varint,1,opt,name=ID,json=iD" json:"ID"`
-	Type             ConfChangeType `protobuf:"varint,2,opt,name=Type,json=type,enum=raftpb.ConfChangeType" json:"Type"`
-	NodeID           uint64         `protobuf:"varint,3,opt,name=NodeID,json=nodeID" json:"NodeID"`
-	Context          []byte         `protobuf:"bytes,4,opt,name=Context,json=context" json:"Context,omitempty"`
+	ID               uint64         `protobuf:"varint,1,opt,name=ID" json:"ID"`
+	Type             ConfChangeType `protobuf:"varint,2,opt,name=Type,enum=raftpb.ConfChangeType" json:"Type"`
+	NodeID           uint64         `protobuf:"varint,3,opt,name=NodeID" json:"NodeID"`
+	Context          []byte         `protobuf:"bytes,4,opt,name=Context" json:"Context,omitempty"`
 	XXX_unrecognized []byte         `json:"-"`
 }
 
@@ -1788,51 +1788,47 @@ var (
 )
 
 var fileDescriptorRaft = []byte{
-	// 735 bytes of a gzipped FileDescriptorProto
-	0x1f, 0x8b, 0x08, 0x00, 0x00, 0x09, 0x6e, 0x88, 0x02, 0xff, 0x64, 0x54, 0x4d, 0x6f, 0xd3, 0x4a,
-	0x14, 0xad, 0x13, 0xe7, 0xeb, 0xa6, 0x4d, 0xa7, 0xd3, 0xbc, 0x27, 0xab, 0x7a, 0xea, 0xeb, 0xb3,
-	0xde, 0x02, 0x15, 0xb5, 0x40, 0x17, 0x2c, 0xd8, 0xf5, 0x03, 0xa9, 0x95, 0x68, 0x05, 0x69, 0xcb,
-	0x02, 0x84, 0xd0, 0xd4, 0x9e, 0x38, 0x81, 0xda, 0x63, 0x8d, 0x27, 0xa5, 0xdd, 0x20, 0x24, 0x16,
-	0x6c, 0xf8, 0x61, 0x5d, 0xf6, 0x17, 0x20, 0xe0, 0x97, 0x70, 0x67, 0x3c, 0x4e, 0xec, 0x66, 0x11,
-	0x69, 0xe6, 0x9c, 0xfb, 0x71, 0xee, 0x99, 0xeb, 0x00, 0x48, 0x36, 0x54, 0xdb, 0xa9, 0x14, 0x4a,
-	0xd0, 0xa6, 0x3e, 0xa7, 0x17, 0x6b, 0xfd, 0x48, 0x44, 0xc2, 0x40, 0x8f, 0xf4, 0x29, 0x67, 0xfd,
-	0xcf, 0xd0, 0x78, 0x9e, 0x28, 0x79, 0x43, 0x1f, 0x82, 0x7b, 0x76, 0x93, 0x72, 0xcf, 0xd9, 0x70,
-	0x1e, 0xf4, 0x76, 0x56, 0xb6, 0xf3, 0xac, 0x6d, 0x43, 0x6a, 0x62, 0xcf, 0xbd, 0xfd, 0xf1, 0xef,
-	0xc2, 0xc0, 0x55, 0x78, 0xa6, 0x1e, 0x06, 0x73, 0x19, 0x7b, 0x35, 0x0c, 0x76, 0xa7, 0x0c, 0x22,
-	0x74, 0x0d, 0x1a, 0x47, 0x49, 0xc8, 0xaf, 0xbd, 0x7a, 0x89, 0x6a, 0x8c, 0x35, 0x44, 0x29, 0xb8,
-	0x07, 0x4c, 0x31, 0xcf, 0x45, 0x6a, 0x71, 0xe0, 0x86, 0x78, 0xf6, 0xbf, 0x38, 0x40, 0x4e, 0x13,
-	0x96, 0x66, 0x23, 0xa1, 0x8e, 0xb9, 0x62, 0x1a, 0xa4, 0x4f, 0x01, 0x02, 0x91, 0x0c, 0xdf, 0x67,
-	0x8a, 0xa9, 0x5c, 0x51, 0x77, 0xa6, 0x68, 0x1f, 0x99, 0x53, 0x4d, 0xd8, 0xe2, 0x9d, 0xa0, 0x00,
-	0x74, 0x73, 0xd3, 0xa9, 0xa2, 0xcb, 0x36, 0x47, 0xc9, 0x5a, 0x60, 0x45, 0x97, 0x41, 0xfc, 0x37,
-	0xd0, 0x2e, 0x14, 0x68, 0x89, 0x5a, 0x81, 0xe9, 0x69, 0x25, 0xd2, 0x67, 0xd0, 0x8e, 0xad, 0x32,
-	0x53, 0xb8, 0xbb, 0xe3, 0x15, 0x5a, 0xee, 0x2b, 0xb7, 0x75, 0xa7, 0xf1, 0xfe, 0xd7, 0x3a, 0xb4,
-	0x8e, 0x79, 0x96, 0xb1, 0x88, 0xd3, 0x2d, 0x30, 0xe6, 0x59, 0x87, 0x57, 0x8b, 0x1a, 0x96, 0x9e,
-	0xf3, 0xb8, 0x0f, 0x35, 0x25, 0x2a, 0x93, 0xe0, 0x5d, 0x8f, 0x31, 0x94, 0xe2, 0xde, 0x18, 0x1a,
-	0x99, 0x0e, 0xe8, 0xce, 0xbd, 0xc9, 0x3a, 0xb4, 0x2e, 0x45, 0x64, 0x1e, 0xac, 0x51, 0x22, 0x0b,
-	0x70, 0x66, 0x5b, 0x73, 0xde, 0xb6, 0x2d, 0x68, 0x71, 0x5c, 0x81, 0x31, 0xcf, 0xbc, 0xd6, 0x46,
-	0x1d, 0x67, 0x5f, 0xaa, 0x6c, 0x46, 0x51, 0xca, 0xc6, 0xd0, 0x7f, 0xa0, 0x19, 0x88, 0x38, 0x1e,
-	0x2b, 0xaf, 0x5d, 0xaa, 0x65, 0x31, 0xba, 0x03, 0xed, 0xcc, 0x3a, 0xe6, 0x75, 0x8c, 0x93, 0xe4,
-	0xbe, 0x93, 0x85, 0x83, 0x45, 0x9c, 0xae, 0x28, 0xf9, 0x07, 0x1e, 0x28, 0x0f, 0x30, 0xa3, 0x5d,
-	0x54, 0xcc, 0x31, 0xfa, 0x3f, 0xae, 0xba, 0x39, 0x1d, 0x8e, 0x13, 0xe5, 0x75, 0x4b, 0x3d, 0x4b,
-	0xb8, 0xff, 0x0e, 0x3a, 0x87, 0x4c, 0x86, 0xf9, 0x92, 0x14, 0x3e, 0x39, 0x73, 0x3e, 0x21, 0x73,
-	0x25, 0x70, 0xe1, 0x2a, 0x5b, 0xad, 0x91, 0xd2, 0x58, 0xf5, 0xf9, 0xb1, 0xfc, 0xff, 0xa0, 0x33,
-	0x5d, 0x4a, 0x7c, 0xb6, 0x46, 0x22, 0x42, 0xb4, 0xcb, 0x41, 0xbb, 0xdc, 0x41, 0x7e, 0xf1, 0xbf,
-	0x3b, 0x00, 0x3a, 0x66, 0x7f, 0xc4, 0x92, 0xc8, 0xbc, 0xed, 0xd1, 0x41, 0x45, 0x41, 0x6d, 0x7c,
-	0x40, 0x1f, 0xdb, 0x4f, 0xb0, 0x66, 0x16, 0xe4, 0xef, 0xf2, 0xc2, 0xe7, 0x79, 0x73, 0x3b, 0x82,
-	0xba, 0x4e, 0xb0, 0x3e, 0xd6, 0xaa, 0xe8, 0x4a, 0x0c, 0x86, 0xf3, 0xb4, 0x30, 0x57, 0xf1, 0x6b,
-	0x65, 0x3f, 0xb9, 0x56, 0x90, 0x5f, 0x37, 0x9f, 0x40, 0x67, 0xfa, 0x61, 0xd3, 0x65, 0xe8, 0x9a,
-	0xcb, 0x89, 0x90, 0x31, 0xbb, 0x24, 0x0b, 0x74, 0x15, 0x96, 0x0d, 0x30, 0x6b, 0x4c, 0x9c, 0xcd,
-	0x6f, 0x35, 0xe8, 0x96, 0x56, 0x95, 0x02, 0x34, 0x8f, 0xb3, 0xe8, 0x70, 0x92, 0x62, 0x42, 0x17,
-	0x97, 0x3c, 0x8b, 0xf6, 0x38, 0x53, 0xc4, 0xb1, 0x97, 0x97, 0x52, 0xa4, 0xa4, 0x66, 0xa3, 0x76,
-	0xd3, 0x94, 0xd4, 0x69, 0x0f, 0x20, 0x3f, 0x0f, 0x78, 0x96, 0x12, 0xd7, 0x06, 0xbe, 0x46, 0x7f,
-	0x49, 0x43, 0x8b, 0xb0, 0x17, 0xc3, 0x36, 0x2d, 0xab, 0xd7, 0x82, 0xb4, 0x28, 0x81, 0x45, 0xdd,
-	0x8c, 0x33, 0xa9, 0x2e, 0x74, 0x97, 0x36, 0x3a, 0x48, 0xca, 0x88, 0x49, 0xea, 0xe0, 0xe7, 0xdb,
-	0x43, 0xf4, 0x3c, 0x91, 0x9c, 0x05, 0x23, 0x76, 0x71, 0xc9, 0x09, 0xd0, 0x15, 0x58, 0xb2, 0x85,
-	0xf4, 0x03, 0x4d, 0x32, 0xd2, 0xb5, 0x61, 0xfb, 0x23, 0x1e, 0x7c, 0x7c, 0x35, 0x11, 0x72, 0x12,
-	0x93, 0x45, 0xfa, 0x17, 0xac, 0x20, 0x76, 0x26, 0x59, 0x92, 0x0d, 0xb9, 0x7c, 0xc1, 0x59, 0xc8,
-	0x25, 0x59, 0xb2, 0xd9, 0x67, 0xe3, 0x98, 0x8b, 0x89, 0x3a, 0x11, 0x9f, 0x48, 0x6f, 0xf3, 0x2d,
-	0xf4, 0xaa, 0x4f, 0xa2, 0x73, 0x67, 0xc8, 0x6e, 0x18, 0xea, 0x37, 0x41, 0x5b, 0x3c, 0xe8, 0xcf,
-	0xe0, 0x01, 0x8f, 0xc5, 0x15, 0x37, 0x8c, 0x53, 0x65, 0xce, 0x53, 0xfc, 0xab, 0xc8, 0x99, 0xda,
-	0x5e, 0xff, 0xf6, 0xd7, 0xfa, 0xc2, 0x1d, 0xfe, 0x6e, 0x7f, 0xaf, 0x3b, 0x77, 0xf8, 0xfb, 0x89,
-	0xbf, 0x3f, 0x01, 0x00, 0x00, 0xff, 0xff, 0xc8, 0x26, 0x45, 0x2d, 0xd0, 0x05, 0x00, 0x00,
+	// 657 bytes of a gzipped FileDescriptorProto
+	0x1f, 0x8b, 0x08, 0x00, 0x00, 0x09, 0x6e, 0x88, 0x02, 0xff, 0x5c, 0x53, 0xdd, 0x6e, 0xd3, 0x4c,
+	0x10, 0xad, 0x5d, 0x27, 0x76, 0xc6, 0x49, 0xba, 0x75, 0xf3, 0x7d, 0xb2, 0x7a, 0x51, 0xaa, 0x08,
+	0x44, 0x15, 0xa4, 0x02, 0x7d, 0x83, 0xfe, 0x20, 0xb5, 0x88, 0x56, 0xd0, 0x1f, 0x6e, 0xb8, 0x40,
+	0xdb, 0x78, 0xe3, 0x04, 0x6a, 0xaf, 0x59, 0x6f, 0x4a, 0xfb, 0x04, 0xbc, 0x0d, 0xcf, 0xd1, 0xcb,
+	0x3e, 0x01, 0x02, 0x9e, 0x84, 0xd9, 0xf5, 0x36, 0x5e, 0xf7, 0xc2, 0x92, 0x7d, 0xce, 0xcc, 0x9c,
+	0x99, 0x39, 0x63, 0x00, 0x41, 0x27, 0x72, 0xbb, 0x10, 0x5c, 0xf2, 0xa8, 0xad, 0xde, 0x8b, 0xcb,
+	0xf5, 0x41, 0xca, 0x53, 0xae, 0xa1, 0x97, 0xea, 0xad, 0x62, 0x87, 0x0c, 0x5a, 0x6f, 0x72, 0x29,
+	0x6e, 0xa3, 0x67, 0xe0, 0x9d, 0xdf, 0x16, 0x2c, 0x76, 0x36, 0x9d, 0xad, 0xfe, 0xce, 0xea, 0x76,
+	0x95, 0xb5, 0xad, 0x49, 0x45, 0xec, 0x79, 0x77, 0xbf, 0x9e, 0x2c, 0x45, 0x11, 0x86, 0x31, 0x91,
+	0xc5, 0x2e, 0x86, 0x79, 0x06, 0x5b, 0x83, 0xd6, 0x51, 0x9e, 0xb0, 0x9b, 0x78, 0xd9, 0x02, 0xbb,
+	0xe0, 0x1d, 0x50, 0x49, 0x63, 0x0f, 0xb1, 0xee, 0x70, 0x0a, 0xe4, 0x2c, 0xa7, 0x45, 0x39, 0xe5,
+	0xf2, 0x98, 0x49, 0x9a, 0x20, 0x13, 0xbd, 0x00, 0x18, 0xf3, 0x7c, 0xf2, 0xb9, 0x94, 0x54, 0x56,
+	0xba, 0x61, 0xad, 0xbb, 0x8f, 0xcc, 0x99, 0x22, 0x6a, 0x8d, 0x99, 0xd6, 0xb0, 0x85, 0xb1, 0x19,
+	0xa9, 0x9a, 0xb1, 0x74, 0x87, 0x6f, 0x21, 0x78, 0x50, 0x52, 0x3d, 0x28, 0x25, 0x5d, 0xbb, 0x1b,
+	0xbd, 0x82, 0x20, 0x33, 0xda, 0xba, 0x4a, 0xb8, 0x13, 0x3f, 0xa8, 0x3d, 0xee, 0xcd, 0xd4, 0xfa,
+	0xe9, 0x82, 0x7f, 0xcc, 0xca, 0x92, 0xa6, 0x2c, 0x7a, 0x8e, 0x5a, 0xf5, 0x7e, 0xd6, 0x1e, 0x32,
+	0x0d, 0x6d, 0x6d, 0x88, 0x80, 0x2b, 0xf9, 0xe3, 0x36, 0x27, 0x82, 0x37, 0xda, 0x5c, 0xb4, 0xee,
+	0x59, 0xd8, 0x7f, 0xe0, 0x5f, 0xf1, 0x54, 0xaf, 0xb7, 0xd5, 0x5c, 0x6f, 0x35, 0x7a, 0xdb, 0x02,
+	0x9f, 0x82, 0xcf, 0xd0, 0x9a, 0x19, 0x2b, 0x63, 0x7f, 0x73, 0x19, 0x67, 0xe9, 0x35, 0x1c, 0x33,
+	0x51, 0x03, 0x68, 0x8f, 0x79, 0x96, 0xcd, 0x64, 0x1c, 0x58, 0xb9, 0x5b, 0x10, 0x94, 0x66, 0xe0,
+	0xb8, 0xa3, 0x17, 0x41, 0x1e, 0x2f, 0xa2, 0xce, 0x17, 0xec, 0x0b, 0x1b, 0xcb, 0x18, 0x30, 0x2e,
+	0x30, 0x68, 0x8c, 0xf7, 0xa5, 0xd1, 0xc3, 0x59, 0x2e, 0xe3, 0xd0, 0x5a, 0xfe, 0x11, 0x74, 0x0e,
+	0xa9, 0x48, 0xb4, 0x71, 0x8b, 0x11, 0x9d, 0xe6, 0xd8, 0xd7, 0x1c, 0xdd, 0xb6, 0xd7, 0x53, 0x37,
+	0x69, 0xfb, 0xb8, 0x0e, 0x9d, 0xc5, 0x0d, 0x44, 0x3d, 0x68, 0xe5, 0x3c, 0xc1, 0x59, 0x1d, 0x9c,
+	0xd5, 0x1b, 0x7e, 0x03, 0x50, 0xdc, 0xfe, 0x94, 0xe6, 0xe8, 0x0c, 0x2e, 0xfc, 0xe8, 0xa0, 0xa1,
+	0x32, 0x32, 0xb7, 0xec, 0x6a, 0xaf, 0xfe, 0xb7, 0x6f, 0xaa, 0xca, 0xb1, 0xec, 0x42, 0xf5, 0x13,
+	0x2c, 0x8d, 0x15, 0x6c, 0x7b, 0x56, 0xc0, 0xc7, 0x68, 0xc9, 0x6e, 0x64, 0x75, 0xc0, 0xa3, 0xd7,
+	0xd0, 0x59, 0xfc, 0x0a, 0xc8, 0x86, 0xfa, 0xe3, 0x84, 0x8b, 0x8c, 0x5e, 0x11, 0x65, 0xd1, 0x8a,
+	0x06, 0x6a, 0x05, 0xe2, 0x8c, 0x7e, 0xb8, 0x10, 0x5a, 0xe7, 0x11, 0x01, 0xb4, 0x8f, 0xcb, 0xf4,
+	0x70, 0x5e, 0x60, 0x42, 0x88, 0x87, 0x55, 0xa6, 0x7b, 0x8c, 0x4a, 0xe2, 0x98, 0x8f, 0xf7, 0x82,
+	0x17, 0xc4, 0x35, 0x51, 0xbb, 0x45, 0x41, 0x96, 0xa3, 0x3e, 0x40, 0xf5, 0x7e, 0xca, 0xca, 0x82,
+	0x78, 0x26, 0xf0, 0x23, 0x2e, 0x90, 0xb4, 0x54, 0x13, 0xe6, 0x43, 0xb3, 0x6d, 0xc3, 0x2a, 0x07,
+	0x89, 0x8f, 0x4b, 0xe9, 0x2a, 0x31, 0x46, 0x85, 0xbc, 0x54, 0x2a, 0x01, 0x0e, 0x4a, 0x6c, 0x44,
+	0x27, 0x75, 0xd0, 0x90, 0x3e, 0xa2, 0x17, 0xb9, 0x60, 0x74, 0x3c, 0xa5, 0x97, 0x57, 0x8c, 0x40,
+	0xb4, 0x0a, 0x3d, 0x53, 0x48, 0x6d, 0x7f, 0x5e, 0x92, 0xd0, 0x84, 0xed, 0x4f, 0xd9, 0xf8, 0xeb,
+	0x87, 0x39, 0x17, 0xf3, 0x8c, 0x74, 0xf1, 0x5c, 0x57, 0x11, 0x3b, 0x17, 0x34, 0x2f, 0x27, 0x4c,
+	0xbc, 0x63, 0x34, 0x61, 0x82, 0xf4, 0x4c, 0xf6, 0xf9, 0x2c, 0x63, 0x7c, 0x2e, 0x4f, 0xf8, 0x77,
+	0xd2, 0x1f, 0x7d, 0x82, 0x7e, 0x73, 0xf7, 0x2a, 0xb7, 0x46, 0x76, 0x93, 0x44, 0x59, 0x40, 0xd4,
+	0x65, 0x0d, 0x6a, 0xf8, 0x94, 0x65, 0xfc, 0x9a, 0x69, 0xc6, 0x69, 0x32, 0x17, 0x05, 0xfe, 0xa4,
+	0x15, 0xe3, 0xee, 0x0d, 0xee, 0xfe, 0x6c, 0x2c, 0xdd, 0xe3, 0x73, 0xf7, 0x77, 0xc3, 0xb9, 0xc7,
+	0xe7, 0x37, 0x3e, 0xff, 0x02, 0x00, 0x00, 0xff, 0xff, 0x3d, 0x27, 0xd9, 0xba, 0x02, 0x05, 0x00,
+	0x00,
 }
diff --git a/scripts/genproto.sh b/scripts/genproto.sh
index 5d44b09..8e45d8a 100755
--- a/scripts/genproto.sh
+++ b/scripts/genproto.sh
@@ -17,7 +17,7 @@ if ! [[ $(protoc --version) =~ "3.0.0" ]]; then
 fi
 
 # directories containing protos to be built
-DIRS="./wal/walpb ./etcdserver/etcdserverpb ./snap/snappb ./raft/raftpb ./storage/storagepb ./lease/leasepb ./auth/authpb"
+DIRS="./wal/walpb ./etcdserver/etcdserverpb ./snap/snappb ./raft/raftpb ./mvcc/mvccpb ./lease/leasepb ./auth/authpb"
 
 # exact version of protoc-gen-gogo to build
 SHA="c3995ae437bb78d1189f4f147dfe5f87ad3596e4"
@@ -75,7 +75,7 @@ if [ "$1" = "-g" ]; then
 		echo "protodoc is updated"
 	popd
 
-	protodoc --directories="etcdserver/etcdserverpb=service_message,storage/storagepb=service_message,lease/leasepb=service_message,auth/authpb=service_message" \
+	protodoc --directories="etcdserver/etcdserverpb=service_message,mvcc/mvccpb=service_message,lease/leasepb=service_message,auth/authpb=service_message" \
 		--title="etcd API Reference" \
 		--output="Documentation/dev-guide/api_reference_v3.md" \
 		--message-only-from-this-file="etcdserver/etcdserverpb/rpc.proto"
diff --git a/snap/snappb/snap.pb.go b/snap/snappb/snap.pb.go
index 8b23ed6..7e5bbe3 100644
--- a/snap/snappb/snap.pb.go
+++ b/snap/snappb/snap.pb.go
@@ -337,13 +337,12 @@ var (
 )
 
 var fileDescriptorSnap = []byte{
-	// 118 bytes of a gzipped FileDescriptorProto
+	// 112 bytes of a gzipped FileDescriptorProto
 	0x1f, 0x8b, 0x08, 0x00, 0x00, 0x09, 0x6e, 0x88, 0x02, 0xff, 0xe2, 0xe2, 0x2a, 0xce, 0x4b, 0x2c,
 	0xd0, 0x2b, 0x28, 0xca, 0x2f, 0xc9, 0x17, 0x62, 0x03, 0xb1, 0x0b, 0x92, 0xa4, 0x44, 0xd2, 0xf3,
-	0xd3, 0xf3, 0xc1, 0x42, 0xfa, 0x20, 0x16, 0x44, 0x56, 0xc9, 0x8c, 0x8b, 0x03, 0x24, 0x5f, 0x9c,
-	0x91, 0x5f, 0x22, 0x24, 0xc6, 0xc5, 0x9c, 0x5c, 0x94, 0x2c, 0xc1, 0xa8, 0xc0, 0xa8, 0xc1, 0xeb,
-	0xc4, 0x72, 0xe2, 0x9e, 0x3c, 0x43, 0x10, 0x48, 0x40, 0x48, 0x88, 0x8b, 0x25, 0x25, 0xb1, 0x24,
-	0x51, 0x82, 0x09, 0x28, 0xc1, 0x13, 0x04, 0x66, 0x3b, 0x09, 0x9c, 0x78, 0x28, 0xc7, 0x70, 0xe2,
-	0x91, 0x1c, 0xe3, 0x05, 0x20, 0x7e, 0x00, 0xc4, 0x80, 0x00, 0x00, 0x00, 0xff, 0xff, 0x0d, 0x03,
-	0x97, 0xa7, 0x74, 0x00, 0x00, 0x00,
+	0xd3, 0xf3, 0xc1, 0x42, 0xfa, 0x20, 0x16, 0x44, 0x56, 0x49, 0x9b, 0x8b, 0x03, 0x24, 0x5f, 0x9c,
+	0x91, 0x5f, 0x22, 0x24, 0xc8, 0xc5, 0x9c, 0x5c, 0x94, 0x2c, 0xc1, 0xa8, 0xc0, 0xa8, 0xc1, 0xeb,
+	0xc4, 0x72, 0xe2, 0x9e, 0x3c, 0x83, 0x10, 0x0f, 0x17, 0x4b, 0x4a, 0x62, 0x49, 0xa2, 0x04, 0x13,
+	0x50, 0x8c, 0xc7, 0x49, 0xe0, 0xc4, 0x43, 0x39, 0x86, 0x13, 0x8f, 0xe4, 0x18, 0x2f, 0x00, 0xf1,
+	0x03, 0x20, 0x06, 0x04, 0x00, 0x00, 0xff, 0xff, 0x89, 0xff, 0x64, 0x86, 0x69, 0x00, 0x00, 0x00,
 }
diff --git a/storage/backend/backend.go b/storage/backend/backend.go
deleted file mode 100644
index 2625e4d..0000000
--- a/storage/backend/backend.go
+++ /dev/null
@@ -1,337 +0,0 @@
-// Copyright 2015 CoreOS, Inc.
-//
-// Licensed under the Apache License, Version 2.0 (the "License");
-// you may not use this file except in compliance with the License.
-// You may obtain a copy of the License at
-//
-//     http://www.apache.org/licenses/LICENSE-2.0
-//
-// Unless required by applicable law or agreed to in writing, software
-// distributed under the License is distributed on an "AS IS" BASIS,
-// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-// See the License for the specific language governing permissions and
-// limitations under the License.
-
-package backend
-
-import (
-	"fmt"
-	"hash/crc32"
-	"io"
-	"io/ioutil"
-	"log"
-	"os"
-	"path"
-	"sync"
-	"sync/atomic"
-	"time"
-
-	"github.com/boltdb/bolt"
-)
-
-var (
-	defaultBatchLimit    = 10000
-	defaultBatchInterval = 100 * time.Millisecond
-
-	defragLimit = 10000
-
-	// InitialMmapSize is the initial size of the mmapped region. Setting this larger than
-	// the potential max db size can prevent writer from blocking reader.
-	// This only works for linux.
-	InitialMmapSize = int64(10 * 1024 * 1024 * 1024)
-)
-
-const (
-	// DefaultQuotaBytes is the number of bytes the backend Size may
-	// consume before exceeding the space quota.
-	DefaultQuotaBytes = int64(2 * 1024 * 1024 * 1024) // 2GB
-	// MaxQuotaBytes is the maximum number of bytes suggested for a backend
-	// quota. A larger quota may lead to degraded performance.
-	MaxQuotaBytes = int64(8 * 1024 * 1024 * 1024) // 8GB
-)
-
-type Backend interface {
-	BatchTx() BatchTx
-	Snapshot() Snapshot
-	Hash() (uint32, error)
-	// Size returns the current size of the backend.
-	Size() int64
-	Defrag() error
-	ForceCommit()
-	Close() error
-}
-
-type Snapshot interface {
-	// Size gets the size of the snapshot.
-	Size() int64
-	// WriteTo writes the snapshot into the given writer.
-	WriteTo(w io.Writer) (n int64, err error)
-	// Close closes the snapshot.
-	Close() error
-}
-
-type backend struct {
-	// size and commits are used with atomic operations so they must be
-	// 64-bit aligned, otherwise 32-bit tests will crash
-
-	// size is the number of bytes in the backend
-	size int64
-	// commits counts number of commits since start
-	commits int64
-
-	mu sync.RWMutex
-	db *bolt.DB
-
-	batchInterval time.Duration
-	batchLimit    int
-	batchTx       *batchTx
-
-	stopc chan struct{}
-	donec chan struct{}
-}
-
-func New(path string, d time.Duration, limit int) Backend {
-	return newBackend(path, d, limit)
-}
-
-func NewDefaultBackend(path string) Backend {
-	return newBackend(path, defaultBatchInterval, defaultBatchLimit)
-}
-
-func newBackend(path string, d time.Duration, limit int) *backend {
-	db, err := bolt.Open(path, 0600, boltOpenOptions)
-	if err != nil {
-		log.Panicf("backend: cannot open database at %s (%v)", path, err)
-	}
-
-	b := &backend{
-		db: db,
-
-		batchInterval: d,
-		batchLimit:    limit,
-
-		stopc: make(chan struct{}),
-		donec: make(chan struct{}),
-	}
-	b.batchTx = newBatchTx(b)
-	go b.run()
-	return b
-}
-
-// BatchTx returns the current batch tx in coalescer. The tx can be used for read and
-// write operations. The write result can be retrieved within the same tx immediately.
-// The write result is isolated with other txs until the current one get committed.
-func (b *backend) BatchTx() BatchTx {
-	return b.batchTx
-}
-
-// ForceCommit forces the current batching tx to commit.
-func (b *backend) ForceCommit() {
-	b.batchTx.Commit()
-}
-
-func (b *backend) Snapshot() Snapshot {
-	b.batchTx.Commit()
-
-	b.mu.RLock()
-	defer b.mu.RUnlock()
-	tx, err := b.db.Begin(false)
-	if err != nil {
-		log.Fatalf("backend: cannot begin tx (%s)", err)
-	}
-	return &snapshot{tx}
-}
-
-func (b *backend) Hash() (uint32, error) {
-	h := crc32.New(crc32.MakeTable(crc32.Castagnoli))
-
-	b.mu.RLock()
-	defer b.mu.RUnlock()
-	err := b.db.View(func(tx *bolt.Tx) error {
-		c := tx.Cursor()
-		for next, _ := c.First(); next != nil; next, _ = c.Next() {
-			b := tx.Bucket(next)
-			if b == nil {
-				return fmt.Errorf("cannot get hash of bucket %s", string(next))
-			}
-			h.Write(next)
-			b.ForEach(func(k, v []byte) error {
-				h.Write(k)
-				h.Write(v)
-				return nil
-			})
-		}
-		return nil
-	})
-
-	if err != nil {
-		return 0, err
-	}
-
-	return h.Sum32(), nil
-}
-
-func (b *backend) Size() int64 {
-	return atomic.LoadInt64(&b.size)
-}
-
-func (b *backend) run() {
-	defer close(b.donec)
-
-	for {
-		select {
-		case <-time.After(b.batchInterval):
-		case <-b.stopc:
-			b.batchTx.CommitAndStop()
-			return
-		}
-		b.batchTx.Commit()
-	}
-}
-
-func (b *backend) Close() error {
-	close(b.stopc)
-	<-b.donec
-	return b.db.Close()
-}
-
-// Commits returns total number of commits since start
-func (b *backend) Commits() int64 {
-	return atomic.LoadInt64(&b.commits)
-}
-
-func (b *backend) Defrag() error {
-	err := b.defrag()
-	if err != nil {
-		return err
-	}
-
-	// commit to update metadata like db.size
-	b.batchTx.Commit()
-
-	return nil
-}
-
-func (b *backend) defrag() error {
-	// TODO: make this non-blocking?
-	// lock batchTx to ensure nobody is using previous tx, and then
-	// close previous ongoing tx.
-	b.batchTx.Lock()
-	defer b.batchTx.Unlock()
-
-	// lock database after lock tx to avoid deadlock.
-	b.mu.Lock()
-	defer b.mu.Unlock()
-
-	b.batchTx.commit(true)
-	b.batchTx.tx = nil
-
-	tmpdb, err := bolt.Open(b.db.Path()+".tmp", 0600, boltOpenOptions)
-	if err != nil {
-		return err
-	}
-
-	err = defragdb(b.db, tmpdb, defragLimit)
-
-	if err != nil {
-		tmpdb.Close()
-		os.RemoveAll(tmpdb.Path())
-		return err
-	}
-
-	dbp := b.db.Path()
-	tdbp := tmpdb.Path()
-
-	err = b.db.Close()
-	if err != nil {
-		log.Fatalf("backend: cannot close database (%s)", err)
-	}
-	err = tmpdb.Close()
-	if err != nil {
-		log.Fatalf("backend: cannot close database (%s)", err)
-	}
-	err = os.Rename(tdbp, dbp)
-	if err != nil {
-		log.Fatalf("backend: cannot rename database (%s)", err)
-	}
-
-	b.db, err = bolt.Open(dbp, 0600, boltOpenOptions)
-	if err != nil {
-		log.Panicf("backend: cannot open database at %s (%v)", dbp, err)
-	}
-	b.batchTx.tx, err = b.db.Begin(true)
-	if err != nil {
-		log.Fatalf("backend: cannot begin tx (%s)", err)
-	}
-
-	return nil
-}
-
-func defragdb(odb, tmpdb *bolt.DB, limit int) error {
-	// open a tx on tmpdb for writes
-	tmptx, err := tmpdb.Begin(true)
-	if err != nil {
-		return err
-	}
-
-	// open a tx on old db for read
-	tx, err := odb.Begin(false)
-	if err != nil {
-		return err
-	}
-	defer tx.Rollback()
-
-	c := tx.Cursor()
-
-	count := 0
-	for next, _ := c.First(); next != nil; next, _ = c.Next() {
-		b := tx.Bucket(next)
-		if b == nil {
-			return fmt.Errorf("backend: cannot defrag bucket %s", string(next))
-		}
-
-		tmpb, berr := tmptx.CreateBucketIfNotExists(next)
-		if berr != nil {
-			return berr
-		}
-
-		b.ForEach(func(k, v []byte) error {
-			count++
-			if count > limit {
-				err = tmptx.Commit()
-				if err != nil {
-					return err
-				}
-				tmptx, err = tmpdb.Begin(true)
-				if err != nil {
-					return err
-				}
-				tmpb = tmptx.Bucket(next)
-				count = 0
-			}
-			return tmpb.Put(k, v)
-		})
-	}
-
-	return tmptx.Commit()
-}
-
-// NewTmpBackend creates a backend implementation for testing.
-func NewTmpBackend(batchInterval time.Duration, batchLimit int) (*backend, string) {
-	dir, err := ioutil.TempDir(os.TempDir(), "etcd_backend_test")
-	if err != nil {
-		log.Fatal(err)
-	}
-	tmpPath := path.Join(dir, "database")
-	return newBackend(tmpPath, batchInterval, batchLimit), tmpPath
-}
-
-func NewDefaultTmpBackend() (*backend, string) {
-	return NewTmpBackend(defaultBatchInterval, defaultBatchLimit)
-}
-
-type snapshot struct {
-	*bolt.Tx
-}
-
-func (s *snapshot) Close() error { return s.Tx.Rollback() }
diff --git a/storage/backend/backend_bench_test.go b/storage/backend/backend_bench_test.go
deleted file mode 100644
index 1e798b8..0000000
--- a/storage/backend/backend_bench_test.go
+++ /dev/null
@@ -1,50 +0,0 @@
-// Copyright 2015 CoreOS, Inc.
-//
-// Licensed under the Apache License, Version 2.0 (the "License");
-// you may not use this file except in compliance with the License.
-// You may obtain a copy of the License at
-//
-//     http://www.apache.org/licenses/LICENSE-2.0
-//
-// Unless required by applicable law or agreed to in writing, software
-// distributed under the License is distributed on an "AS IS" BASIS,
-// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-// See the License for the specific language governing permissions and
-// limitations under the License.
-
-package backend
-
-import (
-	"crypto/rand"
-	"os"
-	"testing"
-	"time"
-)
-
-func BenchmarkBackendPut(b *testing.B) {
-	backend := New("test", 100*time.Millisecond, 10000)
-	defer backend.Close()
-	defer os.Remove("test")
-
-	// prepare keys
-	keys := make([][]byte, b.N)
-	for i := 0; i < b.N; i++ {
-		keys[i] = make([]byte, 64)
-		rand.Read(keys[i])
-	}
-	value := make([]byte, 128)
-	rand.Read(value)
-
-	batchTx := backend.BatchTx()
-
-	batchTx.Lock()
-	batchTx.UnsafeCreateBucket([]byte("test"))
-	batchTx.Unlock()
-
-	b.ResetTimer()
-	for i := 0; i < b.N; i++ {
-		batchTx.Lock()
-		batchTx.UnsafePut([]byte("test"), keys[i], value)
-		batchTx.Unlock()
-	}
-}
diff --git a/storage/backend/backend_test.go b/storage/backend/backend_test.go
deleted file mode 100644
index 1ec10d7..0000000
--- a/storage/backend/backend_test.go
+++ /dev/null
@@ -1,179 +0,0 @@
-// Copyright 2015 CoreOS, Inc.
-//
-// Licensed under the Apache License, Version 2.0 (the "License");
-// you may not use this file except in compliance with the License.
-// You may obtain a copy of the License at
-//
-//     http://www.apache.org/licenses/LICENSE-2.0
-//
-// Unless required by applicable law or agreed to in writing, software
-// distributed under the License is distributed on an "AS IS" BASIS,
-// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-// See the License for the specific language governing permissions and
-// limitations under the License.
-
-package backend
-
-import (
-	"fmt"
-	"io/ioutil"
-	"os"
-	"testing"
-	"time"
-
-	"github.com/boltdb/bolt"
-)
-
-func TestBackendClose(t *testing.T) {
-	b, tmpPath := NewTmpBackend(time.Hour, 10000)
-	defer os.Remove(tmpPath)
-
-	// check close could work
-	done := make(chan struct{})
-	go func() {
-		err := b.Close()
-		if err != nil {
-			t.Errorf("close error = %v, want nil", err)
-		}
-		done <- struct{}{}
-	}()
-	select {
-	case <-done:
-	case <-time.After(10 * time.Second):
-		t.Errorf("failed to close database in 10s")
-	}
-}
-
-func TestBackendSnapshot(t *testing.T) {
-	b, tmpPath := NewTmpBackend(time.Hour, 10000)
-	defer cleanup(b, tmpPath)
-
-	tx := b.BatchTx()
-	tx.Lock()
-	tx.UnsafeCreateBucket([]byte("test"))
-	tx.UnsafePut([]byte("test"), []byte("foo"), []byte("bar"))
-	tx.Unlock()
-	b.ForceCommit()
-
-	// write snapshot to a new file
-	f, err := ioutil.TempFile(os.TempDir(), "etcd_backend_test")
-	if err != nil {
-		t.Fatal(err)
-	}
-	snap := b.Snapshot()
-	defer snap.Close()
-	if _, err := snap.WriteTo(f); err != nil {
-		t.Fatal(err)
-	}
-	f.Close()
-
-	// bootstrap new backend from the snapshot
-	nb := New(f.Name(), time.Hour, 10000)
-	defer cleanup(nb, f.Name())
-
-	newTx := b.BatchTx()
-	newTx.Lock()
-	ks, _ := newTx.UnsafeRange([]byte("test"), []byte("foo"), []byte("goo"), 0)
-	if len(ks) != 1 {
-		t.Errorf("len(kvs) = %d, want 1", len(ks))
-	}
-	newTx.Unlock()
-}
-
-func TestBackendBatchIntervalCommit(t *testing.T) {
-	// start backend with super short batch interval so
-	// we do not need to wait long before commit to happen.
-	b, tmpPath := NewTmpBackend(time.Nanosecond, 10000)
-	defer cleanup(b, tmpPath)
-
-	pc := b.Commits()
-
-	tx := b.BatchTx()
-	tx.Lock()
-	tx.UnsafeCreateBucket([]byte("test"))
-	tx.UnsafePut([]byte("test"), []byte("foo"), []byte("bar"))
-	tx.Unlock()
-
-	for i := 0; i < 10; i++ {
-		if b.Commits() >= pc+1 {
-			break
-		}
-		time.Sleep(time.Duration(i*100) * time.Millisecond)
-	}
-
-	// check whether put happens via db view
-	b.db.View(func(tx *bolt.Tx) error {
-		bucket := tx.Bucket([]byte("test"))
-		if bucket == nil {
-			t.Errorf("bucket test does not exit")
-			return nil
-		}
-		v := bucket.Get([]byte("foo"))
-		if v == nil {
-			t.Errorf("foo key failed to written in backend")
-		}
-		return nil
-	})
-}
-
-func TestBackendDefrag(t *testing.T) {
-	b, tmpPath := NewDefaultTmpBackend()
-	defer cleanup(b, tmpPath)
-
-	tx := b.BatchTx()
-	tx.Lock()
-	tx.UnsafeCreateBucket([]byte("test"))
-	for i := 0; i < defragLimit+100; i++ {
-		tx.UnsafePut([]byte("test"), []byte(fmt.Sprintf("foo_%d", i)), []byte("bar"))
-	}
-	tx.Unlock()
-	b.ForceCommit()
-
-	// remove some keys to ensure the disk space will be reclaimed after defrag
-	tx = b.BatchTx()
-	tx.Lock()
-	for i := 0; i < 50; i++ {
-		tx.UnsafeDelete([]byte("test"), []byte(fmt.Sprintf("foo_%d", i)))
-	}
-	tx.Unlock()
-	b.ForceCommit()
-
-	size := b.Size()
-
-	// shrink and check hash
-	oh, err := b.Hash()
-	if err != nil {
-		t.Fatal(err)
-	}
-
-	err = b.Defrag()
-	if err != nil {
-		t.Fatal(err)
-	}
-
-	nh, err := b.Hash()
-	if err != nil {
-		t.Fatal(err)
-	}
-	if oh != nh {
-		t.Errorf("hash = %v, want %v", nh, oh)
-	}
-
-	nsize := b.Size()
-	if nsize >= size {
-		t.Errorf("new size = %v, want < %d", nsize, size)
-	}
-
-	// try put more keys after shrink.
-	tx = b.BatchTx()
-	tx.Lock()
-	tx.UnsafeCreateBucket([]byte("test"))
-	tx.UnsafePut([]byte("test"), []byte("more"), []byte("bar"))
-	tx.Unlock()
-	b.ForceCommit()
-}
-
-func cleanup(b Backend, path string) {
-	b.Close()
-	os.Remove(path)
-}
diff --git a/storage/backend/batch_tx.go b/storage/backend/batch_tx.go
deleted file mode 100644
index 24817b3..0000000
--- a/storage/backend/batch_tx.go
+++ /dev/null
@@ -1,189 +0,0 @@
-// Copyright 2015 CoreOS, Inc.
-//
-// Licensed under the Apache License, Version 2.0 (the "License");
-// you may not use this file except in compliance with the License.
-// You may obtain a copy of the License at
-//
-//     http://www.apache.org/licenses/LICENSE-2.0
-//
-// Unless required by applicable law or agreed to in writing, software
-// distributed under the License is distributed on an "AS IS" BASIS,
-// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-// See the License for the specific language governing permissions and
-// limitations under the License.
-
-package backend
-
-import (
-	"bytes"
-	"log"
-	"sync"
-	"sync/atomic"
-
-	"github.com/boltdb/bolt"
-)
-
-type BatchTx interface {
-	Lock()
-	Unlock()
-	UnsafeCreateBucket(name []byte)
-	UnsafePut(bucketName []byte, key []byte, value []byte)
-	UnsafeSeqPut(bucketName []byte, key []byte, value []byte)
-	UnsafeRange(bucketName []byte, key, endKey []byte, limit int64) (keys [][]byte, vals [][]byte)
-	UnsafeDelete(bucketName []byte, key []byte)
-	UnsafeForEach(bucketName []byte, visitor func(k, v []byte) error) error
-	Commit()
-	CommitAndStop()
-}
-
-type batchTx struct {
-	sync.Mutex
-	tx      *bolt.Tx
-	backend *backend
-	pending int
-}
-
-func newBatchTx(backend *backend) *batchTx {
-	tx := &batchTx{backend: backend}
-	tx.Commit()
-	return tx
-}
-
-func (t *batchTx) UnsafeCreateBucket(name []byte) {
-	_, err := t.tx.CreateBucket(name)
-	if err != nil && err != bolt.ErrBucketExists {
-		log.Fatalf("storage: cannot create bucket %s (%v)", string(name), err)
-	}
-	t.pending++
-}
-
-// UnsafePut must be called holding the lock on the tx.
-func (t *batchTx) UnsafePut(bucketName []byte, key []byte, value []byte) {
-	t.unsafePut(bucketName, key, value, false)
-}
-
-// UnsafeSeqPut must be called holding the lock on the tx.
-func (t *batchTx) UnsafeSeqPut(bucketName []byte, key []byte, value []byte) {
-	t.unsafePut(bucketName, key, value, true)
-}
-
-func (t *batchTx) unsafePut(bucketName []byte, key []byte, value []byte, seq bool) {
-	bucket := t.tx.Bucket(bucketName)
-	if bucket == nil {
-		log.Fatalf("storage: bucket %s does not exist", string(bucketName))
-	}
-	if seq {
-		// it is useful to increase fill percent when the workloads are mostly append-only.
-		// this can delay the page split and reduce space usage.
-		bucket.FillPercent = 0.9
-	}
-	if err := bucket.Put(key, value); err != nil {
-		log.Fatalf("storage: cannot put key into bucket (%v)", err)
-	}
-	t.pending++
-}
-
-// UnsafeRange must be called holding the lock on the tx.
-func (t *batchTx) UnsafeRange(bucketName []byte, key, endKey []byte, limit int64) (keys [][]byte, vs [][]byte) {
-	bucket := t.tx.Bucket(bucketName)
-	if bucket == nil {
-		log.Fatalf("storage: bucket %s does not exist", string(bucketName))
-	}
-
-	if len(endKey) == 0 {
-		if v := bucket.Get(key); v == nil {
-			return keys, vs
-		} else {
-			return append(keys, key), append(vs, v)
-		}
-	}
-
-	c := bucket.Cursor()
-	for ck, cv := c.Seek(key); ck != nil && bytes.Compare(ck, endKey) < 0; ck, cv = c.Next() {
-		vs = append(vs, cv)
-		keys = append(keys, ck)
-		if limit > 0 && limit == int64(len(keys)) {
-			break
-		}
-	}
-
-	return keys, vs
-}
-
-// UnsafeDelete must be called holding the lock on the tx.
-func (t *batchTx) UnsafeDelete(bucketName []byte, key []byte) {
-	bucket := t.tx.Bucket(bucketName)
-	if bucket == nil {
-		log.Fatalf("storage: bucket %s does not exist", string(bucketName))
-	}
-	err := bucket.Delete(key)
-	if err != nil {
-		log.Fatalf("storage: cannot delete key from bucket (%v)", err)
-	}
-	t.pending++
-}
-
-// UnsafeForEach must be called holding the lock on the tx.
-func (t *batchTx) UnsafeForEach(bucketName []byte, visitor func(k, v []byte) error) error {
-	b := t.tx.Bucket(bucketName)
-	if b == nil {
-		// bucket does not exist
-		return nil
-	}
-	return b.ForEach(visitor)
-}
-
-// Commit commits a previous tx and begins a new writable one.
-func (t *batchTx) Commit() {
-	t.Lock()
-	defer t.Unlock()
-	t.commit(false)
-}
-
-// CommitAndStop commits the previous tx and do not create a new one.
-func (t *batchTx) CommitAndStop() {
-	t.Lock()
-	defer t.Unlock()
-	t.commit(true)
-}
-
-func (t *batchTx) Unlock() {
-	if t.pending >= t.backend.batchLimit {
-		t.commit(false)
-		t.pending = 0
-	}
-	t.Mutex.Unlock()
-}
-
-func (t *batchTx) commit(stop bool) {
-	var err error
-	// commit the last tx
-	if t.tx != nil {
-		if t.pending == 0 && !stop {
-			t.backend.mu.RLock()
-			defer t.backend.mu.RUnlock()
-			atomic.StoreInt64(&t.backend.size, t.tx.Size())
-			return
-		}
-		err = t.tx.Commit()
-		atomic.AddInt64(&t.backend.commits, 1)
-
-		t.pending = 0
-		if err != nil {
-			log.Fatalf("storage: cannot commit tx (%s)", err)
-		}
-	}
-
-	if stop {
-		return
-	}
-
-	t.backend.mu.RLock()
-	defer t.backend.mu.RUnlock()
-	// begin a new tx
-	t.tx, err = t.backend.db.Begin(true)
-	if err != nil {
-		log.Fatalf("storage: cannot begin tx (%s)", err)
-	}
-	atomic.StoreInt64(&t.backend.size, t.tx.Size())
-}
diff --git a/storage/backend/batch_tx_test.go b/storage/backend/batch_tx_test.go
deleted file mode 100644
index e8b1545..0000000
--- a/storage/backend/batch_tx_test.go
+++ /dev/null
@@ -1,197 +0,0 @@
-// Copyright 2015 CoreOS, Inc.
-//
-// Licensed under the Apache License, Version 2.0 (the "License");
-// you may not use this file except in compliance with the License.
-// You may obtain a copy of the License at
-//
-//     http://www.apache.org/licenses/LICENSE-2.0
-//
-// Unless required by applicable law or agreed to in writing, software
-// distributed under the License is distributed on an "AS IS" BASIS,
-// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-// See the License for the specific language governing permissions and
-// limitations under the License.
-
-package backend
-
-import (
-	"reflect"
-	"testing"
-	"time"
-
-	"github.com/boltdb/bolt"
-)
-
-func TestBatchTxPut(t *testing.T) {
-	b, tmpPath := NewTmpBackend(time.Hour, 10000)
-	defer cleanup(b, tmpPath)
-
-	tx := b.batchTx
-	tx.Lock()
-	defer tx.Unlock()
-
-	// create bucket
-	tx.UnsafeCreateBucket([]byte("test"))
-
-	// put
-	v := []byte("bar")
-	tx.UnsafePut([]byte("test"), []byte("foo"), v)
-
-	// check put result before and after tx is committed
-	for k := 0; k < 2; k++ {
-		_, gv := tx.UnsafeRange([]byte("test"), []byte("foo"), nil, 0)
-		if !reflect.DeepEqual(gv[0], v) {
-			t.Errorf("v = %s, want %s", string(gv[0]), string(v))
-		}
-		tx.commit(false)
-	}
-}
-
-func TestBatchTxRange(t *testing.T) {
-	b, tmpPath := NewTmpBackend(time.Hour, 10000)
-	defer cleanup(b, tmpPath)
-
-	tx := b.batchTx
-	tx.Lock()
-	defer tx.Unlock()
-
-	tx.UnsafeCreateBucket([]byte("test"))
-	// put keys
-	allKeys := [][]byte{[]byte("foo"), []byte("foo1"), []byte("foo2")}
-	allVals := [][]byte{[]byte("bar"), []byte("bar1"), []byte("bar2")}
-	for i := range allKeys {
-		tx.UnsafePut([]byte("test"), allKeys[i], allVals[i])
-	}
-
-	tests := []struct {
-		key    []byte
-		endKey []byte
-		limit  int64
-
-		wkeys [][]byte
-		wvals [][]byte
-	}{
-		// single key
-		{
-			[]byte("foo"), nil, 0,
-			allKeys[:1], allVals[:1],
-		},
-		// single key, bad
-		{
-			[]byte("doo"), nil, 0,
-			nil, nil,
-		},
-		// key range
-		{
-			[]byte("foo"), []byte("foo1"), 0,
-			allKeys[:1], allVals[:1],
-		},
-		// key range, get all keys
-		{
-			[]byte("foo"), []byte("foo3"), 0,
-			allKeys, allVals,
-		},
-		// key range, bad
-		{
-			[]byte("goo"), []byte("goo3"), 0,
-			nil, nil,
-		},
-		// key range with effective limit
-		{
-			[]byte("foo"), []byte("foo3"), 1,
-			allKeys[:1], allVals[:1],
-		},
-		// key range with limit
-		{
-			[]byte("foo"), []byte("foo3"), 4,
-			allKeys, allVals,
-		},
-	}
-	for i, tt := range tests {
-		keys, vals := tx.UnsafeRange([]byte("test"), tt.key, tt.endKey, tt.limit)
-		if !reflect.DeepEqual(keys, tt.wkeys) {
-			t.Errorf("#%d: keys = %+v, want %+v", i, keys, tt.wkeys)
-		}
-		if !reflect.DeepEqual(vals, tt.wvals) {
-			t.Errorf("#%d: vals = %+v, want %+v", i, vals, tt.wvals)
-		}
-	}
-}
-
-func TestBatchTxDelete(t *testing.T) {
-	b, tmpPath := NewTmpBackend(time.Hour, 10000)
-	defer cleanup(b, tmpPath)
-
-	tx := b.batchTx
-	tx.Lock()
-	defer tx.Unlock()
-
-	tx.UnsafeCreateBucket([]byte("test"))
-	tx.UnsafePut([]byte("test"), []byte("foo"), []byte("bar"))
-
-	tx.UnsafeDelete([]byte("test"), []byte("foo"))
-
-	// check put result before and after tx is committed
-	for k := 0; k < 2; k++ {
-		ks, _ := tx.UnsafeRange([]byte("test"), []byte("foo"), nil, 0)
-		if len(ks) != 0 {
-			t.Errorf("keys on foo = %v, want nil", ks)
-		}
-		tx.commit(false)
-	}
-}
-
-func TestBatchTxCommit(t *testing.T) {
-	b, tmpPath := NewTmpBackend(time.Hour, 10000)
-	defer cleanup(b, tmpPath)
-
-	tx := b.batchTx
-	tx.Lock()
-	tx.UnsafeCreateBucket([]byte("test"))
-	tx.UnsafePut([]byte("test"), []byte("foo"), []byte("bar"))
-	tx.Unlock()
-
-	tx.Commit()
-
-	// check whether put happens via db view
-	b.db.View(func(tx *bolt.Tx) error {
-		bucket := tx.Bucket([]byte("test"))
-		if bucket == nil {
-			t.Errorf("bucket test does not exit")
-			return nil
-		}
-		v := bucket.Get([]byte("foo"))
-		if v == nil {
-			t.Errorf("foo key failed to written in backend")
-		}
-		return nil
-	})
-}
-
-func TestBatchTxBatchLimitCommit(t *testing.T) {
-	// start backend with batch limit 1 so one write can
-	// trigger a commit
-	b, tmpPath := NewTmpBackend(time.Hour, 1)
-	defer cleanup(b, tmpPath)
-
-	tx := b.batchTx
-	tx.Lock()
-	tx.UnsafeCreateBucket([]byte("test"))
-	tx.UnsafePut([]byte("test"), []byte("foo"), []byte("bar"))
-	tx.Unlock()
-
-	// batch limit commit should have been triggered
-	// check whether put happens via db view
-	b.db.View(func(tx *bolt.Tx) error {
-		bucket := tx.Bucket([]byte("test"))
-		if bucket == nil {
-			t.Errorf("bucket test does not exit")
-			return nil
-		}
-		v := bucket.Get([]byte("foo"))
-		if v == nil {
-			t.Errorf("foo key failed to written in backend")
-		}
-		return nil
-	})
-}
diff --git a/storage/backend/boltoption_darwin.go b/storage/backend/boltoption_darwin.go
deleted file mode 100644
index 67269f0..0000000
--- a/storage/backend/boltoption_darwin.go
+++ /dev/null
@@ -1,19 +0,0 @@
-// Copyright 2015 CoreOS, Inc.
-//
-// Licensed under the Apache License, Version 2.0 (the "License");
-// you may not use this file except in compliance with the License.
-// You may obtain a copy of the License at
-//
-//     http://www.apache.org/licenses/LICENSE-2.0
-//
-// Unless required by applicable law or agreed to in writing, software
-// distributed under the License is distributed on an "AS IS" BASIS,
-// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-// See the License for the specific language governing permissions and
-// limitations under the License.
-
-package backend
-
-import "github.com/boltdb/bolt"
-
-var boltOpenOptions *bolt.Options = nil
diff --git a/storage/backend/boltoption_freebsd.go b/storage/backend/boltoption_freebsd.go
deleted file mode 100644
index 67269f0..0000000
--- a/storage/backend/boltoption_freebsd.go
+++ /dev/null
@@ -1,19 +0,0 @@
-// Copyright 2015 CoreOS, Inc.
-//
-// Licensed under the Apache License, Version 2.0 (the "License");
-// you may not use this file except in compliance with the License.
-// You may obtain a copy of the License at
-//
-//     http://www.apache.org/licenses/LICENSE-2.0
-//
-// Unless required by applicable law or agreed to in writing, software
-// distributed under the License is distributed on an "AS IS" BASIS,
-// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-// See the License for the specific language governing permissions and
-// limitations under the License.
-
-package backend
-
-import "github.com/boltdb/bolt"
-
-var boltOpenOptions *bolt.Options = nil
diff --git a/storage/backend/boltoption_solaris.go b/storage/backend/boltoption_solaris.go
deleted file mode 100644
index f12bf58..0000000
--- a/storage/backend/boltoption_solaris.go
+++ /dev/null
@@ -1,19 +0,0 @@
-// Copyright 2016 CoreOS, Inc.
-//
-// Licensed under the Apache License, Version 2.0 (the "License");
-// you may not use this file except in compliance with the License.
-// You may obtain a copy of the License at
-//
-//     http://www.apache.org/licenses/LICENSE-2.0
-//
-// Unless required by applicable law or agreed to in writing, software
-// distributed under the License is distributed on an "AS IS" BASIS,
-// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-// See the License for the specific language governing permissions and
-// limitations under the License.
-
-package backend
-
-import "github.com/boltdb/bolt"
-
-var boltOpenOptions *bolt.Options = nil
diff --git a/storage/backend/boltoption_unix.go b/storage/backend/boltoption_unix.go
deleted file mode 100644
index 017f603..0000000
--- a/storage/backend/boltoption_unix.go
+++ /dev/null
@@ -1,34 +0,0 @@
-// Copyright 2015 CoreOS, Inc.
-//
-// Licensed under the Apache License, Version 2.0 (the "License");
-// you may not use this file except in compliance with the License.
-// You may obtain a copy of the License at
-//
-//     http://www.apache.org/licenses/LICENSE-2.0
-//
-// Unless required by applicable law or agreed to in writing, software
-// distributed under the License is distributed on an "AS IS" BASIS,
-// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-// See the License for the specific language governing permissions and
-// limitations under the License.
-
-// +build linux
-
-package backend
-
-import (
-	"syscall"
-
-	"github.com/boltdb/bolt"
-)
-
-// syscall.MAP_POPULATE on linux 2.6.23+ does sequential read-ahead
-// which can speed up entire-database read with boltdb. We want to
-// enable MAP_POPULATE for faster key-value store recovery in storage
-// package. If your kernel version is lower than 2.6.23
-// (https://github.com/torvalds/linux/releases/tag/v2.6.23), mmap might
-// silently ignore this flag. Please update your kernel to prevent this.
-var boltOpenOptions = &bolt.Options{
-	MmapFlags:       syscall.MAP_POPULATE,
-	InitialMmapSize: int(InitialMmapSize),
-}
diff --git a/storage/backend/boltoption_windows.go b/storage/backend/boltoption_windows.go
deleted file mode 100644
index 1a790d1..0000000
--- a/storage/backend/boltoption_windows.go
+++ /dev/null
@@ -1,21 +0,0 @@
-// Copyright 2015 CoreOS, Inc.
-//
-// Licensed under the Apache License, Version 2.0 (the "License");
-// you may not use this file except in compliance with the License.
-// You may obtain a copy of the License at
-//
-//     http://www.apache.org/licenses/LICENSE-2.0
-//
-// Unless required by applicable law or agreed to in writing, software
-// distributed under the License is distributed on an "AS IS" BASIS,
-// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-// See the License for the specific language governing permissions and
-// limitations under the License.
-
-package backend
-
-import "github.com/boltdb/bolt"
-
-// TODO: support syscall.MAP_POPULATE in windows.
-// Need upstream patch from boltdb/bolt.
-var boltOpenOptions *bolt.Options = nil
diff --git a/storage/backend/doc.go b/storage/backend/doc.go
deleted file mode 100644
index 50cd8d5..0000000
--- a/storage/backend/doc.go
+++ /dev/null
@@ -1,16 +0,0 @@
-// Copyright 2015 CoreOS, Inc.
-//
-// Licensed under the Apache License, Version 2.0 (the "License");
-// you may not use this file except in compliance with the License.
-// You may obtain a copy of the License at
-//
-//     http://www.apache.org/licenses/LICENSE-2.0
-//
-// Unless required by applicable law or agreed to in writing, software
-// distributed under the License is distributed on an "AS IS" BASIS,
-// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-// See the License for the specific language governing permissions and
-// limitations under the License.
-
-// Package backend defines a standard interface for etcd's backend storage.
-package backend
diff --git a/storage/doc.go b/storage/doc.go
deleted file mode 100644
index 7de6851..0000000
--- a/storage/doc.go
+++ /dev/null
@@ -1,16 +0,0 @@
-// Copyright 2015 CoreOS, Inc.
-//
-// Licensed under the Apache License, Version 2.0 (the "License");
-// you may not use this file except in compliance with the License.
-// You may obtain a copy of the License at
-//
-//     http://www.apache.org/licenses/LICENSE-2.0
-//
-// Unless required by applicable law or agreed to in writing, software
-// distributed under the License is distributed on an "AS IS" BASIS,
-// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-// See the License for the specific language governing permissions and
-// limitations under the License.
-
-// Package storage defines etcd's stable storage.
-package storage
diff --git a/storage/index.go b/storage/index.go
deleted file mode 100644
index b9212a2..0000000
--- a/storage/index.go
+++ /dev/null
@@ -1,218 +0,0 @@
-// Copyright 2015 CoreOS, Inc.
-//
-// Licensed under the Apache License, Version 2.0 (the "License");
-// you may not use this file except in compliance with the License.
-// You may obtain a copy of the License at
-//
-//     http://www.apache.org/licenses/LICENSE-2.0
-//
-// Unless required by applicable law or agreed to in writing, software
-// distributed under the License is distributed on an "AS IS" BASIS,
-// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-// See the License for the specific language governing permissions and
-// limitations under the License.
-
-package storage
-
-import (
-	"log"
-	"sort"
-	"sync"
-
-	"github.com/google/btree"
-)
-
-type index interface {
-	Get(key []byte, atRev int64) (rev, created revision, ver int64, err error)
-	Range(key, end []byte, atRev int64) ([][]byte, []revision)
-	Put(key []byte, rev revision)
-	Restore(key []byte, created, modified revision, ver int64)
-	Tombstone(key []byte, rev revision) error
-	RangeSince(key, end []byte, rev int64) []revision
-	Compact(rev int64) map[revision]struct{}
-	Equal(b index) bool
-}
-
-type treeIndex struct {
-	sync.RWMutex
-	tree *btree.BTree
-}
-
-func newTreeIndex() index {
-	return &treeIndex{
-		tree: btree.New(32),
-	}
-}
-
-func (ti *treeIndex) Put(key []byte, rev revision) {
-	keyi := &keyIndex{key: key}
-
-	ti.Lock()
-	defer ti.Unlock()
-	item := ti.tree.Get(keyi)
-	if item == nil {
-		keyi.put(rev.main, rev.sub)
-		ti.tree.ReplaceOrInsert(keyi)
-		return
-	}
-	okeyi := item.(*keyIndex)
-	okeyi.put(rev.main, rev.sub)
-}
-
-func (ti *treeIndex) Restore(key []byte, created, modified revision, ver int64) {
-	keyi := &keyIndex{key: key}
-
-	ti.Lock()
-	defer ti.Unlock()
-	item := ti.tree.Get(keyi)
-	if item == nil {
-		keyi.restore(created, modified, ver)
-		ti.tree.ReplaceOrInsert(keyi)
-		return
-	}
-	okeyi := item.(*keyIndex)
-	okeyi.put(modified.main, modified.sub)
-}
-
-func (ti *treeIndex) Get(key []byte, atRev int64) (modified, created revision, ver int64, err error) {
-	keyi := &keyIndex{key: key}
-
-	ti.RLock()
-	defer ti.RUnlock()
-	item := ti.tree.Get(keyi)
-	if item == nil {
-		return revision{}, revision{}, 0, ErrRevisionNotFound
-	}
-
-	keyi = item.(*keyIndex)
-	return keyi.get(atRev)
-}
-
-func (ti *treeIndex) Range(key, end []byte, atRev int64) (keys [][]byte, revs []revision) {
-	if end == nil {
-		rev, _, _, err := ti.Get(key, atRev)
-		if err != nil {
-			return nil, nil
-		}
-		return [][]byte{key}, []revision{rev}
-	}
-
-	keyi := &keyIndex{key: key}
-	endi := &keyIndex{key: end}
-
-	ti.RLock()
-	defer ti.RUnlock()
-
-	ti.tree.AscendGreaterOrEqual(keyi, func(item btree.Item) bool {
-		if len(endi.key) > 0 && !item.Less(endi) {
-			return false
-		}
-		curKeyi := item.(*keyIndex)
-		rev, _, _, err := curKeyi.get(atRev)
-		if err != nil {
-			return true
-		}
-		revs = append(revs, rev)
-		keys = append(keys, curKeyi.key)
-		return true
-	})
-
-	return keys, revs
-}
-
-func (ti *treeIndex) Tombstone(key []byte, rev revision) error {
-	keyi := &keyIndex{key: key}
-
-	ti.Lock()
-	defer ti.Unlock()
-	item := ti.tree.Get(keyi)
-	if item == nil {
-		return ErrRevisionNotFound
-	}
-
-	ki := item.(*keyIndex)
-	return ki.tombstone(rev.main, rev.sub)
-}
-
-// RangeSince returns all revisions from key(including) to end(excluding)
-// at or after the given rev. The returned slice is sorted in the order
-// of revision.
-func (ti *treeIndex) RangeSince(key, end []byte, rev int64) []revision {
-	ti.RLock()
-	defer ti.RUnlock()
-
-	keyi := &keyIndex{key: key}
-	if end == nil {
-		item := ti.tree.Get(keyi)
-		if item == nil {
-			return nil
-		}
-		keyi = item.(*keyIndex)
-		return keyi.since(rev)
-	}
-
-	endi := &keyIndex{key: end}
-	var revs []revision
-	ti.tree.AscendGreaterOrEqual(keyi, func(item btree.Item) bool {
-		if len(endi.key) > 0 && !item.Less(endi) {
-			return false
-		}
-		curKeyi := item.(*keyIndex)
-		revs = append(revs, curKeyi.since(rev)...)
-		return true
-	})
-	sort.Sort(revisions(revs))
-
-	return revs
-}
-
-func (ti *treeIndex) Compact(rev int64) map[revision]struct{} {
-	available := make(map[revision]struct{})
-	var emptyki []*keyIndex
-	log.Printf("store.index: compact %d", rev)
-	// TODO: do not hold the lock for long time?
-	// This is probably OK. Compacting 10M keys takes O(10ms).
-	ti.Lock()
-	defer ti.Unlock()
-	ti.tree.Ascend(compactIndex(rev, available, &emptyki))
-	for _, ki := range emptyki {
-		item := ti.tree.Delete(ki)
-		if item == nil {
-			log.Panic("store.index: unexpected delete failure during compaction")
-		}
-	}
-	return available
-}
-
-func compactIndex(rev int64, available map[revision]struct{}, emptyki *[]*keyIndex) func(i btree.Item) bool {
-	return func(i btree.Item) bool {
-		keyi := i.(*keyIndex)
-		keyi.compact(rev, available)
-		if keyi.isEmpty() {
-			*emptyki = append(*emptyki, keyi)
-		}
-		return true
-	}
-}
-
-func (a *treeIndex) Equal(bi index) bool {
-	b := bi.(*treeIndex)
-
-	if a.tree.Len() != b.tree.Len() {
-		return false
-	}
-
-	equal := true
-
-	a.tree.Ascend(func(item btree.Item) bool {
-		aki := item.(*keyIndex)
-		bki := b.tree.Get(item).(*keyIndex)
-		if !aki.equal(bki) {
-			equal = false
-			return false
-		}
-		return true
-	})
-
-	return equal
-}
diff --git a/storage/index_test.go b/storage/index_test.go
deleted file mode 100644
index b1c9af7..0000000
--- a/storage/index_test.go
+++ /dev/null
@@ -1,323 +0,0 @@
-// Copyright 2015 CoreOS, Inc.
-//
-// Licensed under the Apache License, Version 2.0 (the "License");
-// you may not use this file except in compliance with the License.
-// You may obtain a copy of the License at
-//
-//     http://www.apache.org/licenses/LICENSE-2.0
-//
-// Unless required by applicable law or agreed to in writing, software
-// distributed under the License is distributed on an "AS IS" BASIS,
-// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-// See the License for the specific language governing permissions and
-// limitations under the License.
-
-package storage
-
-import (
-	"reflect"
-	"testing"
-)
-
-func TestIndexGet(t *testing.T) {
-	ti := newTreeIndex()
-	ti.Put([]byte("foo"), revision{main: 2})
-	ti.Put([]byte("foo"), revision{main: 4})
-	ti.Tombstone([]byte("foo"), revision{main: 6})
-
-	tests := []struct {
-		rev int64
-
-		wrev     revision
-		wcreated revision
-		wver     int64
-		werr     error
-	}{
-		{0, revision{}, revision{}, 0, ErrRevisionNotFound},
-		{1, revision{}, revision{}, 0, ErrRevisionNotFound},
-		{2, revision{main: 2}, revision{main: 2}, 1, nil},
-		{3, revision{main: 2}, revision{main: 2}, 1, nil},
-		{4, revision{main: 4}, revision{main: 2}, 2, nil},
-		{5, revision{main: 4}, revision{main: 2}, 2, nil},
-		{6, revision{}, revision{}, 0, ErrRevisionNotFound},
-	}
-	for i, tt := range tests {
-		rev, created, ver, err := ti.Get([]byte("foo"), tt.rev)
-		if err != tt.werr {
-			t.Errorf("#%d: err = %v, want %v", i, err, tt.werr)
-		}
-		if rev != tt.wrev {
-			t.Errorf("#%d: rev = %+v, want %+v", i, rev, tt.wrev)
-		}
-		if created != tt.wcreated {
-			t.Errorf("#%d: created = %+v, want %+v", i, created, tt.wcreated)
-		}
-		if ver != tt.wver {
-			t.Errorf("#%d: ver = %d, want %d", i, ver, tt.wver)
-		}
-	}
-}
-
-func TestIndexRange(t *testing.T) {
-	allKeys := [][]byte{[]byte("foo"), []byte("foo1"), []byte("foo2")}
-	allRevs := []revision{{main: 1}, {main: 2}, {main: 3}}
-
-	ti := newTreeIndex()
-	for i := range allKeys {
-		ti.Put(allKeys[i], allRevs[i])
-	}
-
-	atRev := int64(3)
-	tests := []struct {
-		key, end []byte
-		wkeys    [][]byte
-		wrevs    []revision
-	}{
-		// single key that not found
-		{
-			[]byte("bar"), nil, nil, nil,
-		},
-		// single key that found
-		{
-			[]byte("foo"), nil, allKeys[:1], allRevs[:1],
-		},
-		// range keys, return first member
-		{
-			[]byte("foo"), []byte("foo1"), allKeys[:1], allRevs[:1],
-		},
-		// range keys, return first two members
-		{
-			[]byte("foo"), []byte("foo2"), allKeys[:2], allRevs[:2],
-		},
-		// range keys, return all members
-		{
-			[]byte("foo"), []byte("fop"), allKeys, allRevs,
-		},
-		// range keys, return last two members
-		{
-			[]byte("foo1"), []byte("fop"), allKeys[1:], allRevs[1:],
-		},
-		// range keys, return last member
-		{
-			[]byte("foo2"), []byte("fop"), allKeys[2:], allRevs[2:],
-		},
-		// range keys, return nothing
-		{
-			[]byte("foo3"), []byte("fop"), nil, nil,
-		},
-	}
-	for i, tt := range tests {
-		keys, revs := ti.Range(tt.key, tt.end, atRev)
-		if !reflect.DeepEqual(keys, tt.wkeys) {
-			t.Errorf("#%d: keys = %+v, want %+v", i, keys, tt.wkeys)
-		}
-		if !reflect.DeepEqual(revs, tt.wrevs) {
-			t.Errorf("#%d: revs = %+v, want %+v", i, revs, tt.wrevs)
-		}
-	}
-}
-
-func TestIndexTombstone(t *testing.T) {
-	ti := newTreeIndex()
-	ti.Put([]byte("foo"), revision{main: 1})
-
-	err := ti.Tombstone([]byte("foo"), revision{main: 2})
-	if err != nil {
-		t.Errorf("tombstone error = %v, want nil", err)
-	}
-
-	_, _, _, err = ti.Get([]byte("foo"), 2)
-	if err != ErrRevisionNotFound {
-		t.Errorf("get error = %v, want nil", err)
-	}
-	err = ti.Tombstone([]byte("foo"), revision{main: 3})
-	if err != ErrRevisionNotFound {
-		t.Errorf("tombstone error = %v, want %v", err, ErrRevisionNotFound)
-	}
-}
-
-func TestIndexRangeSince(t *testing.T) {
-	allKeys := [][]byte{[]byte("foo"), []byte("foo1"), []byte("foo2"), []byte("foo2"), []byte("foo1"), []byte("foo")}
-	allRevs := []revision{{main: 1}, {main: 2}, {main: 3}, {main: 4}, {main: 5}, {main: 6}}
-
-	ti := newTreeIndex()
-	for i := range allKeys {
-		ti.Put(allKeys[i], allRevs[i])
-	}
-
-	atRev := int64(1)
-	tests := []struct {
-		key, end []byte
-		wrevs    []revision
-	}{
-		// single key that not found
-		{
-			[]byte("bar"), nil, nil,
-		},
-		// single key that found
-		{
-			[]byte("foo"), nil, []revision{{main: 1}, {main: 6}},
-		},
-		// range keys, return first member
-		{
-			[]byte("foo"), []byte("foo1"), []revision{{main: 1}, {main: 6}},
-		},
-		// range keys, return first two members
-		{
-			[]byte("foo"), []byte("foo2"), []revision{{main: 1}, {main: 2}, {main: 5}, {main: 6}},
-		},
-		// range keys, return all members
-		{
-			[]byte("foo"), []byte("fop"), allRevs,
-		},
-		// range keys, return last two members
-		{
-			[]byte("foo1"), []byte("fop"), []revision{{main: 2}, {main: 3}, {main: 4}, {main: 5}},
-		},
-		// range keys, return last member
-		{
-			[]byte("foo2"), []byte("fop"), []revision{{main: 3}, {main: 4}},
-		},
-		// range keys, return nothing
-		{
-			[]byte("foo3"), []byte("fop"), nil,
-		},
-	}
-	for i, tt := range tests {
-		revs := ti.RangeSince(tt.key, tt.end, atRev)
-		if !reflect.DeepEqual(revs, tt.wrevs) {
-			t.Errorf("#%d: revs = %+v, want %+v", i, revs, tt.wrevs)
-		}
-	}
-}
-
-func TestIndexCompact(t *testing.T) {
-	maxRev := int64(20)
-	tests := []struct {
-		key     []byte
-		remove  bool
-		rev     revision
-		created revision
-		ver     int64
-	}{
-		{[]byte("foo"), false, revision{main: 1}, revision{main: 1}, 1},
-		{[]byte("foo1"), false, revision{main: 2}, revision{main: 2}, 1},
-		{[]byte("foo2"), false, revision{main: 3}, revision{main: 3}, 1},
-		{[]byte("foo2"), false, revision{main: 4}, revision{main: 3}, 2},
-		{[]byte("foo"), false, revision{main: 5}, revision{main: 1}, 2},
-		{[]byte("foo1"), false, revision{main: 6}, revision{main: 2}, 2},
-		{[]byte("foo1"), true, revision{main: 7}, revision{}, 0},
-		{[]byte("foo2"), true, revision{main: 8}, revision{}, 0},
-		{[]byte("foo"), true, revision{main: 9}, revision{}, 0},
-		{[]byte("foo"), false, revision{10, 0}, revision{10, 0}, 1},
-		{[]byte("foo1"), false, revision{10, 1}, revision{10, 1}, 1},
-	}
-
-	// Continuous Compact
-	ti := newTreeIndex()
-	for _, tt := range tests {
-		if tt.remove {
-			ti.Tombstone(tt.key, tt.rev)
-		} else {
-			ti.Put(tt.key, tt.rev)
-		}
-	}
-	for i := int64(1); i < maxRev; i++ {
-		am := ti.Compact(i)
-
-		wti := newTreeIndex()
-		for _, tt := range tests {
-			if _, ok := am[tt.rev]; ok || tt.rev.GreaterThan(revision{main: i}) {
-				if tt.remove {
-					wti.Tombstone(tt.key, tt.rev)
-				} else {
-					wti.Restore(tt.key, tt.created, tt.rev, tt.ver)
-				}
-			}
-		}
-		if !ti.Equal(wti) {
-			t.Errorf("#%d: not equal ti", i)
-		}
-	}
-
-	// Once Compact
-	for i := int64(1); i < maxRev; i++ {
-		ti := newTreeIndex()
-		for _, tt := range tests {
-			if tt.remove {
-				ti.Tombstone(tt.key, tt.rev)
-			} else {
-				ti.Put(tt.key, tt.rev)
-			}
-		}
-		am := ti.Compact(i)
-
-		wti := newTreeIndex()
-		for _, tt := range tests {
-			if _, ok := am[tt.rev]; ok || tt.rev.GreaterThan(revision{main: i}) {
-				if tt.remove {
-					wti.Tombstone(tt.key, tt.rev)
-				} else {
-					wti.Restore(tt.key, tt.created, tt.rev, tt.ver)
-				}
-			}
-		}
-		if !ti.Equal(wti) {
-			t.Errorf("#%d: not equal ti", i)
-		}
-	}
-}
-
-func TestIndexRestore(t *testing.T) {
-	key := []byte("foo")
-
-	tests := []struct {
-		created  revision
-		modified revision
-		ver      int64
-	}{
-		{revision{1, 0}, revision{1, 0}, 1},
-		{revision{1, 0}, revision{1, 1}, 2},
-		{revision{1, 0}, revision{2, 0}, 3},
-	}
-
-	// Continuous Restore
-	ti := newTreeIndex()
-	for i, tt := range tests {
-		ti.Restore(key, tt.created, tt.modified, tt.ver)
-
-		modified, created, ver, err := ti.Get(key, tt.modified.main)
-		if modified != tt.modified {
-			t.Errorf("#%d: modified = %v, want %v", i, modified, tt.modified)
-		}
-		if created != tt.created {
-			t.Errorf("#%d: created = %v, want %v", i, created, tt.created)
-		}
-		if ver != tt.ver {
-			t.Errorf("#%d: ver = %d, want %d", i, ver, tt.ver)
-		}
-		if err != nil {
-			t.Errorf("#%d: err = %v, want nil", i, err)
-		}
-	}
-
-	// Once Restore
-	for i, tt := range tests {
-		ti := newTreeIndex()
-		ti.Restore(key, tt.created, tt.modified, tt.ver)
-
-		modified, created, ver, err := ti.Get(key, tt.modified.main)
-		if modified != tt.modified {
-			t.Errorf("#%d: modified = %v, want %v", i, modified, tt.modified)
-		}
-		if created != tt.created {
-			t.Errorf("#%d: created = %v, want %v", i, created, tt.created)
-		}
-		if ver != tt.ver {
-			t.Errorf("#%d: ver = %d, want %d", i, ver, tt.ver)
-		}
-		if err != nil {
-			t.Errorf("#%d: err = %v, want nil", i, err)
-		}
-	}
-}
diff --git a/storage/key_index.go b/storage/key_index.go
deleted file mode 100644
index e8a344e..0000000
--- a/storage/key_index.go
+++ /dev/null
@@ -1,334 +0,0 @@
-// Copyright 2015 CoreOS, Inc.
-//
-// Licensed under the Apache License, Version 2.0 (the "License");
-// you may not use this file except in compliance with the License.
-// You may obtain a copy of the License at
-//
-//     http://www.apache.org/licenses/LICENSE-2.0
-//
-// Unless required by applicable law or agreed to in writing, software
-// distributed under the License is distributed on an "AS IS" BASIS,
-// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-// See the License for the specific language governing permissions and
-// limitations under the License.
-
-package storage
-
-import (
-	"bytes"
-	"errors"
-	"fmt"
-	"log"
-
-	"github.com/google/btree"
-)
-
-var (
-	ErrRevisionNotFound = errors.New("stroage: revision not found")
-)
-
-// keyIndex stores the revisions of a key in the backend.
-// Each keyIndex has at least one key generation.
-// Each generation might have several key versions.
-// Tombstone on a key appends an tombstone version at the end
-// of the current generation and creates a new empty generation.
-// Each version of a key has an index pointing to the backend.
-//
-// For example: put(1.0);put(2.0);tombstone(3.0);put(4.0);tombstone(5.0) on key "foo"
-// generate a keyIndex:
-// key:     "foo"
-// rev: 5
-// generations:
-//    {empty}
-//    {4.0, 5.0(t)}
-//    {1.0, 2.0, 3.0(t)}
-//
-// Compact a keyIndex removes the versions with smaller or equal to
-// rev except the largest one. If the generation becomes empty
-// during compaction, it will be removed. if all the generations get
-// removed, the keyIndex should be removed.
-
-// For example:
-// compact(2) on the previous example
-// generations:
-//    {empty}
-//    {4.0, 5.0(t)}
-//    {2.0, 3.0(t)}
-//
-// compact(4)
-// generations:
-//    {empty}
-//    {4.0, 5.0(t)}
-//
-// compact(5):
-// generations:
-//    {empty} -> key SHOULD be removed.
-//
-// compact(6):
-// generations:
-//    {empty} -> key SHOULD be removed.
-type keyIndex struct {
-	key         []byte
-	modified    revision // the main rev of the last modification
-	generations []generation
-}
-
-// put puts a revision to the keyIndex.
-func (ki *keyIndex) put(main int64, sub int64) {
-	rev := revision{main: main, sub: sub}
-
-	if !rev.GreaterThan(ki.modified) {
-		log.Panicf("store.keyindex: put with unexpected smaller revision [%v / %v]", rev, ki.modified)
-	}
-	if len(ki.generations) == 0 {
-		ki.generations = append(ki.generations, generation{})
-	}
-	g := &ki.generations[len(ki.generations)-1]
-	if len(g.revs) == 0 { // create a new key
-		keysGauge.Inc()
-		g.created = rev
-	}
-	g.revs = append(g.revs, rev)
-	g.ver++
-	ki.modified = rev
-}
-
-func (ki *keyIndex) restore(created, modified revision, ver int64) {
-	if len(ki.generations) != 0 {
-		log.Panicf("store.keyindex: cannot restore non-empty keyIndex")
-	}
-
-	ki.modified = modified
-	g := generation{created: created, ver: ver, revs: []revision{modified}}
-	ki.generations = append(ki.generations, g)
-	keysGauge.Inc()
-}
-
-// tombstone puts a revision, pointing to a tombstone, to the keyIndex.
-// It also creates a new empty generation in the keyIndex.
-// It returns ErrRevisionNotFound when tombstone on an empty generation.
-func (ki *keyIndex) tombstone(main int64, sub int64) error {
-	if ki.isEmpty() {
-		log.Panicf("store.keyindex: unexpected tombstone on empty keyIndex %s", string(ki.key))
-	}
-	if ki.generations[len(ki.generations)-1].isEmpty() {
-		return ErrRevisionNotFound
-	}
-	ki.put(main, sub)
-	ki.generations = append(ki.generations, generation{})
-	keysGauge.Dec()
-	return nil
-}
-
-// get gets the modified, created revision and version of the key that satisfies the given atRev.
-// Rev must be higher than or equal to the given atRev.
-func (ki *keyIndex) get(atRev int64) (modified, created revision, ver int64, err error) {
-	if ki.isEmpty() {
-		log.Panicf("store.keyindex: unexpected get on empty keyIndex %s", string(ki.key))
-	}
-	g := ki.findGeneration(atRev)
-	if g.isEmpty() {
-		return revision{}, revision{}, 0, ErrRevisionNotFound
-	}
-
-	n := g.walk(func(rev revision) bool { return rev.main > atRev })
-	if n != -1 {
-		return g.revs[n], g.created, g.ver - int64(len(g.revs)-n-1), nil
-	}
-
-	return revision{}, revision{}, 0, ErrRevisionNotFound
-}
-
-// since returns revisions since the given rev. Only the revision with the
-// largest sub revision will be returned if multiple revisions have the same
-// main revision.
-func (ki *keyIndex) since(rev int64) []revision {
-	if ki.isEmpty() {
-		log.Panicf("store.keyindex: unexpected get on empty keyIndex %s", string(ki.key))
-	}
-	since := revision{rev, 0}
-	var gi int
-	// find the generations to start checking
-	for gi = len(ki.generations) - 1; gi > 0; gi-- {
-		g := ki.generations[gi]
-		if g.isEmpty() {
-			continue
-		}
-		if since.GreaterThan(g.created) {
-			break
-		}
-	}
-
-	var revs []revision
-	var last int64
-	for ; gi < len(ki.generations); gi++ {
-		for _, r := range ki.generations[gi].revs {
-			if since.GreaterThan(r) {
-				continue
-			}
-			if r.main == last {
-				// replace the revision with a new one that has higher sub value,
-				// because the original one should not be seen by external
-				revs[len(revs)-1] = r
-				continue
-			}
-			revs = append(revs, r)
-			last = r.main
-		}
-	}
-	return revs
-}
-
-// compact compacts a keyIndex by removing the versions with smaller or equal
-// revision than the given atRev except the largest one (If the largest one is
-// a tombstone, it will not be kept).
-// If a generation becomes empty during compaction, it will be removed.
-func (ki *keyIndex) compact(atRev int64, available map[revision]struct{}) {
-	if ki.isEmpty() {
-		log.Panicf("store.keyindex: unexpected compact on empty keyIndex %s", string(ki.key))
-	}
-
-	// walk until reaching the first revision that has an revision smaller or equal to
-	// the atRev.
-	// add it to the available map
-	f := func(rev revision) bool {
-		if rev.main <= atRev {
-			available[rev] = struct{}{}
-			return false
-		}
-		return true
-	}
-
-	i, g := 0, &ki.generations[0]
-	// find first generation includes atRev or created after atRev
-	for i < len(ki.generations)-1 {
-		if tomb := g.revs[len(g.revs)-1].main; tomb > atRev {
-			break
-		}
-		i++
-		g = &ki.generations[i]
-	}
-
-	if !g.isEmpty() {
-		n := g.walk(f)
-		// remove the previous contents.
-		if n != -1 {
-			g.revs = g.revs[n:]
-		}
-		// remove any tombstone
-		if len(g.revs) == 1 && i != len(ki.generations)-1 {
-			delete(available, g.revs[0])
-			i++
-		}
-	}
-	// remove the previous generations.
-	ki.generations = ki.generations[i:]
-	return
-}
-
-func (ki *keyIndex) isEmpty() bool {
-	return len(ki.generations) == 1 && ki.generations[0].isEmpty()
-}
-
-// findGeneration finds out the generation of the keyIndex that the
-// given rev belongs to. If the given rev is at the gap of two generations,
-// which means that the key does not exist at the given rev, it returns nil.
-func (ki *keyIndex) findGeneration(rev int64) *generation {
-	lastg := len(ki.generations) - 1
-	cg := lastg
-
-	for cg >= 0 {
-		if len(ki.generations[cg].revs) == 0 {
-			cg--
-			continue
-		}
-		g := ki.generations[cg]
-		if cg != lastg {
-			if tomb := g.revs[len(g.revs)-1].main; tomb <= rev {
-				return nil
-			}
-		}
-		if g.revs[0].main <= rev {
-			return &ki.generations[cg]
-		}
-		cg--
-	}
-	return nil
-}
-
-func (a *keyIndex) Less(b btree.Item) bool {
-	return bytes.Compare(a.key, b.(*keyIndex).key) == -1
-}
-
-func (a *keyIndex) equal(b *keyIndex) bool {
-	if !bytes.Equal(a.key, b.key) {
-		return false
-	}
-	if a.modified != b.modified {
-		return false
-	}
-	if len(a.generations) != len(b.generations) {
-		return false
-	}
-	for i := range a.generations {
-		ag, bg := a.generations[i], b.generations[i]
-		if !ag.equal(bg) {
-			return false
-		}
-	}
-	return true
-}
-
-func (ki *keyIndex) String() string {
-	var s string
-	for _, g := range ki.generations {
-		s += g.String()
-	}
-	return s
-}
-
-// generation contains multiple revisions of a key.
-type generation struct {
-	ver     int64
-	created revision // when the generation is created (put in first revision).
-	revs    []revision
-}
-
-func (g *generation) isEmpty() bool { return g == nil || len(g.revs) == 0 }
-
-// walk walks through the revisions in the generation in descending order.
-// It passes the revision to the given function.
-// walk returns until: 1. it finishes walking all pairs 2. the function returns false.
-// walk returns the position at where it stopped. If it stopped after
-// finishing walking, -1 will be returned.
-func (g *generation) walk(f func(rev revision) bool) int {
-	l := len(g.revs)
-	for i := range g.revs {
-		ok := f(g.revs[l-i-1])
-		if !ok {
-			return l - i - 1
-		}
-	}
-	return -1
-}
-
-func (g *generation) String() string {
-	return fmt.Sprintf("g: created[%d] ver[%d], revs %#v\n", g.created, g.ver, g.revs)
-}
-
-func (a generation) equal(b generation) bool {
-	if a.ver != b.ver {
-		return false
-	}
-	if len(a.revs) != len(b.revs) {
-		return false
-	}
-
-	for i := range a.revs {
-		ar, br := a.revs[i], b.revs[i]
-		if ar != br {
-			return false
-		}
-	}
-	return true
-}
diff --git a/storage/key_index_test.go b/storage/key_index_test.go
deleted file mode 100644
index 01d9033..0000000
--- a/storage/key_index_test.go
+++ /dev/null
@@ -1,654 +0,0 @@
-// Copyright 2015 CoreOS, Inc.
-//
-// Licensed under the Apache License, Version 2.0 (the "License");
-// you may not use this file except in compliance with the License.
-// You may obtain a copy of the License at
-//
-//     http://www.apache.org/licenses/LICENSE-2.0
-//
-// Unless required by applicable law or agreed to in writing, software
-// distributed under the License is distributed on an "AS IS" BASIS,
-// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-// See the License for the specific language governing permissions and
-// limitations under the License.
-
-package storage
-
-import (
-	"reflect"
-	"testing"
-)
-
-func TestKeyIndexGet(t *testing.T) {
-	// key: "foo"
-	// rev: 16
-	// generations:
-	//    {empty}
-	//    {{14, 0}[1], {14, 1}[2], {16, 0}(t)[3]}
-	//    {{8, 0}[1], {10, 0}[2], {12, 0}(t)[3]}
-	//    {{2, 0}[1], {4, 0}[2], {6, 0}(t)[3]}
-	ki := newTestKeyIndex()
-	ki.compact(4, make(map[revision]struct{}))
-
-	tests := []struct {
-		rev int64
-
-		wmod   revision
-		wcreat revision
-		wver   int64
-		werr   error
-	}{
-		{17, revision{}, revision{}, 0, ErrRevisionNotFound},
-		{16, revision{}, revision{}, 0, ErrRevisionNotFound},
-
-		// get on generation 3
-		{15, revision{14, 1}, revision{14, 0}, 2, nil},
-		{14, revision{14, 1}, revision{14, 0}, 2, nil},
-
-		{13, revision{}, revision{}, 0, ErrRevisionNotFound},
-		{12, revision{}, revision{}, 0, ErrRevisionNotFound},
-
-		// get on generation 2
-		{11, revision{10, 0}, revision{8, 0}, 2, nil},
-		{10, revision{10, 0}, revision{8, 0}, 2, nil},
-		{9, revision{8, 0}, revision{8, 0}, 1, nil},
-		{8, revision{8, 0}, revision{8, 0}, 1, nil},
-
-		{7, revision{}, revision{}, 0, ErrRevisionNotFound},
-		{6, revision{}, revision{}, 0, ErrRevisionNotFound},
-
-		// get on generation 1
-		{5, revision{4, 0}, revision{2, 0}, 2, nil},
-		{4, revision{4, 0}, revision{2, 0}, 2, nil},
-
-		{3, revision{}, revision{}, 0, ErrRevisionNotFound},
-		{2, revision{}, revision{}, 0, ErrRevisionNotFound},
-		{1, revision{}, revision{}, 0, ErrRevisionNotFound},
-		{0, revision{}, revision{}, 0, ErrRevisionNotFound},
-	}
-
-	for i, tt := range tests {
-		mod, creat, ver, err := ki.get(tt.rev)
-		if err != tt.werr {
-			t.Errorf("#%d: err = %v, want %v", i, err, tt.werr)
-		}
-		if mod != tt.wmod {
-			t.Errorf("#%d: modified = %+v, want %+v", i, mod, tt.wmod)
-		}
-		if creat != tt.wcreat {
-			t.Errorf("#%d: created = %+v, want %+v", i, creat, tt.wcreat)
-		}
-		if ver != tt.wver {
-			t.Errorf("#%d: version = %d, want %d", i, ver, tt.wver)
-		}
-	}
-}
-
-func TestKeyIndexSince(t *testing.T) {
-	ki := newTestKeyIndex()
-	ki.compact(4, make(map[revision]struct{}))
-
-	allRevs := []revision{{4, 0}, {6, 0}, {8, 0}, {10, 0}, {12, 0}, {14, 1}, {16, 0}}
-	tests := []struct {
-		rev int64
-
-		wrevs []revision
-	}{
-		{17, nil},
-		{16, allRevs[6:]},
-		{15, allRevs[6:]},
-		{14, allRevs[5:]},
-		{13, allRevs[5:]},
-		{12, allRevs[4:]},
-		{11, allRevs[4:]},
-		{10, allRevs[3:]},
-		{9, allRevs[3:]},
-		{8, allRevs[2:]},
-		{7, allRevs[2:]},
-		{6, allRevs[1:]},
-		{5, allRevs[1:]},
-		{4, allRevs},
-		{3, allRevs},
-		{2, allRevs},
-		{1, allRevs},
-		{0, allRevs},
-	}
-
-	for i, tt := range tests {
-		revs := ki.since(tt.rev)
-		if !reflect.DeepEqual(revs, tt.wrevs) {
-			t.Errorf("#%d: revs = %+v, want %+v", i, revs, tt.wrevs)
-		}
-	}
-}
-
-func TestKeyIndexPut(t *testing.T) {
-	ki := &keyIndex{key: []byte("foo")}
-	ki.put(5, 0)
-
-	wki := &keyIndex{
-		key:         []byte("foo"),
-		modified:    revision{5, 0},
-		generations: []generation{{created: revision{5, 0}, ver: 1, revs: []revision{{main: 5}}}},
-	}
-	if !reflect.DeepEqual(ki, wki) {
-		t.Errorf("ki = %+v, want %+v", ki, wki)
-	}
-
-	ki.put(7, 0)
-
-	wki = &keyIndex{
-		key:         []byte("foo"),
-		modified:    revision{7, 0},
-		generations: []generation{{created: revision{5, 0}, ver: 2, revs: []revision{{main: 5}, {main: 7}}}},
-	}
-	if !reflect.DeepEqual(ki, wki) {
-		t.Errorf("ki = %+v, want %+v", ki, wki)
-	}
-}
-
-func TestKeyIndexRestore(t *testing.T) {
-	ki := &keyIndex{key: []byte("foo")}
-	ki.restore(revision{5, 0}, revision{7, 0}, 2)
-
-	wki := &keyIndex{
-		key:         []byte("foo"),
-		modified:    revision{7, 0},
-		generations: []generation{{created: revision{5, 0}, ver: 2, revs: []revision{{main: 7}}}},
-	}
-	if !reflect.DeepEqual(ki, wki) {
-		t.Errorf("ki = %+v, want %+v", ki, wki)
-	}
-}
-
-func TestKeyIndexTombstone(t *testing.T) {
-	ki := &keyIndex{key: []byte("foo")}
-	ki.put(5, 0)
-
-	err := ki.tombstone(7, 0)
-	if err != nil {
-		t.Errorf("unexpected tombstone error: %v", err)
-	}
-
-	wki := &keyIndex{
-		key:         []byte("foo"),
-		modified:    revision{7, 0},
-		generations: []generation{{created: revision{5, 0}, ver: 2, revs: []revision{{main: 5}, {main: 7}}}, {}},
-	}
-	if !reflect.DeepEqual(ki, wki) {
-		t.Errorf("ki = %+v, want %+v", ki, wki)
-	}
-
-	ki.put(8, 0)
-	ki.put(9, 0)
-	err = ki.tombstone(15, 0)
-	if err != nil {
-		t.Errorf("unexpected tombstone error: %v", err)
-	}
-
-	wki = &keyIndex{
-		key:      []byte("foo"),
-		modified: revision{15, 0},
-		generations: []generation{
-			{created: revision{5, 0}, ver: 2, revs: []revision{{main: 5}, {main: 7}}},
-			{created: revision{8, 0}, ver: 3, revs: []revision{{main: 8}, {main: 9}, {main: 15}}},
-			{},
-		},
-	}
-	if !reflect.DeepEqual(ki, wki) {
-		t.Errorf("ki = %+v, want %+v", ki, wki)
-	}
-
-	err = ki.tombstone(16, 0)
-	if err != ErrRevisionNotFound {
-		t.Errorf("tombstone error = %v, want %v", err, ErrRevisionNotFound)
-	}
-}
-
-func TestKeyIndexCompact(t *testing.T) {
-	tests := []struct {
-		compact int64
-
-		wki *keyIndex
-		wam map[revision]struct{}
-	}{
-		{
-			1,
-			&keyIndex{
-				key:      []byte("foo"),
-				modified: revision{16, 0},
-				generations: []generation{
-					{created: revision{2, 0}, ver: 3, revs: []revision{{main: 2}, {main: 4}, {main: 6}}},
-					{created: revision{8, 0}, ver: 3, revs: []revision{{main: 8}, {main: 10}, {main: 12}}},
-					{created: revision{14, 0}, ver: 3, revs: []revision{{main: 14}, {main: 14, sub: 1}, {main: 16}}},
-					{},
-				},
-			},
-			map[revision]struct{}{},
-		},
-		{
-			2,
-			&keyIndex{
-				key:      []byte("foo"),
-				modified: revision{16, 0},
-				generations: []generation{
-					{created: revision{2, 0}, ver: 3, revs: []revision{{main: 2}, {main: 4}, {main: 6}}},
-					{created: revision{8, 0}, ver: 3, revs: []revision{{main: 8}, {main: 10}, {main: 12}}},
-					{created: revision{14, 0}, ver: 3, revs: []revision{{main: 14}, {main: 14, sub: 1}, {main: 16}}},
-					{},
-				},
-			},
-			map[revision]struct{}{
-				revision{main: 2}: {},
-			},
-		},
-		{
-			3,
-			&keyIndex{
-				key:      []byte("foo"),
-				modified: revision{16, 0},
-				generations: []generation{
-					{created: revision{2, 0}, ver: 3, revs: []revision{{main: 2}, {main: 4}, {main: 6}}},
-					{created: revision{8, 0}, ver: 3, revs: []revision{{main: 8}, {main: 10}, {main: 12}}},
-					{created: revision{14, 0}, ver: 3, revs: []revision{{main: 14}, {main: 14, sub: 1}, {main: 16}}},
-					{},
-				},
-			},
-			map[revision]struct{}{
-				revision{main: 2}: {},
-			},
-		},
-		{
-			4,
-			&keyIndex{
-				key:      []byte("foo"),
-				modified: revision{16, 0},
-				generations: []generation{
-					{created: revision{2, 0}, ver: 3, revs: []revision{{main: 4}, {main: 6}}},
-					{created: revision{8, 0}, ver: 3, revs: []revision{{main: 8}, {main: 10}, {main: 12}}},
-					{created: revision{14, 0}, ver: 3, revs: []revision{{main: 14}, {main: 14, sub: 1}, {main: 16}}},
-					{},
-				},
-			},
-			map[revision]struct{}{
-				revision{main: 4}: {},
-			},
-		},
-		{
-			5,
-			&keyIndex{
-				key:      []byte("foo"),
-				modified: revision{16, 0},
-				generations: []generation{
-					{created: revision{2, 0}, ver: 3, revs: []revision{{main: 4}, {main: 6}}},
-					{created: revision{8, 0}, ver: 3, revs: []revision{{main: 8}, {main: 10}, {main: 12}}},
-					{created: revision{14, 0}, ver: 3, revs: []revision{{main: 14}, {main: 14, sub: 1}, {main: 16}}},
-					{},
-				},
-			},
-			map[revision]struct{}{
-				revision{main: 4}: {},
-			},
-		},
-		{
-			6,
-			&keyIndex{
-				key:      []byte("foo"),
-				modified: revision{16, 0},
-				generations: []generation{
-					{created: revision{8, 0}, ver: 3, revs: []revision{{main: 8}, {main: 10}, {main: 12}}},
-					{created: revision{14, 0}, ver: 3, revs: []revision{{main: 14}, {main: 14, sub: 1}, {main: 16}}},
-					{},
-				},
-			},
-			map[revision]struct{}{},
-		},
-		{
-			7,
-			&keyIndex{
-				key:      []byte("foo"),
-				modified: revision{16, 0},
-				generations: []generation{
-					{created: revision{8, 0}, ver: 3, revs: []revision{{main: 8}, {main: 10}, {main: 12}}},
-					{created: revision{14, 0}, ver: 3, revs: []revision{{main: 14}, {main: 14, sub: 1}, {main: 16}}},
-					{},
-				},
-			},
-			map[revision]struct{}{},
-		},
-		{
-			8,
-			&keyIndex{
-				key:      []byte("foo"),
-				modified: revision{16, 0},
-				generations: []generation{
-					{created: revision{8, 0}, ver: 3, revs: []revision{{main: 8}, {main: 10}, {main: 12}}},
-					{created: revision{14, 0}, ver: 3, revs: []revision{{main: 14}, {main: 14, sub: 1}, {main: 16}}},
-					{},
-				},
-			},
-			map[revision]struct{}{
-				revision{main: 8}: {},
-			},
-		},
-		{
-			9,
-			&keyIndex{
-				key:      []byte("foo"),
-				modified: revision{16, 0},
-				generations: []generation{
-					{created: revision{8, 0}, ver: 3, revs: []revision{{main: 8}, {main: 10}, {main: 12}}},
-					{created: revision{14, 0}, ver: 3, revs: []revision{{main: 14}, {main: 14, sub: 1}, {main: 16}}},
-					{},
-				},
-			},
-			map[revision]struct{}{
-				revision{main: 8}: {},
-			},
-		},
-		{
-			10,
-			&keyIndex{
-				key:      []byte("foo"),
-				modified: revision{16, 0},
-				generations: []generation{
-					{created: revision{8, 0}, ver: 3, revs: []revision{{main: 10}, {main: 12}}},
-					{created: revision{14, 0}, ver: 3, revs: []revision{{main: 14}, {main: 14, sub: 1}, {main: 16}}},
-					{},
-				},
-			},
-			map[revision]struct{}{
-				revision{main: 10}: {},
-			},
-		},
-		{
-			11,
-			&keyIndex{
-				key:      []byte("foo"),
-				modified: revision{16, 0},
-				generations: []generation{
-					{created: revision{8, 0}, ver: 3, revs: []revision{{main: 10}, {main: 12}}},
-					{created: revision{14, 0}, ver: 3, revs: []revision{{main: 14}, {main: 14, sub: 1}, {main: 16}}},
-					{},
-				},
-			},
-			map[revision]struct{}{
-				revision{main: 10}: {},
-			},
-		},
-		{
-			12,
-			&keyIndex{
-				key:      []byte("foo"),
-				modified: revision{16, 0},
-				generations: []generation{
-					{created: revision{14, 0}, ver: 3, revs: []revision{{main: 14}, {main: 14, sub: 1}, {main: 16}}},
-					{},
-				},
-			},
-			map[revision]struct{}{},
-		},
-		{
-			13,
-			&keyIndex{
-				key:      []byte("foo"),
-				modified: revision{16, 0},
-				generations: []generation{
-					{created: revision{14, 0}, ver: 3, revs: []revision{{main: 14}, {main: 14, sub: 1}, {main: 16}}},
-					{},
-				},
-			},
-			map[revision]struct{}{},
-		},
-		{
-			14,
-			&keyIndex{
-				key:      []byte("foo"),
-				modified: revision{16, 0},
-				generations: []generation{
-					{created: revision{14, 0}, ver: 3, revs: []revision{{main: 14, sub: 1}, {main: 16}}},
-					{},
-				},
-			},
-			map[revision]struct{}{
-				revision{main: 14, sub: 1}: {},
-			},
-		},
-		{
-			15,
-			&keyIndex{
-				key:      []byte("foo"),
-				modified: revision{16, 0},
-				generations: []generation{
-					{created: revision{14, 0}, ver: 3, revs: []revision{{main: 14, sub: 1}, {main: 16}}},
-					{},
-				},
-			},
-			map[revision]struct{}{
-				revision{main: 14, sub: 1}: {},
-			},
-		},
-		{
-			16,
-			&keyIndex{
-				key:      []byte("foo"),
-				modified: revision{16, 0},
-				generations: []generation{
-					{},
-				},
-			},
-			map[revision]struct{}{},
-		},
-	}
-
-	// Continuous Compaction
-	ki := newTestKeyIndex()
-	for i, tt := range tests {
-		am := make(map[revision]struct{})
-		ki.compact(tt.compact, am)
-		if !reflect.DeepEqual(ki, tt.wki) {
-			t.Errorf("#%d: ki = %+v, want %+v", i, ki, tt.wki)
-		}
-		if !reflect.DeepEqual(am, tt.wam) {
-			t.Errorf("#%d: am = %+v, want %+v", i, am, tt.wam)
-		}
-	}
-
-	// Jump Compaction
-	ki = newTestKeyIndex()
-	for i, tt := range tests {
-		if (i%2 == 0 && i < 6) || (i%2 == 1 && i > 6) {
-			am := make(map[revision]struct{})
-			ki.compact(tt.compact, am)
-			if !reflect.DeepEqual(ki, tt.wki) {
-				t.Errorf("#%d: ki = %+v, want %+v", i, ki, tt.wki)
-			}
-			if !reflect.DeepEqual(am, tt.wam) {
-				t.Errorf("#%d: am = %+v, want %+v", i, am, tt.wam)
-			}
-		}
-	}
-
-	// Once Compaction
-	for i, tt := range tests {
-		ki := newTestKeyIndex()
-		am := make(map[revision]struct{})
-		ki.compact(tt.compact, am)
-		if !reflect.DeepEqual(ki, tt.wki) {
-			t.Errorf("#%d: ki = %+v, want %+v", i, ki, tt.wki)
-		}
-		if !reflect.DeepEqual(am, tt.wam) {
-			t.Errorf("#%d: am = %+v, want %+v", i, am, tt.wam)
-		}
-	}
-}
-
-// test that compact on version that higher than last modified version works well
-func TestKeyIndexCompactOnFurtherRev(t *testing.T) {
-	ki := &keyIndex{key: []byte("foo")}
-	ki.put(1, 0)
-	ki.put(2, 0)
-	am := make(map[revision]struct{})
-	ki.compact(3, am)
-
-	wki := &keyIndex{
-		key:      []byte("foo"),
-		modified: revision{2, 0},
-		generations: []generation{
-			{created: revision{1, 0}, ver: 2, revs: []revision{{main: 2}}},
-		},
-	}
-	wam := map[revision]struct{}{
-		revision{main: 2}: {},
-	}
-	if !reflect.DeepEqual(ki, wki) {
-		t.Errorf("ki = %+v, want %+v", ki, wki)
-	}
-	if !reflect.DeepEqual(am, wam) {
-		t.Errorf("am = %+v, want %+v", am, wam)
-	}
-}
-
-func TestKeyIndexIsEmpty(t *testing.T) {
-	tests := []struct {
-		ki *keyIndex
-		w  bool
-	}{
-		{
-			&keyIndex{
-				key:         []byte("foo"),
-				generations: []generation{{}},
-			},
-			true,
-		},
-		{
-			&keyIndex{
-				key:      []byte("foo"),
-				modified: revision{2, 0},
-				generations: []generation{
-					{created: revision{1, 0}, ver: 2, revs: []revision{{main: 2}}},
-				},
-			},
-			false,
-		},
-	}
-	for i, tt := range tests {
-		g := tt.ki.isEmpty()
-		if g != tt.w {
-			t.Errorf("#%d: isEmpty = %v, want %v", i, g, tt.w)
-		}
-	}
-}
-
-func TestKeyIndexFindGeneration(t *testing.T) {
-	ki := newTestKeyIndex()
-
-	tests := []struct {
-		rev int64
-		wg  *generation
-	}{
-		{0, nil},
-		{1, nil},
-		{2, &ki.generations[0]},
-		{3, &ki.generations[0]},
-		{4, &ki.generations[0]},
-		{5, &ki.generations[0]},
-		{6, nil},
-		{7, nil},
-		{8, &ki.generations[1]},
-		{9, &ki.generations[1]},
-		{10, &ki.generations[1]},
-		{11, &ki.generations[1]},
-		{12, nil},
-		{13, nil},
-	}
-	for i, tt := range tests {
-		g := ki.findGeneration(tt.rev)
-		if g != tt.wg {
-			t.Errorf("#%d: generation = %+v, want %+v", i, g, tt.wg)
-		}
-	}
-}
-
-func TestKeyIndexLess(t *testing.T) {
-	ki := &keyIndex{key: []byte("foo")}
-
-	tests := []struct {
-		ki *keyIndex
-		w  bool
-	}{
-		{&keyIndex{key: []byte("doo")}, false},
-		{&keyIndex{key: []byte("foo")}, false},
-		{&keyIndex{key: []byte("goo")}, true},
-	}
-	for i, tt := range tests {
-		g := ki.Less(tt.ki)
-		if g != tt.w {
-			t.Errorf("#%d: Less = %v, want %v", i, g, tt.w)
-		}
-	}
-}
-
-func TestGenerationIsEmpty(t *testing.T) {
-	tests := []struct {
-		g *generation
-		w bool
-	}{
-		{nil, true},
-		{&generation{}, true},
-		{&generation{revs: []revision{{main: 1}}}, false},
-	}
-	for i, tt := range tests {
-		g := tt.g.isEmpty()
-		if g != tt.w {
-			t.Errorf("#%d: isEmpty = %v, want %v", i, g, tt.w)
-		}
-	}
-}
-
-func TestGenerationWalk(t *testing.T) {
-	g := &generation{
-		ver:     3,
-		created: revision{2, 0},
-		revs:    []revision{{main: 2}, {main: 4}, {main: 6}},
-	}
-	tests := []struct {
-		f  func(rev revision) bool
-		wi int
-	}{
-		{func(rev revision) bool { return rev.main >= 7 }, 2},
-		{func(rev revision) bool { return rev.main >= 6 }, 1},
-		{func(rev revision) bool { return rev.main >= 5 }, 1},
-		{func(rev revision) bool { return rev.main >= 4 }, 0},
-		{func(rev revision) bool { return rev.main >= 3 }, 0},
-		{func(rev revision) bool { return rev.main >= 2 }, -1},
-	}
-	for i, tt := range tests {
-		idx := g.walk(tt.f)
-		if idx != tt.wi {
-			t.Errorf("#%d: index = %d, want %d", i, idx, tt.wi)
-		}
-	}
-}
-
-func newTestKeyIndex() *keyIndex {
-	// key: "foo"
-	// rev: 16
-	// generations:
-	//    {empty}
-	//    {{14, 0}[1], {14, 1}[2], {16, 0}(t)[3]}
-	//    {{8, 0}[1], {10, 0}[2], {12, 0}(t)[3]}
-	//    {{2, 0}[1], {4, 0}[2], {6, 0}(t)[3]}
-
-	ki := &keyIndex{key: []byte("foo")}
-	ki.put(2, 0)
-	ki.put(4, 0)
-	ki.tombstone(6, 0)
-	ki.put(8, 0)
-	ki.put(10, 0)
-	ki.tombstone(12, 0)
-	ki.put(14, 0)
-	ki.put(14, 1)
-	ki.tombstone(16, 0)
-	return ki
-}
diff --git a/storage/kv.go b/storage/kv.go
deleted file mode 100644
index 0fb2377..0000000
--- a/storage/kv.go
+++ /dev/null
@@ -1,107 +0,0 @@
-// Copyright 2015 CoreOS, Inc.
-//
-// Licensed under the Apache License, Version 2.0 (the "License");
-// you may not use this file except in compliance with the License.
-// You may obtain a copy of the License at
-//
-//     http://www.apache.org/licenses/LICENSE-2.0
-//
-// Unless required by applicable law or agreed to in writing, software
-// distributed under the License is distributed on an "AS IS" BASIS,
-// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-// See the License for the specific language governing permissions and
-// limitations under the License.
-
-package storage
-
-import (
-	"github.com/coreos/etcd/lease"
-	"github.com/coreos/etcd/storage/backend"
-	"github.com/coreos/etcd/storage/storagepb"
-)
-
-type KV interface {
-	// Rev returns the current revision of the KV.
-	Rev() int64
-
-	// FirstRev returns the first revision of the KV.
-	// After a compaction, the first revision increases to the compaction
-	// revision.
-	FirstRev() int64
-
-	// Range gets the keys in the range at rangeRev.
-	// The returned rev is the current revision of the KV when the operation is executed.
-	// If rangeRev <=0, range gets the keys at currentRev.
-	// If `end` is nil, the request returns the key.
-	// If `end` is not nil and not empty, it gets the keys in range [key, range_end).
-	// If `end` is not nil and empty, it gets the keys greater than or equal to key.
-	// Limit limits the number of keys returned.
-	// If the required rev is compacted, ErrCompacted will be returned.
-	Range(key, end []byte, limit, rangeRev int64) (kvs []storagepb.KeyValue, rev int64, err error)
-
-	// Put puts the given key, value into the store. Put also takes additional argument lease to
-	// attach a lease to a key-value pair as meta-data. KV implementation does not validate the lease
-	// id.
-	// A put also increases the rev of the store, and generates one event in the event history.
-	// The returned rev is the current revision of the KV when the operation is executed.
-	Put(key, value []byte, lease lease.LeaseID) (rev int64)
-
-	// DeleteRange deletes the given range from the store.
-	// A deleteRange increases the rev of the store if any key in the range exists.
-	// The number of key deleted will be returned.
-	// The returned rev is the current revision of the KV when the operation is executed.
-	// It also generates one event for each key delete in the event history.
-	// if the `end` is nil, deleteRange deletes the key.
-	// if the `end` is not nil, deleteRange deletes the keys in range [key, range_end).
-	DeleteRange(key, end []byte) (n, rev int64)
-
-	// TxnBegin begins a txn. Only Txn prefixed operation can be executed, others will be blocked
-	// until txn ends. Only one on-going txn is allowed.
-	// TxnBegin returns an int64 txn ID.
-	// All txn prefixed operations with same txn ID will be done with the same rev.
-	TxnBegin() int64
-	// TxnEnd ends the on-going txn with txn ID. If the on-going txn ID is not matched, error is returned.
-	TxnEnd(txnID int64) error
-	// TxnRange returns the current revision of the KV when the operation is executed.
-	TxnRange(txnID int64, key, end []byte, limit, rangeRev int64) (kvs []storagepb.KeyValue, rev int64, err error)
-	TxnPut(txnID int64, key, value []byte, lease lease.LeaseID) (rev int64, err error)
-	TxnDeleteRange(txnID int64, key, end []byte) (n, rev int64, err error)
-
-	// Compact frees all superseded keys with revisions less than rev.
-	Compact(rev int64) (<-chan struct{}, error)
-
-	// Hash retrieves the hash of KV state.
-	// This method is designed for consistency checking purpose.
-	Hash() (uint32, error)
-
-	// Commit commits txns into the underlying backend.
-	Commit()
-
-	// Restore restores the KV store from a backend.
-	Restore(b backend.Backend) error
-	Close() error
-}
-
-// WatchableKV is a KV that can be watched.
-type WatchableKV interface {
-	KV
-	Watchable
-}
-
-// Watchable is the interface that wraps the NewWatchStream function.
-type Watchable interface {
-	// NewWatchStream returns a WatchStream that can be used to
-	// watch events happened or happening on the KV.
-	NewWatchStream() WatchStream
-}
-
-// ConsistentWatchableKV is a WatchableKV that understands the consistency
-// algorithm and consistent index.
-// If the consistent index of executing entry is not larger than the
-// consistent index of ConsistentWatchableKV, all operations in
-// this entry are skipped and return empty response.
-type ConsistentWatchableKV interface {
-	WatchableKV
-	// ConsistentIndex returns the current consistent index of the KV.
-	ConsistentIndex() uint64
-}
diff --git a/storage/kv_test.go b/storage/kv_test.go
deleted file mode 100644
index 0d67972..0000000
--- a/storage/kv_test.go
+++ /dev/null
@@ -1,839 +0,0 @@
-// Copyright 2015 CoreOS, Inc.
-//
-// Licensed under the Apache License, Version 2.0 (the "License");
-// you may not use this file except in compliance with the License.
-// You may obtain a copy of the License at
-//
-//     http://www.apache.org/licenses/LICENSE-2.0
-//
-// Unless required by applicable law or agreed to in writing, software
-// distributed under the License is distributed on an "AS IS" BASIS,
-// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-// See the License for the specific language governing permissions and
-// limitations under the License.
-
-package storage
-
-import (
-	"fmt"
-	"os"
-	"reflect"
-	"testing"
-	"time"
-
-	"github.com/coreos/etcd/lease"
-	"github.com/coreos/etcd/pkg/testutil"
-	"github.com/coreos/etcd/storage/backend"
-	"github.com/coreos/etcd/storage/storagepb"
-)
-
-// Functional tests for features implemented in v3 store. It treats v3 store
-// as a black box, and tests it by feeding the input and validating the output.
-
-// TODO: add similar tests on operations in one txn/rev
-
-type (
-	rangeFunc       func(kv KV, key, end []byte, limit, rangeRev int64) ([]storagepb.KeyValue, int64, error)
-	putFunc         func(kv KV, key, value []byte, lease lease.LeaseID) int64
-	deleteRangeFunc func(kv KV, key, end []byte) (n, rev int64)
-)
-
-var (
-	normalRangeFunc = func(kv KV, key, end []byte, limit, rangeRev int64) ([]storagepb.KeyValue, int64, error) {
-		return kv.Range(key, end, limit, rangeRev)
-	}
-	txnRangeFunc = func(kv KV, key, end []byte, limit, rangeRev int64) ([]storagepb.KeyValue, int64, error) {
-		id := kv.TxnBegin()
-		defer kv.TxnEnd(id)
-		return kv.TxnRange(id, key, end, limit, rangeRev)
-	}
-
-	normalPutFunc = func(kv KV, key, value []byte, lease lease.LeaseID) int64 {
-		return kv.Put(key, value, lease)
-	}
-	txnPutFunc = func(kv KV, key, value []byte, lease lease.LeaseID) int64 {
-		id := kv.TxnBegin()
-		defer kv.TxnEnd(id)
-		rev, err := kv.TxnPut(id, key, value, lease)
-		if err != nil {
-			panic("txn put error")
-		}
-		return rev
-	}
-
-	normalDeleteRangeFunc = func(kv KV, key, end []byte) (n, rev int64) {
-		return kv.DeleteRange(key, end)
-	}
-	txnDeleteRangeFunc = func(kv KV, key, end []byte) (n, rev int64) {
-		id := kv.TxnBegin()
-		defer kv.TxnEnd(id)
-		n, rev, err := kv.TxnDeleteRange(id, key, end)
-		if err != nil {
-			panic("txn delete error")
-		}
-		return n, rev
-	}
-)
-
-func TestKVRange(t *testing.T)    { testKVRange(t, normalRangeFunc) }
-func TestKVTxnRange(t *testing.T) { testKVRange(t, txnRangeFunc) }
-
-func testKVRange(t *testing.T, f rangeFunc) {
-	b, tmpPath := backend.NewDefaultTmpBackend()
-	s := NewStore(b, &lease.FakeLessor{}, nil)
-	defer cleanup(s, b, tmpPath)
-
-	kvs := put3TestKVs(s)
-
-	wrev := int64(4)
-	tests := []struct {
-		key, end []byte
-		wkvs     []storagepb.KeyValue
-	}{
-		// get no keys
-		{
-			[]byte("doo"), []byte("foo"),
-			nil,
-		},
-		// get no keys when key == end
-		{
-			[]byte("foo"), []byte("foo"),
-			nil,
-		},
-		// get no keys when ranging single key
-		{
-			[]byte("doo"), nil,
-			nil,
-		},
-		// get all keys
-		{
-			[]byte("foo"), []byte("foo3"),
-			kvs,
-		},
-		// get partial keys
-		{
-			[]byte("foo"), []byte("foo1"),
-			kvs[:1],
-		},
-		// get single key
-		{
-			[]byte("foo"), nil,
-			kvs[:1],
-		},
-		// get entire keyspace
-		{
-			[]byte(""), []byte(""),
-			kvs,
-		},
-	}
-
-	for i, tt := range tests {
-		kvs, rev, err := f(s, tt.key, tt.end, 0, 0)
-		if err != nil {
-			t.Fatal(err)
-		}
-		if rev != wrev {
-			t.Errorf("#%d: rev = %d, want %d", i, rev, wrev)
-		}
-		if !reflect.DeepEqual(kvs, tt.wkvs) {
-			t.Errorf("#%d: kvs = %+v, want %+v", i, kvs, tt.wkvs)
-		}
-	}
-}
-
-func TestKVRangeRev(t *testing.T)    { testKVRangeRev(t, normalRangeFunc) }
-func TestKVTxnRangeRev(t *testing.T) { testKVRangeRev(t, normalRangeFunc) }
-
-func testKVRangeRev(t *testing.T, f rangeFunc) {
-	b, tmpPath := backend.NewDefaultTmpBackend()
-	s := NewStore(b, &lease.FakeLessor{}, nil)
-	defer cleanup(s, b, tmpPath)
-
-	kvs := put3TestKVs(s)
-
-	tests := []struct {
-		rev  int64
-		wrev int64
-		wkvs []storagepb.KeyValue
-	}{
-		{-1, 4, kvs},
-		{0, 4, kvs},
-		{2, 4, kvs[:1]},
-		{3, 4, kvs[:2]},
-		{4, 4, kvs},
-	}
-
-	for i, tt := range tests {
-		kvs, rev, err := f(s, []byte("foo"), []byte("foo3"), 0, tt.rev)
-		if err != nil {
-			t.Fatal(err)
-		}
-		if rev != tt.wrev {
-			t.Errorf("#%d: rev = %d, want %d", i, rev, tt.wrev)
-		}
-		if !reflect.DeepEqual(kvs, tt.wkvs) {
-			t.Errorf("#%d: kvs = %+v, want %+v", i, kvs, tt.wkvs)
-		}
-	}
-}
-
-func TestKVRangeBadRev(t *testing.T)    { testKVRangeBadRev(t, normalRangeFunc) }
-func TestKVTxnRangeBadRev(t *testing.T) { testKVRangeBadRev(t, normalRangeFunc) }
-
-func testKVRangeBadRev(t *testing.T, f rangeFunc) {
-	b, tmpPath := backend.NewDefaultTmpBackend()
-	s := NewStore(b, &lease.FakeLessor{}, nil)
-	defer cleanup(s, b, tmpPath)
-
-	put3TestKVs(s)
-	if _, err := s.Compact(4); err != nil {
-		t.Fatalf("compact error (%v)", err)
-	}
-
-	tests := []struct {
-		rev  int64
-		werr error
-	}{
-		{-1, nil}, // <= 0 is most recent store
-		{0, nil},
-		{1, ErrCompacted},
-		{2, ErrCompacted},
-		{4, nil},
-		{5, ErrFutureRev},
-		{100, ErrFutureRev},
-	}
-	for i, tt := range tests {
-		_, _, err := f(s, []byte("foo"), []byte("foo3"), 0, tt.rev)
-		if err != tt.werr {
-			t.Errorf("#%d: error = %v, want %v", i, err, tt.werr)
-		}
-	}
-}
-
-func TestKVRangeLimit(t *testing.T)    { testKVRangeLimit(t, normalRangeFunc) }
-func TestKVTxnRangeLimit(t *testing.T) { testKVRangeLimit(t, txnRangeFunc) }
-
-func testKVRangeLimit(t *testing.T, f rangeFunc) {
-	b, tmpPath := backend.NewDefaultTmpBackend()
-	s := NewStore(b, &lease.FakeLessor{}, nil)
-	defer cleanup(s, b, tmpPath)
-
-	kvs := put3TestKVs(s)
-
-	wrev := int64(4)
-	tests := []struct {
-		limit int64
-		wkvs  []storagepb.KeyValue
-	}{
-		// no limit
-		{-1, kvs},
-		// no limit
-		{0, kvs},
-		{1, kvs[:1]},
-		{2, kvs[:2]},
-		{3, kvs},
-		{100, kvs},
-	}
-	for i, tt := range tests {
-		kvs, rev, err := f(s, []byte("foo"), []byte("foo3"), tt.limit, 0)
-		if err != nil {
-			t.Fatalf("#%d: range error (%v)", i, err)
-		}
-		if !reflect.DeepEqual(kvs, tt.wkvs) {
-			t.Errorf("#%d: kvs = %+v, want %+v", i, kvs, tt.wkvs)
-		}
-		if rev != wrev {
-			t.Errorf("#%d: rev = %d, want %d", i, rev, wrev)
-		}
-	}
-}
-
-func TestKVPutMultipleTimes(t *testing.T)    { testKVPutMultipleTimes(t, normalPutFunc) }
-func TestKVTxnPutMultipleTimes(t *testing.T) { testKVPutMultipleTimes(t, txnPutFunc) }
-
-func testKVPutMultipleTimes(t *testing.T, f putFunc) {
-	b, tmpPath := backend.NewDefaultTmpBackend()
-	s := NewStore(b, &lease.FakeLessor{}, nil)
-	defer cleanup(s, b, tmpPath)
-
-	for i := 0; i < 10; i++ {
-		base := int64(i + 1)
-
-		rev := f(s, []byte("foo"), []byte("bar"), lease.LeaseID(base))
-		if rev != base+1 {
-			t.Errorf("#%d: rev = %d, want %d", i, rev, base+1)
-		}
-
-		kvs, _, err := s.Range([]byte("foo"), nil, 0, 0)
-		if err != nil {
-			t.Fatal(err)
-		}
-		wkvs := []storagepb.KeyValue{
-			{Key: []byte("foo"), Value: []byte("bar"), CreateRevision: 2, ModRevision: base + 1, Version: base, Lease: base},
-		}
-		if !reflect.DeepEqual(kvs, wkvs) {
-			t.Errorf("#%d: kvs = %+v, want %+v", i, kvs, wkvs)
-		}
-	}
-}
-
-func TestKVDeleteRange(t *testing.T)    { testKVDeleteRange(t, normalDeleteRangeFunc) }
-func TestKVTxnDeleteRange(t *testing.T) { testKVDeleteRange(t, txnDeleteRangeFunc) }
-
-func testKVDeleteRange(t *testing.T, f deleteRangeFunc) {
-	tests := []struct {
-		key, end []byte
-
-		wrev int64
-		wN   int64
-	}{
-		{
-			[]byte("foo"), nil,
-			5, 1,
-		},
-		{
-			[]byte("foo"), []byte("foo1"),
-			5, 1,
-		},
-		{
-			[]byte("foo"), []byte("foo2"),
-			5, 2,
-		},
-		{
-			[]byte("foo"), []byte("foo3"),
-			5, 3,
-		},
-		{
-			[]byte("foo3"), []byte("foo8"),
-			4, 0,
-		},
-		{
-			[]byte("foo3"), nil,
-			4, 0,
-		},
-	}
-
-	for i, tt := range tests {
-		b, tmpPath := backend.NewDefaultTmpBackend()
-		s := NewStore(b, &lease.FakeLessor{}, nil)
-
-		s.Put([]byte("foo"), []byte("bar"), lease.NoLease)
-		s.Put([]byte("foo1"), []byte("bar1"), lease.NoLease)
-		s.Put([]byte("foo2"), []byte("bar2"), lease.NoLease)
-
-		n, rev := f(s, tt.key, tt.end)
-		if n != tt.wN || rev != tt.wrev {
-			t.Errorf("#%d: n = %d, rev = %d, want (%d, %d)", i, n, rev, tt.wN, tt.wrev)
-		}
-
-		cleanup(s, b, tmpPath)
-	}
-}
-
-func TestKVDeleteMultipleTimes(t *testing.T)    { testKVDeleteMultipleTimes(t, normalDeleteRangeFunc) }
-func TestKVTxnDeleteMultipleTimes(t *testing.T) { testKVDeleteMultipleTimes(t, txnDeleteRangeFunc) }
-
-func testKVDeleteMultipleTimes(t *testing.T, f deleteRangeFunc) {
-	b, tmpPath := backend.NewDefaultTmpBackend()
-	s := NewStore(b, &lease.FakeLessor{}, nil)
-	defer cleanup(s, b, tmpPath)
-
-	s.Put([]byte("foo"), []byte("bar"), lease.NoLease)
-
-	n, rev := f(s, []byte("foo"), nil)
-	if n != 1 || rev != 3 {
-		t.Fatalf("n = %d, rev = %d, want (%d, %d)", n, rev, 1, 3)
-	}
-
-	for i := 0; i < 10; i++ {
-		n, rev := f(s, []byte("foo"), nil)
-		if n != 0 || rev != 3 {
-			t.Fatalf("#%d: n = %d, rev = %d, want (%d, %d)", i, n, rev, 0, 3)
-		}
-	}
-}
-
-// test that range, put, delete on single key in sequence repeatedly works correctly.
-func TestKVOperationInSequence(t *testing.T) {
-	b, tmpPath := backend.NewDefaultTmpBackend()
-	s := NewStore(b, &lease.FakeLessor{}, nil)
-	defer cleanup(s, b, tmpPath)
-
-	for i := 0; i < 10; i++ {
-		base := int64(i*2 + 1)
-
-		// put foo
-		rev := s.Put([]byte("foo"), []byte("bar"), lease.NoLease)
-		if rev != base+1 {
-			t.Errorf("#%d: put rev = %d, want %d", i, rev, base+1)
-		}
-
-		kvs, rev, err := s.Range([]byte("foo"), nil, 0, base+1)
-		if err != nil {
-			t.Fatal(err)
-		}
-		wkvs := []storagepb.KeyValue{
-			{Key: []byte("foo"), Value: []byte("bar"), CreateRevision: base + 1, ModRevision: base + 1, Version: 1, Lease: int64(lease.NoLease)},
-		}
-		if !reflect.DeepEqual(kvs, wkvs) {
-			t.Errorf("#%d: kvs = %+v, want %+v", i, kvs, wkvs)
-		}
-		if rev != base+1 {
-			t.Errorf("#%d: range rev = %d, want %d", i, rev, base+1)
-		}
-
-		// delete foo
-		n, rev := s.DeleteRange([]byte("foo"), nil)
-		if n != 1 || rev != base+2 {
-			t.Errorf("#%d: n = %d, rev = %d, want (%d, %d)", i, n, rev, 1, base+2)
-		}
-
-		kvs, rev, err = s.Range([]byte("foo"), nil, 0, base+2)
-		if err != nil {
-			t.Fatal(err)
-		}
-		if kvs != nil {
-			t.Errorf("#%d: kvs = %+v, want %+v", i, kvs, nil)
-		}
-		if rev != base+2 {
-			t.Errorf("#%d: range rev = %d, want %d", i, rev, base+2)
-		}
-	}
-}
-
-func TestKVTxnBlockNonTxnOperations(t *testing.T) {
-	b, tmpPath := backend.NewDefaultTmpBackend()
-	s := NewStore(b, &lease.FakeLessor{}, nil)
-
-	tests := []func(){
-		func() { s.Range([]byte("foo"), nil, 0, 0) },
-		func() { s.Put([]byte("foo"), nil, lease.NoLease) },
-		func() { s.DeleteRange([]byte("foo"), nil) },
-	}
-	for i, tt := range tests {
-		id := s.TxnBegin()
-		done := make(chan struct{}, 1)
-		go func() {
-			tt()
-			done <- struct{}{}
-		}()
-		select {
-		case <-done:
-			t.Fatalf("#%d: operation failed to be blocked", i)
-		case <-time.After(10 * time.Millisecond):
-		}
-
-		s.TxnEnd(id)
-		select {
-		case <-done:
-		case <-time.After(10 * time.Second):
-			testutil.FatalStack(t, fmt.Sprintf("#%d: operation failed to be unblocked", i))
-		}
-	}
-
-	// only close backend when we know all the tx are finished
-	cleanup(s, b, tmpPath)
-}
-
-func TestKVTxnWrongID(t *testing.T) {
-	b, tmpPath := backend.NewDefaultTmpBackend()
-	s := NewStore(b, &lease.FakeLessor{}, nil)
-	defer cleanup(s, b, tmpPath)
-
-	id := s.TxnBegin()
-	wrongid := id + 1
-
-	tests := []func() error{
-		func() error {
-			_, _, err := s.TxnRange(wrongid, []byte("foo"), nil, 0, 0)
-			return err
-		},
-		func() error {
-			_, err := s.TxnPut(wrongid, []byte("foo"), nil, lease.NoLease)
-			return err
-		},
-		func() error {
-			_, _, err := s.TxnDeleteRange(wrongid, []byte("foo"), nil)
-			return err
-		},
-		func() error { return s.TxnEnd(wrongid) },
-	}
-	for i, tt := range tests {
-		err := tt()
-		if err != ErrTxnIDMismatch {
-			t.Fatalf("#%d: err = %+v, want %+v", i, err, ErrTxnIDMismatch)
-		}
-	}
-
-	err := s.TxnEnd(id)
-	if err != nil {
-		t.Fatalf("end err = %+v, want %+v", err, nil)
-	}
-}
-
-// test that txn range, put, delete on single key in sequence repeatedly works correctly.
-func TestKVTxnOperationInSequence(t *testing.T) {
-	b, tmpPath := backend.NewDefaultTmpBackend()
-	s := NewStore(b, &lease.FakeLessor{}, nil)
-	defer cleanup(s, b, tmpPath)
-
-	for i := 0; i < 10; i++ {
-		id := s.TxnBegin()
-		base := int64(i + 1)
-
-		// put foo
-		rev, err := s.TxnPut(id, []byte("foo"), []byte("bar"), lease.NoLease)
-		if err != nil {
-			t.Fatal(err)
-		}
-		if rev != base+1 {
-			t.Errorf("#%d: put rev = %d, want %d", i, rev, base+1)
-		}
-
-		kvs, rev, err := s.TxnRange(id, []byte("foo"), nil, 0, base+1)
-		if err != nil {
-			t.Fatal(err)
-		}
-		wkvs := []storagepb.KeyValue{
-			{Key: []byte("foo"), Value: []byte("bar"), CreateRevision: base + 1, ModRevision: base + 1, Version: 1, Lease: int64(lease.NoLease)},
-		}
-		if !reflect.DeepEqual(kvs, wkvs) {
-			t.Errorf("#%d: kvs = %+v, want %+v", i, kvs, wkvs)
-		}
-		if rev != base+1 {
-			t.Errorf("#%d: range rev = %d, want %d", i, rev, base+1)
-		}
-
-		// delete foo
-		n, rev, err := s.TxnDeleteRange(id, []byte("foo"), nil)
-		if err != nil {
-			t.Fatal(err)
-		}
-		if n != 1 || rev != base+1 {
-			t.Errorf("#%d: n = %d, rev = %d, want (%d, %d)", i, n, rev, 1, base+1)
-		}
-
-		kvs, rev, err = s.TxnRange(id, []byte("foo"), nil, 0, base+1)
-		if err != nil {
-			t.Errorf("#%d: range error (%v)", i, err)
-		}
-		if kvs != nil {
-			t.Errorf("#%d: kvs = %+v, want %+v", i, kvs, nil)
-		}
-		if rev != base+1 {
-			t.Errorf("#%d: range rev = %d, want %d", i, rev, base+1)
-		}
-
-		s.TxnEnd(id)
-	}
-}
-
-func TestKVCompactReserveLastValue(t *testing.T) {
-	b, tmpPath := backend.NewDefaultTmpBackend()
-	s := NewStore(b, &lease.FakeLessor{}, nil)
-	defer cleanup(s, b, tmpPath)
-
-	s.Put([]byte("foo"), []byte("bar0"), 1)
-	s.Put([]byte("foo"), []byte("bar1"), 2)
-	s.DeleteRange([]byte("foo"), nil)
-	s.Put([]byte("foo"), []byte("bar2"), 3)
-
-	// rev in tests will be called in Compact() one by one on the same store
-	tests := []struct {
-		rev int64
-		// wanted kvs right after the compacted rev
-		wkvs []storagepb.KeyValue
-	}{
-		{
-			1,
-			[]storagepb.KeyValue{
-				{Key: []byte("foo"), Value: []byte("bar0"), CreateRevision: 2, ModRevision: 2, Version: 1, Lease: 1},
-			},
-		},
-		{
-			2,
-			[]storagepb.KeyValue{
-				{Key: []byte("foo"), Value: []byte("bar1"), CreateRevision: 2, ModRevision: 3, Version: 2, Lease: 2},
-			},
-		},
-		{
-			3,
-			nil,
-		},
-		{
-			4,
-			[]storagepb.KeyValue{
-				{Key: []byte("foo"), Value: []byte("bar2"), CreateRevision: 5, ModRevision: 5, Version: 1, Lease: 3},
-			},
-		},
-	}
-	for i, tt := range tests {
-		_, err := s.Compact(tt.rev)
-		if err != nil {
-			t.Errorf("#%d: unexpect compact error %v", i, err)
-		}
-		kvs, _, err := s.Range([]byte("foo"), nil, 0, tt.rev+1)
-		if err != nil {
-			t.Errorf("#%d: unexpect range error %v", i, err)
-		}
-		if !reflect.DeepEqual(kvs, tt.wkvs) {
-			t.Errorf("#%d: kvs = %+v, want %+v", i, kvs, tt.wkvs)
-		}
-	}
-}
-
-func TestKVCompactBad(t *testing.T) {
-	b, tmpPath := backend.NewDefaultTmpBackend()
-	s := NewStore(b, &lease.FakeLessor{}, nil)
-	defer cleanup(s, b, tmpPath)
-
-	s.Put([]byte("foo"), []byte("bar0"), lease.NoLease)
-	s.Put([]byte("foo"), []byte("bar1"), lease.NoLease)
-	s.Put([]byte("foo"), []byte("bar2"), lease.NoLease)
-
-	// rev in tests will be called in Compact() one by one on the same store
-	tests := []struct {
-		rev  int64
-		werr error
-	}{
-		{0, nil},
-		{1, nil},
-		{1, ErrCompacted},
-		{4, nil},
-		{5, ErrFutureRev},
-		{100, ErrFutureRev},
-	}
-	for i, tt := range tests {
-		_, err := s.Compact(tt.rev)
-		if err != tt.werr {
-			t.Errorf("#%d: compact error = %v, want %v", i, err, tt.werr)
-		}
-	}
-}
-
-func TestKVHash(t *testing.T) {
-	hashes := make([]uint32, 3)
-
-	for i := 0; i < len(hashes); i++ {
-		var err error
-		b, tmpPath := backend.NewDefaultTmpBackend()
-		kv := NewStore(b, &lease.FakeLessor{}, nil)
-		kv.Put([]byte("foo0"), []byte("bar0"), lease.NoLease)
-		kv.Put([]byte("foo1"), []byte("bar0"), lease.NoLease)
-		hashes[i], err = kv.Hash()
-		if err != nil {
-			t.Fatalf("failed to get hash: %v", err)
-		}
-		cleanup(kv, b, tmpPath)
-	}
-
-	for i := 1; i < len(hashes); i++ {
-		if hashes[i-1] != hashes[i] {
-			t.Errorf("hash[%d](%d) != hash[%d](%d)", i-1, hashes[i-1], i, hashes[i])
-		}
-	}
-}
-
-func TestKVRestore(t *testing.T) {
-	tests := []func(kv KV){
-		func(kv KV) {
-			kv.Put([]byte("foo"), []byte("bar0"), 1)
-			kv.Put([]byte("foo"), []byte("bar1"), 2)
-			kv.Put([]byte("foo"), []byte("bar2"), 3)
-		},
-		func(kv KV) {
-			kv.Put([]byte("foo"), []byte("bar0"), 1)
-			kv.DeleteRange([]byte("foo"), nil)
-			kv.Put([]byte("foo"), []byte("bar1"), 2)
-		},
-		func(kv KV) {
-			kv.Put([]byte("foo"), []byte("bar0"), 1)
-			kv.Put([]byte("foo"), []byte("bar1"), 2)
-			kv.Compact(1)
-		},
-	}
-	for i, tt := range tests {
-		b, tmpPath := backend.NewDefaultTmpBackend()
-		s := NewStore(b, &lease.FakeLessor{}, nil)
-		tt(s)
-		var kvss [][]storagepb.KeyValue
-		for k := int64(0); k < 10; k++ {
-			kvs, _, _ := s.Range([]byte("a"), []byte("z"), 0, k)
-			kvss = append(kvss, kvs)
-		}
-		s.Close()
-
-		// ns should recover the the previous state from backend.
-		ns := NewStore(b, &lease.FakeLessor{}, nil)
-		// wait for possible compaction to finish
-		testutil.WaitSchedule()
-		var nkvss [][]storagepb.KeyValue
-		for k := int64(0); k < 10; k++ {
-			nkvs, _, _ := ns.Range([]byte("a"), []byte("z"), 0, k)
-			nkvss = append(nkvss, nkvs)
-		}
-		cleanup(ns, b, tmpPath)
-
-		if !reflect.DeepEqual(nkvss, kvss) {
-			t.Errorf("#%d: kvs history = %+v, want %+v", i, nkvss, kvss)
-		}
-	}
-}
-
-func TestKVSnapshot(t *testing.T) {
-	b, tmpPath := backend.NewDefaultTmpBackend()
-	s := NewStore(b, &lease.FakeLessor{}, nil)
-	defer cleanup(s, b, tmpPath)
-
-	wkvs := put3TestKVs(s)
-
-	newPath := "new_test"
-	f, err := os.Create(newPath)
-	if err != nil {
-		t.Fatal(err)
-	}
-	defer os.Remove(newPath)
-
-	snap := s.b.Snapshot()
-	defer snap.Close()
-	_, err = snap.WriteTo(f)
-	if err != nil {
-		t.Fatal(err)
-	}
-	f.Close()
-
-	ns := NewStore(b, &lease.FakeLessor{}, nil)
-	defer ns.Close()
-	kvs, rev, err := ns.Range([]byte("a"), []byte("z"), 0, 0)
-	if err != nil {
-		t.Errorf("unexpect range error (%v)", err)
-	}
-	if !reflect.DeepEqual(kvs, wkvs) {
-		t.Errorf("kvs = %+v, want %+v", kvs, wkvs)
-	}
-	if rev != 4 {
-		t.Errorf("rev = %d, want %d", rev, 4)
-	}
-}
-
-func TestWatchableKVWatch(t *testing.T) {
-	b, tmpPath := backend.NewDefaultTmpBackend()
-	s := WatchableKV(newWatchableStore(b, &lease.FakeLessor{}, nil))
-	defer cleanup(s, b, tmpPath)
-
-	w := s.NewWatchStream()
-	defer w.Close()
-
-	wid := w.Watch([]byte("foo"), []byte("fop"), 0)
-
-	wev := []storagepb.Event{
-		{Type: storagepb.PUT,
-			Kv: &storagepb.KeyValue{
-				Key:            []byte("foo"),
-				Value:          []byte("bar"),
-				CreateRevision: 2,
-				ModRevision:    2,
-				Version:        1,
-				Lease:          1,
-			},
-		},
-		{
-			Type: storagepb.PUT,
-			Kv: &storagepb.KeyValue{
-				Key:            []byte("foo1"),
-				Value:          []byte("bar1"),
-				CreateRevision: 3,
-				ModRevision:    3,
-				Version:        1,
-				Lease:          2,
-			},
-		},
-		{
-			Type: storagepb.PUT,
-			Kv: &storagepb.KeyValue{
-				Key:            []byte("foo1"),
-				Value:          []byte("bar11"),
-				CreateRevision: 3,
-				ModRevision:    4,
-				Version:        2,
-				Lease:          3,
-			},
-		},
-	}
-
-	s.Put([]byte("foo"), []byte("bar"), 1)
-	select {
-	case resp := <-w.Chan():
-		if resp.WatchID != wid {
-			t.Errorf("resp.WatchID got = %d, want = %d", resp.WatchID, wid)
-		}
-		ev := resp.Events[0]
-		if !reflect.DeepEqual(ev, wev[0]) {
-			t.Errorf("watched event = %+v, want %+v", ev, wev[0])
-		}
-	case <-time.After(5 * time.Second):
-		// CPU might be too slow, and the routine is not able to switch around
-		testutil.FatalStack(t, "failed to watch the event")
-	}
-
-	s.Put([]byte("foo1"), []byte("bar1"), 2)
-	select {
-	case resp := <-w.Chan():
-		if resp.WatchID != wid {
-			t.Errorf("resp.WatchID got = %d, want = %d", resp.WatchID, wid)
-		}
-		ev := resp.Events[0]
-		if !reflect.DeepEqual(ev, wev[1]) {
-			t.Errorf("watched event = %+v, want %+v", ev, wev[1])
-		}
-	case <-time.After(5 * time.Second):
-		testutil.FatalStack(t, "failed to watch the event")
-	}
-
-	w = s.NewWatchStream()
-	wid = w.Watch([]byte("foo1"), []byte("foo2"), 3)
-
-	select {
-	case resp := <-w.Chan():
-		if resp.WatchID != wid {
-			t.Errorf("resp.WatchID got = %d, want = %d", resp.WatchID, wid)
-		}
-		ev := resp.Events[0]
-		if !reflect.DeepEqual(ev, wev[1]) {
-			t.Errorf("watched event = %+v, want %+v", ev, wev[1])
-		}
-	case <-time.After(5 * time.Second):
-		testutil.FatalStack(t, "failed to watch the event")
-	}
-
-	s.Put([]byte("foo1"), []byte("bar11"), 3)
-	select {
-	case resp := <-w.Chan():
-		if resp.WatchID != wid {
-			t.Errorf("resp.WatchID got = %d, want = %d", resp.WatchID, wid)
-		}
-		ev := resp.Events[0]
-		if !reflect.DeepEqual(ev, wev[2]) {
-			t.Errorf("watched event = %+v, want %+v", ev, wev[2])
-		}
-	case <-time.After(5 * time.Second):
-		testutil.FatalStack(t, "failed to watch the event")
-	}
-}
-
-func cleanup(s KV, b backend.Backend, path string) {
-	s.Close()
-	b.Close()
-	os.Remove(path)
-}
-
-func put3TestKVs(s KV) []storagepb.KeyValue {
-	s.Put([]byte("foo"), []byte("bar"), 1)
-	s.Put([]byte("foo1"), []byte("bar1"), 2)
-	s.Put([]byte("foo2"), []byte("bar2"), 3)
-	return []storagepb.KeyValue{
-		{Key: []byte("foo"), Value: []byte("bar"), CreateRevision: 2, ModRevision: 2, Version: 1, Lease: 1},
-		{Key: []byte("foo1"), Value: []byte("bar1"), CreateRevision: 3, ModRevision: 3, Version: 1, Lease: 2},
-		{Key: []byte("foo2"), Value: []byte("bar2"), CreateRevision: 4, ModRevision: 4, Version: 1, Lease: 3},
-	}
-}
diff --git a/storage/kvstore.go b/storage/kvstore.go
deleted file mode 100644
index 185ec11..0000000
--- a/storage/kvstore.go
+++ /dev/null
@@ -1,636 +0,0 @@
-// Copyright 2015 CoreOS, Inc.
-//
-// Licensed under the Apache License, Version 2.0 (the "License");
-// you may not use this file except in compliance with the License.
-// You may obtain a copy of the License at
-//
-//     http://www.apache.org/licenses/LICENSE-2.0
-//
-// Unless required by applicable law or agreed to in writing, software
-// distributed under the License is distributed on an "AS IS" BASIS,
-// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-// See the License for the specific language governing permissions and
-// limitations under the License.
-
-package storage
-
-import (
-	"encoding/binary"
-	"errors"
-	"log"
-	"math"
-	"math/rand"
-	"sync"
-	"time"
-
-	"github.com/coreos/etcd/lease"
-	"github.com/coreos/etcd/pkg/schedule"
-	"github.com/coreos/etcd/storage/backend"
-	"github.com/coreos/etcd/storage/storagepb"
-	"golang.org/x/net/context"
-)
-
-var (
-	keyBucketName  = []byte("key")
-	metaBucketName = []byte("meta")
-
-	// markedRevBytesLen is the byte length of marked revision.
-	// The first `revBytesLen` bytes represents a normal revision. The last
-	// one byte is the mark.
-	markedRevBytesLen      = revBytesLen + 1
-	markBytePosition       = markedRevBytesLen - 1
-	markTombstone     byte = 't'
-
-	consistentIndexKeyName  = []byte("consistent_index")
-	scheduledCompactKeyName = []byte("scheduledCompactRev")
-	finishedCompactKeyName  = []byte("finishedCompactRev")
-
-	ErrTxnIDMismatch = errors.New("storage: txn id mismatch")
-	ErrCompacted     = errors.New("storage: required revision has been compacted")
-	ErrFutureRev     = errors.New("storage: required revision is a future revision")
-	ErrCanceled      = errors.New("storage: watcher is canceled")
-)
-
-// ConsistentIndexGetter is an interface that wraps the Get method.
-// Consistent index is the offset of an entry in a consistent replicated log.
-type ConsistentIndexGetter interface {
-	// ConsistentIndex returns the consistent index of current executing entry.
-	ConsistentIndex() uint64
-}
-
-type store struct {
-	mu sync.Mutex // guards the following
-
-	ig ConsistentIndexGetter
-
-	b       backend.Backend
-	kvindex index
-
-	le lease.Lessor
-
-	currentRev revision
-	// the main revision of the last compaction
-	compactMainRev int64
-
-	tx    backend.BatchTx
-	txnID int64 // tracks the current txnID to verify txn operations
-
-	changes   []storagepb.KeyValue
-	fifoSched schedule.Scheduler
-
-	stopc chan struct{}
-}
-
-// NewStore returns a new store. It is useful to create a store inside
-// storage pkg. It should only be used for testing externally.
-func NewStore(b backend.Backend, le lease.Lessor, ig ConsistentIndexGetter) *store {
-	s := &store{
-		b:       b,
-		ig:      ig,
-		kvindex: newTreeIndex(),
-
-		le: le,
-
-		currentRev:     revision{main: 1},
-		compactMainRev: -1,
-
-		fifoSched: schedule.NewFIFOScheduler(),
-
-		stopc: make(chan struct{}),
-	}
-
-	if s.le != nil {
-		s.le.SetRangeDeleter(s)
-	}
-
-	tx := s.b.BatchTx()
-	tx.Lock()
-	tx.UnsafeCreateBucket(keyBucketName)
-	tx.UnsafeCreateBucket(metaBucketName)
-	tx.Unlock()
-	s.b.ForceCommit()
-
-	if err := s.restore(); err != nil {
-		// TODO: return the error instead of panic here?
-		panic("failed to recover store from backend")
-	}
-
-	return s
-}
-
-func (s *store) Rev() int64 {
-	s.mu.Lock()
-	defer s.mu.Unlock()
-
-	return s.currentRev.main
-}
-
-func (s *store) FirstRev() int64 {
-	s.mu.Lock()
-	defer s.mu.Unlock()
-
-	return s.compactMainRev
-}
-
-func (s *store) Put(key, value []byte, lease lease.LeaseID) int64 {
-	id := s.TxnBegin()
-	s.put(key, value, lease)
-	s.txnEnd(id)
-
-	putCounter.Inc()
-
-	return int64(s.currentRev.main)
-}
-
-func (s *store) Range(key, end []byte, limit, rangeRev int64) (kvs []storagepb.KeyValue, rev int64, err error) {
-	id := s.TxnBegin()
-	kvs, rev, err = s.rangeKeys(key, end, limit, rangeRev)
-	s.txnEnd(id)
-
-	rangeCounter.Inc()
-
-	return kvs, rev, err
-}
-
-func (s *store) DeleteRange(key, end []byte) (n, rev int64) {
-	id := s.TxnBegin()
-	n = s.deleteRange(key, end)
-	s.txnEnd(id)
-
-	deleteCounter.Inc()
-
-	return n, int64(s.currentRev.main)
-}
-
-func (s *store) TxnBegin() int64 {
-	s.mu.Lock()
-	s.currentRev.sub = 0
-	s.tx = s.b.BatchTx()
-	s.tx.Lock()
-	s.saveIndex()
-
-	s.txnID = rand.Int63()
-	return s.txnID
-}
-
-func (s *store) TxnEnd(txnID int64) error {
-	err := s.txnEnd(txnID)
-	if err != nil {
-		return err
-	}
-
-	txnCounter.Inc()
-	return nil
-}
-
-// txnEnd is used for unlocking an internal txn. It does
-// not increase the txnCounter.
-func (s *store) txnEnd(txnID int64) error {
-	if txnID != s.txnID {
-		return ErrTxnIDMismatch
-	}
-
-	s.tx.Unlock()
-	if s.currentRev.sub != 0 {
-		s.currentRev.main += 1
-	}
-	s.currentRev.sub = 0
-
-	dbTotalSize.Set(float64(s.b.Size()))
-	s.mu.Unlock()
-	return nil
-}
-
-func (s *store) TxnRange(txnID int64, key, end []byte, limit, rangeRev int64) (kvs []storagepb.KeyValue, rev int64, err error) {
-	if txnID != s.txnID {
-		return nil, 0, ErrTxnIDMismatch
-	}
-	return s.rangeKeys(key, end, limit, rangeRev)
-}
-
-func (s *store) TxnPut(txnID int64, key, value []byte, lease lease.LeaseID) (rev int64, err error) {
-	if txnID != s.txnID {
-		return 0, ErrTxnIDMismatch
-	}
-
-	s.put(key, value, lease)
-	return int64(s.currentRev.main + 1), nil
-}
-
-func (s *store) TxnDeleteRange(txnID int64, key, end []byte) (n, rev int64, err error) {
-	if txnID != s.txnID {
-		return 0, 0, ErrTxnIDMismatch
-	}
-
-	n = s.deleteRange(key, end)
-	if n != 0 || s.currentRev.sub != 0 {
-		rev = int64(s.currentRev.main + 1)
-	} else {
-		rev = int64(s.currentRev.main)
-	}
-	return n, rev, nil
-}
-
-func (s *store) compactBarrier(ctx context.Context, ch chan struct{}) {
-	if ctx == nil || ctx.Err() != nil {
-		s.mu.Lock()
-		select {
-		case <-s.stopc:
-		default:
-			f := func(ctx context.Context) { s.compactBarrier(ctx, ch) }
-			s.fifoSched.Schedule(f)
-		}
-		s.mu.Unlock()
-		return
-	}
-	close(ch)
-}
-
-func (s *store) Compact(rev int64) (<-chan struct{}, error) {
-	s.mu.Lock()
-	defer s.mu.Unlock()
-	if rev <= s.compactMainRev {
-		ch := make(chan struct{})
-		f := func(ctx context.Context) { s.compactBarrier(ctx, ch) }
-		s.fifoSched.Schedule(f)
-		return ch, ErrCompacted
-	}
-	if rev > s.currentRev.main {
-		return nil, ErrFutureRev
-	}
-
-	start := time.Now()
-
-	s.compactMainRev = rev
-
-	rbytes := newRevBytes()
-	revToBytes(revision{main: rev}, rbytes)
-
-	tx := s.b.BatchTx()
-	tx.Lock()
-	tx.UnsafePut(metaBucketName, scheduledCompactKeyName, rbytes)
-	tx.Unlock()
-	// ensure that desired compaction is persisted
-	s.b.ForceCommit()
-
-	keep := s.kvindex.Compact(rev)
-	ch := make(chan struct{})
-	var j = func(ctx context.Context) {
-		if ctx.Err() != nil {
-			s.compactBarrier(ctx, ch)
-			return
-		}
-		if !s.scheduleCompaction(rev, keep) {
-			s.compactBarrier(nil, ch)
-			return
-		}
-		close(ch)
-	}
-
-	s.fifoSched.Schedule(j)
-
-	indexCompactionPauseDurations.Observe(float64(time.Since(start) / time.Millisecond))
-	return ch, nil
-}
-
-func (s *store) Hash() (uint32, error) {
-	s.b.ForceCommit()
-	return s.b.Hash()
-}
-
-func (s *store) Commit() { s.b.ForceCommit() }
-
-func (s *store) Restore(b backend.Backend) error {
-	s.mu.Lock()
-	defer s.mu.Unlock()
-
-	close(s.stopc)
-	s.fifoSched.Stop()
-
-	s.b = b
-	s.kvindex = newTreeIndex()
-	s.currentRev = revision{main: 1}
-	s.compactMainRev = -1
-	s.tx = b.BatchTx()
-	s.txnID = -1
-	s.fifoSched = schedule.NewFIFOScheduler()
-	s.stopc = make(chan struct{})
-
-	return s.restore()
-}
-
-func (s *store) restore() error {
-	min, max := newRevBytes(), newRevBytes()
-	revToBytes(revision{main: 1}, min)
-	revToBytes(revision{main: math.MaxInt64, sub: math.MaxInt64}, max)
-
-	// restore index
-	tx := s.b.BatchTx()
-	tx.Lock()
-	_, finishedCompactBytes := tx.UnsafeRange(metaBucketName, finishedCompactKeyName, nil, 0)
-	if len(finishedCompactBytes) != 0 {
-		s.compactMainRev = bytesToRev(finishedCompactBytes[0]).main
-		log.Printf("storage: restore compact to %d", s.compactMainRev)
-	}
-
-	// TODO: limit N to reduce max memory usage
-	keys, vals := tx.UnsafeRange(keyBucketName, min, max, 0)
-	for i, key := range keys {
-		var kv storagepb.KeyValue
-		if err := kv.Unmarshal(vals[i]); err != nil {
-			log.Fatalf("storage: cannot unmarshal event: %v", err)
-		}
-
-		rev := bytesToRev(key[:revBytesLen])
-
-		// restore index
-		switch {
-		case isTombstone(key):
-			s.kvindex.Tombstone(kv.Key, rev)
-			if lease.LeaseID(kv.Lease) != lease.NoLease {
-				err := s.le.Detach(lease.LeaseID(kv.Lease), []lease.LeaseItem{{Key: string(kv.Key)}})
-				if err != nil && err != lease.ErrLeaseNotFound {
-					log.Fatalf("storage: unexpected Detach error %v", err)
-				}
-			}
-		default:
-			s.kvindex.Restore(kv.Key, revision{kv.CreateRevision, 0}, rev, kv.Version)
-			if lease.LeaseID(kv.Lease) != lease.NoLease {
-				if s.le == nil {
-					panic("no lessor to attach lease")
-				}
-				err := s.le.Attach(lease.LeaseID(kv.Lease), []lease.LeaseItem{{Key: string(kv.Key)}})
-				// We are walking through the kv history here. It is possible that we attached a key to
-				// the lease and the lease was revoked later.
-				// Thus attaching an old version of key to a none existing lease is possible here, and
-				// we should just ignore the error.
-				if err != nil && err != lease.ErrLeaseNotFound {
-					panic("unexpected Attach error")
-				}
-			}
-		}
-
-		// update revision
-		s.currentRev = rev
-	}
-
-	_, scheduledCompactBytes := tx.UnsafeRange(metaBucketName, scheduledCompactKeyName, nil, 0)
-	scheduledCompact := int64(0)
-	if len(scheduledCompactBytes) != 0 {
-		scheduledCompact = bytesToRev(scheduledCompactBytes[0]).main
-		if scheduledCompact <= s.compactMainRev {
-			scheduledCompact = 0
-		}
-	}
-
-	tx.Unlock()
-
-	if scheduledCompact != 0 {
-		s.Compact(scheduledCompact)
-		log.Printf("storage: resume scheduled compaction at %d", scheduledCompact)
-	}
-
-	return nil
-}
-
-func (s *store) Close() error {
-	close(s.stopc)
-	s.fifoSched.Stop()
-	return nil
-}
-
-func (a *store) Equal(b *store) bool {
-	if a.currentRev != b.currentRev {
-		return false
-	}
-	if a.compactMainRev != b.compactMainRev {
-		return false
-	}
-	return a.kvindex.Equal(b.kvindex)
-}
-
-// range is a keyword in Go, add Keys suffix.
-func (s *store) rangeKeys(key, end []byte, limit, rangeRev int64) (kvs []storagepb.KeyValue, curRev int64, err error) {
-	curRev = int64(s.currentRev.main)
-	if s.currentRev.sub > 0 {
-		curRev += 1
-	}
-
-	if rangeRev > curRev {
-		return nil, s.currentRev.main, ErrFutureRev
-	}
-	var rev int64
-	if rangeRev <= 0 {
-		rev = curRev
-	} else {
-		rev = rangeRev
-	}
-	if rev < s.compactMainRev {
-		return nil, 0, ErrCompacted
-	}
-
-	_, revpairs := s.kvindex.Range(key, end, int64(rev))
-	if len(revpairs) == 0 {
-		return nil, curRev, nil
-	}
-
-	for _, revpair := range revpairs {
-		start, end := revBytesRange(revpair)
-
-		_, vs := s.tx.UnsafeRange(keyBucketName, start, end, 0)
-		if len(vs) != 1 {
-			log.Fatalf("storage: range cannot find rev (%d,%d)", revpair.main, revpair.sub)
-		}
-
-		var kv storagepb.KeyValue
-		if err := kv.Unmarshal(vs[0]); err != nil {
-			log.Fatalf("storage: cannot unmarshal event: %v", err)
-		}
-		kvs = append(kvs, kv)
-		if limit > 0 && len(kvs) >= int(limit) {
-			break
-		}
-	}
-	return kvs, curRev, nil
-}
-
-func (s *store) put(key, value []byte, leaseID lease.LeaseID) {
-	rev := s.currentRev.main + 1
-	c := rev
-	oldLease := lease.NoLease
-
-	// if the key exists before, use its previous created and
-	// get its previous leaseID
-	grev, created, ver, err := s.kvindex.Get(key, rev)
-	if err == nil {
-		c = created.main
-		ibytes := newRevBytes()
-		revToBytes(grev, ibytes)
-		_, vs := s.tx.UnsafeRange(keyBucketName, ibytes, nil, 0)
-		var kv storagepb.KeyValue
-		if err = kv.Unmarshal(vs[0]); err != nil {
-			log.Fatalf("storage: cannot unmarshal value: %v", err)
-		}
-		oldLease = lease.LeaseID(kv.Lease)
-	}
-
-	ibytes := newRevBytes()
-	revToBytes(revision{main: rev, sub: s.currentRev.sub}, ibytes)
-
-	ver = ver + 1
-	kv := storagepb.KeyValue{
-		Key:            key,
-		Value:          value,
-		CreateRevision: c,
-		ModRevision:    rev,
-		Version:        ver,
-		Lease:          int64(leaseID),
-	}
-
-	d, err := kv.Marshal()
-	if err != nil {
-		log.Fatalf("storage: cannot marshal event: %v", err)
-	}
-
-	s.tx.UnsafeSeqPut(keyBucketName, ibytes, d)
-	s.kvindex.Put(key, revision{main: rev, sub: s.currentRev.sub})
-	s.changes = append(s.changes, kv)
-	s.currentRev.sub += 1
-
-	if oldLease != lease.NoLease {
-		if s.le == nil {
-			panic("no lessor to detach lease")
-		}
-
-		err = s.le.Detach(oldLease, []lease.LeaseItem{{Key: string(key)}})
-		if err != nil {
-			panic("unexpected error from lease detach")
-		}
-	}
-
-	if leaseID != lease.NoLease {
-		if s.le == nil {
-			panic("no lessor to attach lease")
-		}
-
-		err = s.le.Attach(leaseID, []lease.LeaseItem{{Key: string(key)}})
-		if err != nil {
-			panic("unexpected error from lease Attach")
-		}
-	}
-}
-
-func (s *store) deleteRange(key, end []byte) int64 {
-	rrev := s.currentRev.main
-	if s.currentRev.sub > 0 {
-		rrev += 1
-	}
-	keys, revs := s.kvindex.Range(key, end, rrev)
-
-	if len(keys) == 0 {
-		return 0
-	}
-
-	for i, key := range keys {
-		s.delete(key, revs[i])
-	}
-	return int64(len(keys))
-}
-
-func (s *store) delete(key []byte, rev revision) {
-	mainrev := s.currentRev.main + 1
-
-	ibytes := newRevBytes()
-	revToBytes(revision{main: mainrev, sub: s.currentRev.sub}, ibytes)
-	ibytes = appendMarkTombstone(ibytes)
-
-	kv := storagepb.KeyValue{
-		Key: key,
-	}
-
-	d, err := kv.Marshal()
-	if err != nil {
-		log.Fatalf("storage: cannot marshal event: %v", err)
-	}
-
-	s.tx.UnsafeSeqPut(keyBucketName, ibytes, d)
-	err = s.kvindex.Tombstone(key, revision{main: mainrev, sub: s.currentRev.sub})
-	if err != nil {
-		log.Fatalf("storage: cannot tombstone an existing key (%s): %v", string(key), err)
-	}
-	s.changes = append(s.changes, kv)
-	s.currentRev.sub += 1
-
-	ibytes = newRevBytes()
-	revToBytes(rev, ibytes)
-	_, vs := s.tx.UnsafeRange(keyBucketName, ibytes, nil, 0)
-
-	kv.Reset()
-	if err = kv.Unmarshal(vs[0]); err != nil {
-		log.Fatalf("storage: cannot unmarshal value: %v", err)
-	}
-
-	if lease.LeaseID(kv.Lease) != lease.NoLease {
-		err = s.le.Detach(lease.LeaseID(kv.Lease), []lease.LeaseItem{{Key: string(kv.Key)}})
-		if err != nil {
-			log.Fatalf("storage: cannot detach %v", err)
-		}
-	}
-}
-
-func (s *store) getChanges() []storagepb.KeyValue {
-	changes := s.changes
-	s.changes = make([]storagepb.KeyValue, 0, 128)
-	return changes
-}
-
-func (s *store) saveIndex() {
-	if s.ig == nil {
-		return
-	}
-	tx := s.tx
-	// TODO: avoid this unnecessary allocation
-	bs := make([]byte, 8)
-	binary.BigEndian.PutUint64(bs, s.ig.ConsistentIndex())
-	// put the index into the underlying backend
-	// tx has been locked in TxnBegin, so there is no need to lock it again
-	tx.UnsafePut(metaBucketName, consistentIndexKeyName, bs)
-}
-
-func (s *store) ConsistentIndex() uint64 {
-	// TODO: cache index in a uint64 field?
-	tx := s.b.BatchTx()
-	tx.Lock()
-	defer tx.Unlock()
-	_, vs := tx.UnsafeRange(metaBucketName, consistentIndexKeyName, nil, 0)
-	if len(vs) == 0 {
-		return 0
-	}
-	return binary.BigEndian.Uint64(vs[0])
-}
-
-// appendMarkTombstone appends tombstone mark to normal revision bytes.
-func appendMarkTombstone(b []byte) []byte {
-	if len(b) != revBytesLen {
-		log.Panicf("cannot append mark to non normal revision bytes")
-	}
-	return append(b, markTombstone)
-}
-
-// isTombstone checks whether the revision bytes is a tombstone.
-func isTombstone(b []byte) bool {
-	return len(b) == markedRevBytesLen && b[markBytePosition] == markTombstone
-}
-
-// revBytesRange returns the range of revision bytes at
-// the given revision.
-func revBytesRange(rev revision) (start, end []byte) {
-	start = newRevBytes()
-	revToBytes(rev, start)
-
-	end = newRevBytes()
-	endRev := revision{main: rev.main, sub: rev.sub + 1}
-	revToBytes(endRev, end)
-
-	return start, end
-}
diff --git a/storage/kvstore_bench_test.go b/storage/kvstore_bench_test.go
deleted file mode 100644
index 45c778a..0000000
--- a/storage/kvstore_bench_test.go
+++ /dev/null
@@ -1,62 +0,0 @@
-// Copyright 2015 CoreOS, Inc.
-//
-// Licensed under the Apache License, Version 2.0 (the "License");
-// you may not use this file except in compliance with the License.
-// You may obtain a copy of the License at
-//
-//     http://www.apache.org/licenses/LICENSE-2.0
-//
-// Unless required by applicable law or agreed to in writing, software
-// distributed under the License is distributed on an "AS IS" BASIS,
-// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-// See the License for the specific language governing permissions and
-// limitations under the License.
-
-package storage
-
-import (
-	"log"
-	"testing"
-
-	"github.com/coreos/etcd/lease"
-	"github.com/coreos/etcd/storage/backend"
-)
-
-func BenchmarkStorePut(b *testing.B) {
-	be, tmpPath := backend.NewDefaultTmpBackend()
-	s := NewStore(be, &lease.FakeLessor{}, nil)
-	defer cleanup(s, be, tmpPath)
-
-	// arbitrary number of bytes
-	bytesN := 64
-	keys := createBytesSlice(bytesN, b.N)
-	vals := createBytesSlice(bytesN, b.N)
-
-	b.ResetTimer()
-	for i := 0; i < b.N; i++ {
-		s.Put(keys[i], vals[i], lease.NoLease)
-	}
-}
-
-// BenchmarkStoreTxnPut benchmarks the Put operation
-// with transaction begin and end, where transaction involves
-// some synchronization operations, such as mutex locking.
-func BenchmarkStoreTxnPut(b *testing.B) {
-	be, tmpPath := backend.NewDefaultTmpBackend()
-	s := NewStore(be, &lease.FakeLessor{}, nil)
-	defer cleanup(s, be, tmpPath)
-
-	// arbitrary number of bytes
-	bytesN := 64
-	keys := createBytesSlice(bytesN, b.N)
-	vals := createBytesSlice(bytesN, b.N)
-
-	b.ResetTimer()
-	for i := 0; i < b.N; i++ {
-		id := s.TxnBegin()
-		if _, err := s.TxnPut(id, keys[i], vals[i], lease.NoLease); err != nil {
-			log.Fatalf("txn put error: %v", err)
-		}
-		s.TxnEnd(id)
-	}
-}
diff --git a/storage/kvstore_compaction.go b/storage/kvstore_compaction.go
deleted file mode 100644
index 4f3c083..0000000
--- a/storage/kvstore_compaction.go
+++ /dev/null
@@ -1,65 +0,0 @@
-// Copyright 2015 CoreOS, Inc.
-//
-// Licensed under the Apache License, Version 2.0 (the "License");
-// you may not use this file except in compliance with the License.
-// You may obtain a copy of the License at
-//
-//     http://www.apache.org/licenses/LICENSE-2.0
-//
-// Unless required by applicable law or agreed to in writing, software
-// distributed under the License is distributed on an "AS IS" BASIS,
-// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-// See the License for the specific language governing permissions and
-// limitations under the License.
-
-package storage
-
-import (
-	"encoding/binary"
-	"time"
-)
-
-func (s *store) scheduleCompaction(compactMainRev int64, keep map[revision]struct{}) bool {
-	totalStart := time.Now()
-	defer dbCompactionTotalDurations.Observe(float64(time.Since(totalStart) / time.Millisecond))
-
-	end := make([]byte, 8)
-	binary.BigEndian.PutUint64(end, uint64(compactMainRev+1))
-
-	batchsize := int64(10000)
-	last := make([]byte, 8+1+8)
-	for {
-		var rev revision
-
-		start := time.Now()
-		tx := s.b.BatchTx()
-		tx.Lock()
-
-		keys, _ := tx.UnsafeRange(keyBucketName, last, end, batchsize)
-		for _, key := range keys {
-			rev = bytesToRev(key)
-			if _, ok := keep[rev]; !ok {
-				tx.UnsafeDelete(keyBucketName, key)
-			}
-		}
-
-		if len(keys) < int(batchsize) {
-			rbytes := make([]byte, 8+1+8)
-			revToBytes(revision{main: compactMainRev}, rbytes)
-			tx.UnsafePut(metaBucketName, finishedCompactKeyName, rbytes)
-			tx.Unlock()
-			return true
-		}
-
-		// update last
-		revToBytes(revision{main: rev.main, sub: rev.sub + 1}, last)
-		tx.Unlock()
-		dbCompactionPauseDurations.Observe(float64(time.Since(start) / time.Millisecond))
-
-		select {
-		case <-time.After(100 * time.Millisecond):
-		case <-s.stopc:
-			return false
-		}
-	}
-}
diff --git a/storage/kvstore_compaction_test.go b/storage/kvstore_compaction_test.go
deleted file mode 100644
index 2bd7281..0000000
--- a/storage/kvstore_compaction_test.go
+++ /dev/null
@@ -1,95 +0,0 @@
-// Copyright 2015 CoreOS, Inc.
-//
-// Licensed under the Apache License, Version 2.0 (the "License");
-// you may not use this file except in compliance with the License.
-// You may obtain a copy of the License at
-//
-//     http://www.apache.org/licenses/LICENSE-2.0
-//
-// Unless required by applicable law or agreed to in writing, software
-// distributed under the License is distributed on an "AS IS" BASIS,
-// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-// See the License for the specific language governing permissions and
-// limitations under the License.
-
-package storage
-
-import (
-	"reflect"
-	"testing"
-
-	"github.com/coreos/etcd/lease"
-	"github.com/coreos/etcd/storage/backend"
-)
-
-func TestScheduleCompaction(t *testing.T) {
-	revs := []revision{{1, 0}, {2, 0}, {3, 0}}
-
-	tests := []struct {
-		rev   int64
-		keep  map[revision]struct{}
-		wrevs []revision
-	}{
-		// compact at 1 and discard all history
-		{
-			1,
-			nil,
-			revs[1:],
-		},
-		// compact at 3 and discard all history
-		{
-			3,
-			nil,
-			nil,
-		},
-		// compact at 1 and keeps history one step earlier
-		{
-			1,
-			map[revision]struct{}{
-				revision{main: 1}: {},
-			},
-			revs,
-		},
-		// compact at 1 and keeps history two steps earlier
-		{
-			3,
-			map[revision]struct{}{
-				revision{main: 2}: {},
-				revision{main: 3}: {},
-			},
-			revs[1:],
-		},
-	}
-	for i, tt := range tests {
-		b, tmpPath := backend.NewDefaultTmpBackend()
-		s := NewStore(b, &lease.FakeLessor{}, nil)
-		tx := s.b.BatchTx()
-
-		tx.Lock()
-		ibytes := newRevBytes()
-		for _, rev := range revs {
-			revToBytes(rev, ibytes)
-			tx.UnsafePut(keyBucketName, ibytes, []byte("bar"))
-		}
-		tx.Unlock()
-
-		s.scheduleCompaction(tt.rev, tt.keep)
-
-		tx.Lock()
-		for _, rev := range tt.wrevs {
-			revToBytes(rev, ibytes)
-			keys, _ := tx.UnsafeRange(keyBucketName, ibytes, nil, 0)
-			if len(keys) != 1 {
-				t.Errorf("#%d: range on %v = %d, want 1", i, rev, len(keys))
-			}
-		}
-		_, vals := tx.UnsafeRange(metaBucketName, finishedCompactKeyName, nil, 0)
-		revToBytes(revision{main: tt.rev}, ibytes)
-		if w := [][]byte{ibytes}; !reflect.DeepEqual(vals, w) {
-			t.Errorf("#%d: vals on %v = %+v, want %+v", i, finishedCompactKeyName, vals, w)
-		}
-		tx.Unlock()
-
-		cleanup(s, b, tmpPath)
-	}
-}
diff --git a/storage/kvstore_test.go b/storage/kvstore_test.go
deleted file mode 100644
index 46aebbb..0000000
--- a/storage/kvstore_test.go
+++ /dev/null
@@ -1,669 +0,0 @@
-// Copyright 2015 CoreOS, Inc.
-//
-// Licensed under the Apache License, Version 2.0 (the "License");
-// you may not use this file except in compliance with the License.
-// You may obtain a copy of the License at
-//
-//     http://www.apache.org/licenses/LICENSE-2.0
-//
-// Unless required by applicable law or agreed to in writing, software
-// distributed under the License is distributed on an "AS IS" BASIS,
-// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-// See the License for the specific language governing permissions and
-// limitations under the License.
-
-package storage
-
-import (
-	"crypto/rand"
-	"encoding/binary"
-	"math"
-	"os"
-	"reflect"
-	"testing"
-	"time"
-
-	"github.com/coreos/etcd/lease"
-	"github.com/coreos/etcd/pkg/schedule"
-	"github.com/coreos/etcd/pkg/testutil"
-	"github.com/coreos/etcd/storage/backend"
-	"github.com/coreos/etcd/storage/storagepb"
-)
-
-func TestStoreRev(t *testing.T) {
-	b, tmpPath := backend.NewDefaultTmpBackend()
-	s := NewStore(b, &lease.FakeLessor{}, nil)
-	defer s.Close()
-	defer os.Remove(tmpPath)
-
-	for i := 1; i <= 3; i++ {
-		s.Put([]byte("foo"), []byte("bar"), lease.NoLease)
-		if r := s.Rev(); r != int64(i+1) {
-			t.Errorf("#%d: rev = %d, want %d", i, r, i+1)
-		}
-	}
-}
-
-func TestStorePut(t *testing.T) {
-	kv := storagepb.KeyValue{
-		Key:            []byte("foo"),
-		Value:          []byte("bar"),
-		CreateRevision: 1,
-		ModRevision:    2,
-		Version:        1,
-	}
-	kvb, err := kv.Marshal()
-	if err != nil {
-		t.Fatal(err)
-	}
-
-	tests := []struct {
-		rev revision
-		r   indexGetResp
-		rr  *rangeResp
-
-		wrev    revision
-		wkey    []byte
-		wkv     storagepb.KeyValue
-		wputrev revision
-	}{
-		{
-			revision{1, 0},
-			indexGetResp{revision{}, revision{}, 0, ErrRevisionNotFound},
-			nil,
-
-			revision{1, 1},
-			newTestKeyBytes(revision{2, 0}, false),
-			storagepb.KeyValue{
-				Key:            []byte("foo"),
-				Value:          []byte("bar"),
-				CreateRevision: 2,
-				ModRevision:    2,
-				Version:        1,
-				Lease:          1,
-			},
-			revision{2, 0},
-		},
-		{
-			revision{1, 1},
-			indexGetResp{revision{2, 0}, revision{2, 0}, 1, nil},
-			&rangeResp{[][]byte{newTestKeyBytes(revision{2, 1}, false)}, [][]byte{kvb}},
-
-			revision{1, 2},
-			newTestKeyBytes(revision{2, 1}, false),
-			storagepb.KeyValue{
-				Key:            []byte("foo"),
-				Value:          []byte("bar"),
-				CreateRevision: 2,
-				ModRevision:    2,
-				Version:        2,
-				Lease:          2,
-			},
-			revision{2, 1},
-		},
-		{
-			revision{2, 0},
-			indexGetResp{revision{2, 1}, revision{2, 0}, 2, nil},
-			&rangeResp{[][]byte{newTestKeyBytes(revision{2, 1}, false)}, [][]byte{kvb}},
-
-			revision{2, 1},
-			newTestKeyBytes(revision{3, 0}, false),
-			storagepb.KeyValue{
-				Key:            []byte("foo"),
-				Value:          []byte("bar"),
-				CreateRevision: 2,
-				ModRevision:    3,
-				Version:        3,
-				Lease:          3,
-			},
-			revision{3, 0},
-		},
-	}
-	for i, tt := range tests {
-		s := newFakeStore()
-		b := s.b.(*fakeBackend)
-		fi := s.kvindex.(*fakeIndex)
-
-		s.currentRev = tt.rev
-		s.tx = b.BatchTx()
-		fi.indexGetRespc <- tt.r
-		if tt.rr != nil {
-			b.tx.rangeRespc <- *tt.rr
-		}
-
-		s.put([]byte("foo"), []byte("bar"), lease.LeaseID(i+1))
-
-		data, err := tt.wkv.Marshal()
-		if err != nil {
-			t.Errorf("#%d: marshal err = %v, want nil", i, err)
-		}
-
-		wact := []testutil.Action{
-			{"seqput", []interface{}{keyBucketName, tt.wkey, data}},
-		}
-
-		if tt.rr != nil {
-			wact = []testutil.Action{
-				{"range", []interface{}{keyBucketName, newTestKeyBytes(tt.r.rev, false), []byte(nil), int64(0)}},
-				{"seqput", []interface{}{keyBucketName, tt.wkey, data}},
-			}
-		}
-
-		if g := b.tx.Action(); !reflect.DeepEqual(g, wact) {
-			t.Errorf("#%d: tx action = %+v, want %+v", i, g, wact)
-		}
-		wact = []testutil.Action{
-			{"get", []interface{}{[]byte("foo"), tt.wputrev.main}},
-			{"put", []interface{}{[]byte("foo"), tt.wputrev}},
-		}
-		if g := fi.Action(); !reflect.DeepEqual(g, wact) {
-			t.Errorf("#%d: index action = %+v, want %+v", i, g, wact)
-		}
-		if s.currentRev != tt.wrev {
-			t.Errorf("#%d: rev = %+v, want %+v", i, s.currentRev, tt.wrev)
-		}
-
-		s.Close()
-	}
-}
-
-func TestStoreRange(t *testing.T) {
-	key := newTestKeyBytes(revision{2, 0}, false)
-	kv := storagepb.KeyValue{
-		Key:            []byte("foo"),
-		Value:          []byte("bar"),
-		CreateRevision: 1,
-		ModRevision:    2,
-		Version:        1,
-	}
-	kvb, err := kv.Marshal()
-	if err != nil {
-		t.Fatal(err)
-	}
-	currev := revision{1, 1}
-	wrev := int64(2)
-
-	tests := []struct {
-		idxr indexRangeResp
-		r    rangeResp
-	}{
-		{
-			indexRangeResp{[][]byte{[]byte("foo")}, []revision{{2, 0}}},
-			rangeResp{[][]byte{key}, [][]byte{kvb}},
-		},
-		{
-			indexRangeResp{[][]byte{[]byte("foo"), []byte("foo1")}, []revision{{2, 0}, {3, 0}}},
-			rangeResp{[][]byte{key}, [][]byte{kvb}},
-		},
-	}
-	for i, tt := range tests {
-		s := newFakeStore()
-		b := s.b.(*fakeBackend)
-		fi := s.kvindex.(*fakeIndex)
-
-		s.currentRev = currev
-		s.tx = b.BatchTx()
-		b.tx.rangeRespc <- tt.r
-		fi.indexRangeRespc <- tt.idxr
-
-		kvs, rev, err := s.rangeKeys([]byte("foo"), []byte("goo"), 1, 0)
-		if err != nil {
-			t.Errorf("#%d: err = %v, want nil", i, err)
-		}
-		if w := []storagepb.KeyValue{kv}; !reflect.DeepEqual(kvs, w) {
-			t.Errorf("#%d: kvs = %+v, want %+v", i, kvs, w)
-		}
-		if rev != wrev {
-			t.Errorf("#%d: rev = %d, want %d", i, rev, wrev)
-		}
-
-		wstart, wend := revBytesRange(tt.idxr.revs[0])
-		wact := []testutil.Action{
-			{"range", []interface{}{keyBucketName, wstart, wend, int64(0)}},
-		}
-		if g := b.tx.Action(); !reflect.DeepEqual(g, wact) {
-			t.Errorf("#%d: tx action = %+v, want %+v", i, g, wact)
-		}
-		wact = []testutil.Action{
-			{"range", []interface{}{[]byte("foo"), []byte("goo"), wrev}},
-		}
-		if g := fi.Action(); !reflect.DeepEqual(g, wact) {
-			t.Errorf("#%d: index action = %+v, want %+v", i, g, wact)
-		}
-		if s.currentRev != currev {
-			t.Errorf("#%d: current rev = %+v, want %+v", i, s.currentRev, currev)
-		}
-
-		s.Close()
-	}
-}
-
-func TestStoreDeleteRange(t *testing.T) {
-	key := newTestKeyBytes(revision{2, 0}, false)
-	kv := storagepb.KeyValue{
-		Key:            []byte("foo"),
-		Value:          []byte("bar"),
-		CreateRevision: 1,
-		ModRevision:    2,
-		Version:        1,
-	}
-	kvb, err := kv.Marshal()
-	if err != nil {
-		t.Fatal(err)
-	}
-
-	tests := []struct {
-		rev revision
-		r   indexRangeResp
-		rr  rangeResp
-
-		wkey    []byte
-		wrev    revision
-		wrrev   int64
-		wdelrev revision
-	}{
-		{
-			revision{2, 0},
-			indexRangeResp{[][]byte{[]byte("foo")}, []revision{{2, 0}}},
-			rangeResp{[][]byte{key}, [][]byte{kvb}},
-
-			newTestKeyBytes(revision{3, 0}, true),
-			revision{2, 1},
-			2,
-			revision{3, 0},
-		},
-		{
-			revision{2, 1},
-			indexRangeResp{[][]byte{[]byte("foo")}, []revision{{2, 0}}},
-			rangeResp{[][]byte{key}, [][]byte{kvb}},
-
-			newTestKeyBytes(revision{3, 1}, true),
-			revision{2, 2},
-			3,
-			revision{3, 1},
-		},
-	}
-	for i, tt := range tests {
-		s := newFakeStore()
-		b := s.b.(*fakeBackend)
-		fi := s.kvindex.(*fakeIndex)
-
-		s.currentRev = tt.rev
-		s.tx = b.BatchTx()
-		fi.indexRangeRespc <- tt.r
-		b.tx.rangeRespc <- tt.rr
-
-		n := s.deleteRange([]byte("foo"), []byte("goo"))
-		if n != 1 {
-			t.Errorf("#%d: n = %d, want 1", i, n)
-		}
-
-		data, err := (&storagepb.KeyValue{
-			Key: []byte("foo"),
-		}).Marshal()
-		if err != nil {
-			t.Errorf("#%d: marshal err = %v, want nil", i, err)
-		}
-		wact := []testutil.Action{
-			{"seqput", []interface{}{keyBucketName, tt.wkey, data}},
-			{"range", []interface{}{keyBucketName, newTestKeyBytes(revision{2, 0}, false), []byte(nil), int64(0)}},
-		}
-		if g := b.tx.Action(); !reflect.DeepEqual(g, wact) {
-			t.Errorf("#%d: tx action = %+v, want %+v", i, g, wact)
-		}
-		wact = []testutil.Action{
-			{"range", []interface{}{[]byte("foo"), []byte("goo"), tt.wrrev}},
-			{"tombstone", []interface{}{[]byte("foo"), tt.wdelrev}},
-		}
-		if g := fi.Action(); !reflect.DeepEqual(g, wact) {
-			t.Errorf("#%d: index action = %+v, want %+v", i, g, wact)
-		}
-		if s.currentRev != tt.wrev {
-			t.Errorf("#%d: rev = %+v, want %+v", i, s.currentRev, tt.wrev)
-		}
-	}
-}
-
-func TestStoreCompact(t *testing.T) {
-	s := newFakeStore()
-	defer s.Close()
-	b := s.b.(*fakeBackend)
-	fi := s.kvindex.(*fakeIndex)
-
-	s.currentRev = revision{3, 0}
-	fi.indexCompactRespc <- map[revision]struct{}{revision{1, 0}: {}}
-	key1 := newTestKeyBytes(revision{1, 0}, false)
-	key2 := newTestKeyBytes(revision{2, 0}, false)
-	b.tx.rangeRespc <- rangeResp{[][]byte{key1, key2}, nil}
-
-	s.Compact(3)
-	s.fifoSched.WaitFinish(1)
-
-	if s.compactMainRev != 3 {
-		t.Errorf("compact main rev = %d, want 3", s.compactMainRev)
-	}
-	end := make([]byte, 8)
-	binary.BigEndian.PutUint64(end, uint64(4))
-	wact := []testutil.Action{
-		{"put", []interface{}{metaBucketName, scheduledCompactKeyName, newTestRevBytes(revision{3, 0})}},
-		{"range", []interface{}{keyBucketName, make([]byte, 17), end, int64(10000)}},
-		{"delete", []interface{}{keyBucketName, key2}},
-		{"put", []interface{}{metaBucketName, finishedCompactKeyName, newTestRevBytes(revision{3, 0})}},
-	}
-	if g := b.tx.Action(); !reflect.DeepEqual(g, wact) {
-		t.Errorf("tx actions = %+v, want %+v", g, wact)
-	}
-	wact = []testutil.Action{
-		{"compact", []interface{}{int64(3)}},
-	}
-	if g := fi.Action(); !reflect.DeepEqual(g, wact) {
-		t.Errorf("index action = %+v, want %+v", g, wact)
-	}
-}
-
-func TestStoreRestore(t *testing.T) {
-	s := newFakeStore()
-	b := s.b.(*fakeBackend)
-	fi := s.kvindex.(*fakeIndex)
-
-	putkey := newTestKeyBytes(revision{3, 0}, false)
-	putkv := storagepb.KeyValue{
-		Key:            []byte("foo"),
-		Value:          []byte("bar"),
-		CreateRevision: 4,
-		ModRevision:    4,
-		Version:        1,
-	}
-	putkvb, err := putkv.Marshal()
-	if err != nil {
-		t.Fatal(err)
-	}
-	delkey := newTestKeyBytes(revision{5, 0}, true)
-	delkv := storagepb.KeyValue{
-		Key: []byte("foo"),
-	}
-	delkvb, err := delkv.Marshal()
-	if err != nil {
-		t.Fatal(err)
-	}
-	b.tx.rangeRespc <- rangeResp{[][]byte{finishedCompactKeyName}, [][]byte{newTestRevBytes(revision{3, 0})}}
-	b.tx.rangeRespc <- rangeResp{[][]byte{putkey, delkey}, [][]byte{putkvb, delkvb}}
-	b.tx.rangeRespc <- rangeResp{[][]byte{scheduledCompactKeyName}, [][]byte{newTestRevBytes(revision{3, 0})}}
-
-	s.restore()
-
-	if s.compactMainRev != 3 {
-		t.Errorf("compact rev = %d, want 5", s.compactMainRev)
-	}
-	wrev := revision{5, 0}
-	if !reflect.DeepEqual(s.currentRev, wrev) {
-		t.Errorf("current rev = %v, want %v", s.currentRev, wrev)
-	}
-	wact := []testutil.Action{
-		{"range", []interface{}{metaBucketName, finishedCompactKeyName, []byte(nil), int64(0)}},
-		{"range", []interface{}{keyBucketName, newTestRevBytes(revision{1, 0}), newTestRevBytes(revision{math.MaxInt64, math.MaxInt64}), int64(0)}},
-		{"range", []interface{}{metaBucketName, scheduledCompactKeyName, []byte(nil), int64(0)}},
-	}
-	if g := b.tx.Action(); !reflect.DeepEqual(g, wact) {
-		t.Errorf("tx actions = %+v, want %+v", g, wact)
-	}
-	wact = []testutil.Action{
-		{"restore", []interface{}{[]byte("foo"), revision{4, 0}, revision{3, 0}, int64(1)}},
-		{"tombstone", []interface{}{[]byte("foo"), revision{5, 0}}},
-	}
-	if g := fi.Action(); !reflect.DeepEqual(g, wact) {
-		t.Errorf("index action = %+v, want %+v", g, wact)
-	}
-}
-
-func TestRestoreContinueUnfinishedCompaction(t *testing.T) {
-	b, tmpPath := backend.NewDefaultTmpBackend()
-	s0 := NewStore(b, &lease.FakeLessor{}, nil)
-	defer os.Remove(tmpPath)
-
-	s0.Put([]byte("foo"), []byte("bar"), lease.NoLease)
-	s0.Put([]byte("foo"), []byte("bar1"), lease.NoLease)
-	s0.Put([]byte("foo"), []byte("bar2"), lease.NoLease)
-
-	// write scheduled compaction, but not do compaction
-	rbytes := newRevBytes()
-	revToBytes(revision{main: 2}, rbytes)
-	tx := s0.b.BatchTx()
-	tx.Lock()
-	tx.UnsafePut(metaBucketName, scheduledCompactKeyName, rbytes)
-	tx.Unlock()
-
-	s0.Close()
-
-	s1 := NewStore(b, &lease.FakeLessor{}, nil)
-
-	// wait for scheduled compaction to be finished
-	time.Sleep(100 * time.Millisecond)
-
-	if _, _, err := s1.Range([]byte("foo"), nil, 0, 1); err != ErrCompacted {
-		t.Errorf("range on compacted rev error = %v, want %v", err, ErrCompacted)
-	}
-	// check the key in backend is deleted
-	revbytes := newRevBytes()
-	revToBytes(revision{main: 1}, revbytes)
-
-	// The disk compaction is done asynchronously and requires more time on slow disk.
-	// try 5 times for CI with slow IO.
-	for i := 0; i < 5; i++ {
-		tx = s1.b.BatchTx()
-		tx.Lock()
-		ks, _ := tx.UnsafeRange(keyBucketName, revbytes, nil, 0)
-		tx.Unlock()
-		if len(ks) != 0 {
-			time.Sleep(100 * time.Millisecond)
-			continue
-		}
-		return
-	}
-
-	t.Errorf("key for rev %+v still exists, want deleted", bytesToRev(revbytes))
-}
-
-func TestTxnPut(t *testing.T) {
-	// assign arbitrary size
-	bytesN := 30
-	sliceN := 100
-	keys := createBytesSlice(bytesN, sliceN)
-	vals := createBytesSlice(bytesN, sliceN)
-
-	b, tmpPath := backend.NewDefaultTmpBackend()
-	s := NewStore(b, &lease.FakeLessor{}, nil)
-	defer cleanup(s, b, tmpPath)
-
-	for i := 0; i < sliceN; i++ {
-		id := s.TxnBegin()
-		base := int64(i + 2)
-
-		rev, err := s.TxnPut(id, keys[i], vals[i], lease.NoLease)
-		if err != nil {
-			t.Error("txn put error")
-		}
-		if rev != base {
-			t.Errorf("#%d: rev = %d, want %d", i, rev, base)
-		}
-
-		s.TxnEnd(id)
-	}
-}
-
-func TestTxnBlockBackendForceCommit(t *testing.T) {
-	b, tmpPath := backend.NewDefaultTmpBackend()
-	s := NewStore(b, &lease.FakeLessor{}, nil)
-	defer os.Remove(tmpPath)
-
-	id := s.TxnBegin()
-
-	done := make(chan struct{})
-	go func() {
-		s.b.ForceCommit()
-		done <- struct{}{}
-	}()
-	select {
-	case <-done:
-		t.Fatalf("failed to block ForceCommit")
-	case <-time.After(100 * time.Millisecond):
-	}
-
-	s.TxnEnd(id)
-	select {
-	case <-done:
-	case <-time.After(5 * time.Second): // wait 5 seconds for CI with slow IO
-		testutil.FatalStack(t, "failed to execute ForceCommit")
-	}
-}
-
-// TODO: test attach key to lessor
-
-func newTestRevBytes(rev revision) []byte {
-	bytes := newRevBytes()
-	revToBytes(rev, bytes)
-	return bytes
-}
-
-func newTestKeyBytes(rev revision, tombstone bool) []byte {
-	bytes := newRevBytes()
-	revToBytes(rev, bytes)
-	if tombstone {
-		bytes = appendMarkTombstone(bytes)
-	}
-	return bytes
-}
-
-func newFakeStore() *store {
-	b := &fakeBackend{&fakeBatchTx{
-		Recorder:   &testutil.RecorderBuffered{},
-		rangeRespc: make(chan rangeResp, 5)}}
-	fi := &fakeIndex{
-		Recorder:              &testutil.RecorderBuffered{},
-		indexGetRespc:         make(chan indexGetResp, 1),
-		indexRangeRespc:       make(chan indexRangeResp, 1),
-		indexRangeEventsRespc: make(chan indexRangeEventsResp, 1),
-		indexCompactRespc:     make(chan map[revision]struct{}, 1),
-	}
-	return &store{
-		b:              b,
-		le:             &lease.FakeLessor{},
-		kvindex:        fi,
-		currentRev:     revision{},
-		compactMainRev: -1,
-		fifoSched:      schedule.NewFIFOScheduler(),
-		stopc:          make(chan struct{}),
-	}
-}
-
-type rangeResp struct {
-	keys [][]byte
-	vals [][]byte
-}
-
-type fakeBatchTx struct {
-	testutil.Recorder
-	rangeRespc chan rangeResp
-}
-
-func (b *fakeBatchTx) Lock()                          {}
-func (b *fakeBatchTx) Unlock()                        {}
-func (b *fakeBatchTx) UnsafeCreateBucket(name []byte) {}
-func (b *fakeBatchTx) UnsafePut(bucketName []byte, key []byte, value []byte) {
-	b.Recorder.Record(testutil.Action{Name: "put", Params: []interface{}{bucketName, key, value}})
-}
-func (b *fakeBatchTx) UnsafeSeqPut(bucketName []byte, key []byte, value []byte) {
-	b.Recorder.Record(testutil.Action{Name: "seqput", Params: []interface{}{bucketName, key, value}})
-}
-func (b *fakeBatchTx) UnsafeRange(bucketName []byte, key, endKey []byte, limit int64) (keys [][]byte, vals [][]byte) {
-	b.Recorder.Record(testutil.Action{Name: "range", Params: []interface{}{bucketName, key, endKey, limit}})
-	r := <-b.rangeRespc
-	return r.keys, r.vals
-}
-func (b *fakeBatchTx) UnsafeDelete(bucketName []byte, key []byte) {
-	b.Recorder.Record(testutil.Action{Name: "delete", Params: []interface{}{bucketName, key}})
-}
-func (b *fakeBatchTx) UnsafeForEach(bucketName []byte, visitor func(k, v []byte) error) error {
-	return nil
-}
-func (b *fakeBatchTx) Commit()        {}
-func (b *fakeBatchTx) CommitAndStop() {}
-
-type fakeBackend struct {
-	tx *fakeBatchTx
-}
-
-func (b *fakeBackend) BatchTx() backend.BatchTx   { return b.tx }
-func (b *fakeBackend) Hash() (uint32, error)      { return 0, nil }
-func (b *fakeBackend) Size() int64                { return 0 }
-func (b *fakeBackend) Snapshot() backend.Snapshot { return nil }
-func (b *fakeBackend) ForceCommit()               {}
-func (b *fakeBackend) Defrag() error              { return nil }
-func (b *fakeBackend) Close() error               { return nil }
-
-type indexGetResp struct {
-	rev     revision
-	created revision
-	ver     int64
-	err     error
-}
-
-type indexRangeResp struct {
-	keys [][]byte
-	revs []revision
-}
-
-type indexRangeEventsResp struct {
-	revs []revision
-}
-
-type fakeIndex struct {
-	testutil.Recorder
-	indexGetRespc         chan indexGetResp
-	indexRangeRespc       chan indexRangeResp
-	indexRangeEventsRespc chan indexRangeEventsResp
-	indexCompactRespc     chan map[revision]struct{}
-}
-
-func (i *fakeIndex) Get(key []byte, atRev int64) (rev, created revision, ver int64, err error) {
-	i.Recorder.Record(testutil.Action{Name: "get", Params: []interface{}{key, atRev}})
-	r := <-i.indexGetRespc
-	return r.rev, r.created, r.ver, r.err
-}
-func (i *fakeIndex) Range(key, end []byte, atRev int64) ([][]byte, []revision) {
-	i.Recorder.Record(testutil.Action{Name: "range", Params: []interface{}{key, end, atRev}})
-	r := <-i.indexRangeRespc
-	return r.keys, r.revs
-}
-func (i *fakeIndex) Put(key []byte, rev revision) {
-	i.Recorder.Record(testutil.Action{Name: "put", Params: []interface{}{key, rev}})
-}
-func (i *fakeIndex) Restore(key []byte, created, modified revision, ver int64) {
-	i.Recorder.Record(testutil.Action{Name: "restore", Params: []interface{}{key, created, modified, ver}})
-}
-func (i *fakeIndex) Tombstone(key []byte, rev revision) error {
-	i.Recorder.Record(testutil.Action{Name: "tombstone", Params: []interface{}{key, rev}})
-	return nil
-}
-func (i *fakeIndex) RangeSince(key, end []byte, rev int64) []revision {
-	i.Recorder.Record(testutil.Action{Name: "rangeEvents", Params: []interface{}{key, end, rev}})
-	r := <-i.indexRangeEventsRespc
-	return r.revs
-}
-func (i *fakeIndex) Compact(rev int64) map[revision]struct{} {
-	i.Recorder.Record(testutil.Action{Name: "compact", Params: []interface{}{rev}})
-	return <-i.indexCompactRespc
-}
-func (i *fakeIndex) Equal(b index) bool { return false }
-
-func createBytesSlice(bytesN, sliceN int) [][]byte {
-	rs := [][]byte{}
-	for len(rs) != sliceN {
-		v := make([]byte, bytesN)
-		if _, err := rand.Read(v); err != nil {
-			panic(err)
-		}
-		rs = append(rs, v)
-	}
-	return rs
-}
diff --git a/storage/metrics.go b/storage/metrics.go
deleted file mode 100644
index 24b45a7..0000000
--- a/storage/metrics.go
+++ /dev/null
@@ -1,163 +0,0 @@
-// Copyright 2015 CoreOS, Inc.
-//
-// Licensed under the Apache License, Version 2.0 (the "License");
-// you may not use this file except in compliance with the License.
-// You may obtain a copy of the License at
-//
-//     http://www.apache.org/licenses/LICENSE-2.0
-//
-// Unless required by applicable law or agreed to in writing, software
-// distributed under the License is distributed on an "AS IS" BASIS,
-// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-// See the License for the specific language governing permissions and
-// limitations under the License.
-
-package storage
-
-import (
-	"github.com/prometheus/client_golang/prometheus"
-)
-
-var (
-	rangeCounter = prometheus.NewCounter(
-		prometheus.CounterOpts{
-			Namespace: "etcd",
-			Subsystem: "storage",
-			Name:      "range_total",
-			Help:      "Total number of ranges seen by this member.",
-		})
-
-	putCounter = prometheus.NewCounter(
-		prometheus.CounterOpts{
-			Namespace: "etcd",
-			Subsystem: "storage",
-			Name:      "put_total",
-			Help:      "Total number of puts seen by this member.",
-		})
-
-	deleteCounter = prometheus.NewCounter(
-		prometheus.CounterOpts{
-			Namespace: "etcd",
-			Subsystem: "storage",
-			Name:      "delete_total",
-			Help:      "Total number of deletes seen by this member.",
-		})
-
-	txnCounter = prometheus.NewCounter(
-		prometheus.CounterOpts{
-			Namespace: "etcd",
-			Subsystem: "storage",
-			Name:      "txn_total",
-			Help:      "Total number of txns seen by this member.",
-		})
-
-	keysGauge = prometheus.NewGauge(
-		prometheus.GaugeOpts{
-			Namespace: "etcd",
-			Subsystem: "storage",
-			Name:      "keys_total",
-			Help:      "Total number of keys.",
-		})
-
-	watchStreamGauge = prometheus.NewGauge(
-		prometheus.GaugeOpts{
-			Namespace: "etcd",
-			Subsystem: "storage",
-			Name:      "watch_stream_total",
-			Help:      "Total number of watch streams.",
-		})
-
-	watcherGauge = prometheus.NewGauge(
-		prometheus.GaugeOpts{
-			Namespace: "etcd",
-			Subsystem: "storage",
-			Name:      "watcher_total",
-			Help:      "Total number of watchers.",
-		})
-
-	slowWatcherGauge = prometheus.NewGauge(
-		prometheus.GaugeOpts{
-			Namespace: "etcd",
-			Subsystem: "storage",
-			Name:      "slow_watcher_total",
-			Help:      "Total number of unsynced slow watchers.",
-		})
-
-	totalEventsCounter = prometheus.NewCounter(
-		prometheus.CounterOpts{
-			Namespace: "etcd",
-			Subsystem: "storage",
-			Name:      "events_total",
-			Help:      "Total number of events sent by this member.",
-		})
-
-	pendingEventsGauge = prometheus.NewGauge(
-		prometheus.GaugeOpts{
-			Namespace: "etcd",
-			Subsystem: "storage",
-			Name:      "pending_events_total",
-			Help:      "Total number of pending events to be sent.",
-		})
-
-	indexCompactionPauseDurations = prometheus.NewHistogram(
-		prometheus.HistogramOpts{
-			Namespace: "etcd",
-			Subsystem: "storage",
-			Name:      "index_compaction_pause_duration_milliseconds",
-			Help:      "Bucketed histogram of index compaction pause duration.",
-			// 0.5ms -> 1second
-			Buckets: prometheus.ExponentialBuckets(0.5, 2, 12),
-		})
-
-	dbCompactionPauseDurations = prometheus.NewHistogram(
-		prometheus.HistogramOpts{
-			Namespace: "etcd",
-			Subsystem: "storage",
-			Name:      "db_compaction_pause_duration_milliseconds",
-			Help:      "Bucketed histogram of db compaction pause duration.",
-			// 1ms -> 4second
-			Buckets: prometheus.ExponentialBuckets(1, 2, 13),
-		})
-
-	dbCompactionTotalDurations = prometheus.NewHistogram(
-		prometheus.HistogramOpts{
-			Namespace: "etcd",
-			Subsystem: "storage",
-			Name:      "db_compaction_total_duration_milliseconds",
-			Help:      "Bucketed histogram of db compaction total duration.",
-			// 100ms -> 800second
-			Buckets: prometheus.ExponentialBuckets(100, 2, 14),
-		})
-
-	dbTotalSize = prometheus.NewGauge(prometheus.GaugeOpts{
-		Namespace: "etcd",
-		Subsystem: "storage",
-		Name:      "db_total_size_in_bytes",
-		Help:      "Total size of the underlying database in bytes.",
-	})
-)
-
-func init() {
-	prometheus.MustRegister(rangeCounter)
-	prometheus.MustRegister(putCounter)
-	prometheus.MustRegister(deleteCounter)
-	prometheus.MustRegister(txnCounter)
-	prometheus.MustRegister(keysGauge)
-	prometheus.MustRegister(watchStreamGauge)
-	prometheus.MustRegister(watcherGauge)
-	prometheus.MustRegister(slowWatcherGauge)
-	prometheus.MustRegister(totalEventsCounter)
-	prometheus.MustRegister(pendingEventsGauge)
-	prometheus.MustRegister(indexCompactionPauseDurations)
-	prometheus.MustRegister(dbCompactionPauseDurations)
-	prometheus.MustRegister(dbCompactionTotalDurations)
-	prometheus.MustRegister(dbTotalSize)
-}
-
-// ReportEventReceived reports that an event is received.
-// This function should be called when the external systems received an
-// event from storage.Watcher.
-func ReportEventReceived() {
-	pendingEventsGauge.Dec()
-	totalEventsCounter.Inc()
-}
diff --git a/storage/revision.go b/storage/revision.go
deleted file mode 100644
index d31bcbb..0000000
--- a/storage/revision.go
+++ /dev/null
@@ -1,67 +0,0 @@
-// Copyright 2015 CoreOS, Inc.
-//
-// Licensed under the Apache License, Version 2.0 (the "License");
-// you may not use this file except in compliance with the License.
-// You may obtain a copy of the License at
-//
-//     http://www.apache.org/licenses/LICENSE-2.0
-//
-// Unless required by applicable law or agreed to in writing, software
-// distributed under the License is distributed on an "AS IS" BASIS,
-// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-// See the License for the specific language governing permissions and
-// limitations under the License.
-
-package storage
-
-import "encoding/binary"
-
-// revBytesLen is the byte length of a normal revision.
-// First 8 bytes is the revision.main in big-endian format. The 9th byte
-// is a '_'. The last 8 bytes is the revision.sub in big-endian format.
-const revBytesLen = 8 + 1 + 8
-
-// A revision indicates modification of the key-value space.
-// The set of changes that share same main revision changes the key-value space atomically.
-type revision struct {
-	// main is the main revision of a set of changes that happen atomically.
-	main int64
-
-	// sub is the the sub revision of a change in a set of changes that happen
-	// atomically. Each change has different increasing sub revision in that
-	// set.
-	sub int64
-}
-
-func (a revision) GreaterThan(b revision) bool {
-	if a.main > b.main {
-		return true
-	}
-	if a.main < b.main {
-		return false
-	}
-	return a.sub > b.sub
-}
-
-func newRevBytes() []byte {
-	return make([]byte, revBytesLen, markedRevBytesLen)
-}
-
-func revToBytes(rev revision, bytes []byte) {
-	binary.BigEndian.PutUint64(bytes, uint64(rev.main))
-	bytes[8] = '_'
-	binary.BigEndian.PutUint64(bytes[9:], uint64(rev.sub))
-}
-
-func bytesToRev(bytes []byte) revision {
-	return revision{
-		main: int64(binary.BigEndian.Uint64(bytes[0:8])),
-		sub:  int64(binary.BigEndian.Uint64(bytes[9:])),
-	}
-}
-
-type revisions []revision
-
-func (a revisions) Len() int           { return len(a) }
-func (a revisions) Less(i, j int) bool { return a[j].GreaterThan(a[i]) }
-func (a revisions) Swap(i, j int)      { a[i], a[j] = a[j], a[i] }
diff --git a/storage/revision_test.go b/storage/revision_test.go
deleted file mode 100644
index eee5fc7..0000000
--- a/storage/revision_test.go
+++ /dev/null
@@ -1,53 +0,0 @@
-// Copyright 2015 CoreOS, Inc.
-//
-// Licensed under the Apache License, Version 2.0 (the "License");
-// you may not use this file except in compliance with the License.
-// You may obtain a copy of the License at
-//
-//     http://www.apache.org/licenses/LICENSE-2.0
-//
-// Unless required by applicable law or agreed to in writing, software
-// distributed under the License is distributed on an "AS IS" BASIS,
-// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-// See the License for the specific language governing permissions and
-// limitations under the License.
-
-package storage
-
-import (
-	"bytes"
-	"math"
-	"reflect"
-	"testing"
-)
-
-// TestRevision tests that revision could be encoded to and decoded from
-// bytes slice. Moreover, the lexicographical order of its byte slice representation
-// follows the order of (main, sub).
-func TestRevision(t *testing.T) {
-	tests := []revision{
-		// order in (main, sub)
-		{},
-		{main: 1, sub: 0},
-		{main: 1, sub: 1},
-		{main: 2, sub: 0},
-		{main: math.MaxInt64, sub: math.MaxInt64},
-	}
-
-	bs := make([][]byte, len(tests))
-	for i, tt := range tests {
-		b := newRevBytes()
-		revToBytes(tt, b)
-		bs[i] = b
-
-		if grev := bytesToRev(b); !reflect.DeepEqual(grev, tt) {
-			t.Errorf("#%d: revision = %+v, want %+v", i, grev, tt)
-		}
-	}
-
-	for i := 0; i < len(tests)-1; i++ {
-		if bytes.Compare(bs[i], bs[i+1]) >= 0 {
-			t.Errorf("#%d: %v (%+v) should be smaller than %v (%+v)", i, bs[i], tests[i], bs[i+1], tests[i+1])
-		}
-	}
-}
diff --git a/storage/storagepb/kv.pb.go b/storage/storagepb/kv.pb.go
deleted file mode 100644
index b26dcf1..0000000
--- a/storage/storagepb/kv.pb.go
+++ /dev/null
@@ -1,684 +0,0 @@
-// Code generated by protoc-gen-gogo.
-// source: kv.proto
-// DO NOT EDIT!
-
-/*
-	Package storagepb is a generated protocol buffer package.
-
-	It is generated from these files:
-		kv.proto
-
-	It has these top-level messages:
-		KeyValue
-		Event
-*/
-package storagepb
-
-import (
-	"fmt"
-
-	proto "github.com/gogo/protobuf/proto"
-
-	math "math"
-)
-
-import io "io"
-
-// Reference imports to suppress errors if they are not otherwise used.
-var _ = proto.Marshal
-var _ = fmt.Errorf
-var _ = math.Inf
-
-// This is a compile-time assertion to ensure that this generated file
-// is compatible with the proto package it is being compiled against.
-const _ = proto.GoGoProtoPackageIsVersion1
-
-type Event_EventType int32
-
-const (
-	PUT    Event_EventType = 0
-	DELETE Event_EventType = 1
-	EXPIRE Event_EventType = 2
-)
-
-var Event_EventType_name = map[int32]string{
-	0: "PUT",
-	1: "DELETE",
-	2: "EXPIRE",
-}
-var Event_EventType_value = map[string]int32{
-	"PUT":    0,
-	"DELETE": 1,
-	"EXPIRE": 2,
-}
-
-func (x Event_EventType) String() string {
-	return proto.EnumName(Event_EventType_name, int32(x))
-}
-func (Event_EventType) EnumDescriptor() ([]byte, []int) { return fileDescriptorKv, []int{1, 0} }
-
-type KeyValue struct {
-	// key is the key in bytes. An empty key is not allowed.
-	Key []byte `protobuf:"bytes,1,opt,name=key,proto3" json:"key,omitempty"`
-	// create_revision is the revision of last creation on this key.
-	CreateRevision int64 `protobuf:"varint,2,opt,name=create_revision,json=createRevision,proto3" json:"create_revision,omitempty"`
-	// mod_revision is the revision of last modification on this key.
-	ModRevision int64 `protobuf:"varint,3,opt,name=mod_revision,json=modRevision,proto3" json:"mod_revision,omitempty"`
-	// version is the version of the key. A deletion resets
-	// the version to zero and any modification of the key
-	// increases its version.
-	Version int64 `protobuf:"varint,4,opt,name=version,proto3" json:"version,omitempty"`
-	// value is the value held by the key, in bytes.
-	Value []byte `protobuf:"bytes,5,opt,name=value,proto3" json:"value,omitempty"`
-	// lease is the ID of the lease that attached to key.
-	// When the attached lease expires, the key will be deleted.
-	// If lease is 0, then no lease is attached to the key.
-	Lease int64 `protobuf:"varint,6,opt,name=lease,proto3" json:"lease,omitempty"`
-}
-
-func (m *KeyValue) Reset()                    { *m = KeyValue{} }
-func (m *KeyValue) String() string            { return proto.CompactTextString(m) }
-func (*KeyValue) ProtoMessage()               {}
-func (*KeyValue) Descriptor() ([]byte, []int) { return fileDescriptorKv, []int{0} }
-
-type Event struct {
-	// type is the kind of event. If type is a PUT, it indicates
-	// new data has been stored to the key. If type is a DELETE,
-	// it indicates the key was deleted.
-	Type Event_EventType `protobuf:"varint,1,opt,name=type,proto3,enum=storagepb.Event_EventType" json:"type,omitempty"`
-	// kv holds the KeyValue for the event.
-	// A PUT event contains current kv pair.
-	// A PUT event with kv.Version=1 indicates the creation of a key.
-	// A DELETE/EXPIRE event contains the deleted key with
-	// its modification revision set to the revision of deletion.
-	Kv *KeyValue `protobuf:"bytes,2,opt,name=kv" json:"kv,omitempty"`
-}
-
-func (m *Event) Reset()                    { *m = Event{} }
-func (m *Event) String() string            { return proto.CompactTextString(m) }
-func (*Event) ProtoMessage()               {}
-func (*Event) Descriptor() ([]byte, []int) { return fileDescriptorKv, []int{1} }
-
-func init() {
-	proto.RegisterType((*KeyValue)(nil), "storagepb.KeyValue")
-	proto.RegisterType((*Event)(nil), "storagepb.Event")
-	proto.RegisterEnum("storagepb.Event_EventType", Event_EventType_name, Event_EventType_value)
-}
-func (m *KeyValue) Marshal() (data []byte, err error) {
-	size := m.Size()
-	data = make([]byte, size)
-	n, err := m.MarshalTo(data)
-	if err != nil {
-		return nil, err
-	}
-	return data[:n], nil
-}
-
-func (m *KeyValue) MarshalTo(data []byte) (int, error) {
-	var i int
-	_ = i
-	var l int
-	_ = l
-	if len(m.Key) > 0 {
-		data[i] = 0xa
-		i++
-		i = encodeVarintKv(data, i, uint64(len(m.Key)))
-		i += copy(data[i:], m.Key)
-	}
-	if m.CreateRevision != 0 {
-		data[i] = 0x10
-		i++
-		i = encodeVarintKv(data, i, uint64(m.CreateRevision))
-	}
-	if m.ModRevision != 0 {
-		data[i] = 0x18
-		i++
-		i = encodeVarintKv(data, i, uint64(m.ModRevision))
-	}
-	if m.Version != 0 {
-		data[i] = 0x20
-		i++
-		i = encodeVarintKv(data, i, uint64(m.Version))
-	}
-	if len(m.Value) > 0 {
-		data[i] = 0x2a
-		i++
-		i = encodeVarintKv(data, i, uint64(len(m.Value)))
-		i += copy(data[i:], m.Value)
-	}
-	if m.Lease != 0 {
-		data[i] = 0x30
-		i++
-		i = encodeVarintKv(data, i, uint64(m.Lease))
-	}
-	return i, nil
-}
-
-func (m *Event) Marshal() (data []byte, err error) {
-	size := m.Size()
-	data = make([]byte, size)
-	n, err := m.MarshalTo(data)
-	if err != nil {
-		return nil, err
-	}
-	return data[:n], nil
-}
-
-func (m *Event) MarshalTo(data []byte) (int, error) {
-	var i int
-	_ = i
-	var l int
-	_ = l
-	if m.Type != 0 {
-		data[i] = 0x8
-		i++
-		i = encodeVarintKv(data, i, uint64(m.Type))
-	}
-	if m.Kv != nil {
-		data[i] = 0x12
-		i++
-		i = encodeVarintKv(data, i, uint64(m.Kv.Size()))
-		n1, err := m.Kv.MarshalTo(data[i:])
-		if err != nil {
-			return 0, err
-		}
-		i += n1
-	}
-	return i, nil
-}
-
-func encodeFixed64Kv(data []byte, offset int, v uint64) int {
-	data[offset] = uint8(v)
-	data[offset+1] = uint8(v >> 8)
-	data[offset+2] = uint8(v >> 16)
-	data[offset+3] = uint8(v >> 24)
-	data[offset+4] = uint8(v >> 32)
-	data[offset+5] = uint8(v >> 40)
-	data[offset+6] = uint8(v >> 48)
-	data[offset+7] = uint8(v >> 56)
-	return offset + 8
-}
-func encodeFixed32Kv(data []byte, offset int, v uint32) int {
-	data[offset] = uint8(v)
-	data[offset+1] = uint8(v >> 8)
-	data[offset+2] = uint8(v >> 16)
-	data[offset+3] = uint8(v >> 24)
-	return offset + 4
-}
-func encodeVarintKv(data []byte, offset int, v uint64) int {
-	for v >= 1<<7 {
-		data[offset] = uint8(v&0x7f | 0x80)
-		v >>= 7
-		offset++
-	}
-	data[offset] = uint8(v)
-	return offset + 1
-}
-func (m *KeyValue) Size() (n int) {
-	var l int
-	_ = l
-	l = len(m.Key)
-	if l > 0 {
-		n += 1 + l + sovKv(uint64(l))
-	}
-	if m.CreateRevision != 0 {
-		n += 1 + sovKv(uint64(m.CreateRevision))
-	}
-	if m.ModRevision != 0 {
-		n += 1 + sovKv(uint64(m.ModRevision))
-	}
-	if m.Version != 0 {
-		n += 1 + sovKv(uint64(m.Version))
-	}
-	l = len(m.Value)
-	if l > 0 {
-		n += 1 + l + sovKv(uint64(l))
-	}
-	if m.Lease != 0 {
-		n += 1 + sovKv(uint64(m.Lease))
-	}
-	return n
-}
-
-func (m *Event) Size() (n int) {
-	var l int
-	_ = l
-	if m.Type != 0 {
-		n += 1 + sovKv(uint64(m.Type))
-	}
-	if m.Kv != nil {
-		l = m.Kv.Size()
-		n += 1 + l + sovKv(uint64(l))
-	}
-	return n
-}
-
-func sovKv(x uint64) (n int) {
-	for {
-		n++
-		x >>= 7
-		if x == 0 {
-			break
-		}
-	}
-	return n
-}
-func sozKv(x uint64) (n int) {
-	return sovKv(uint64((x << 1) ^ uint64((int64(x) >> 63))))
-}
-func (m *KeyValue) Unmarshal(data []byte) error {
-	l := len(data)
-	iNdEx := 0
-	for iNdEx < l {
-		preIndex := iNdEx
-		var wire uint64
-		for shift := uint(0); ; shift += 7 {
-			if shift >= 64 {
-				return ErrIntOverflowKv
-			}
-			if iNdEx >= l {
-				return io.ErrUnexpectedEOF
-			}
-			b := data[iNdEx]
-			iNdEx++
-			wire |= (uint64(b) & 0x7F) << shift
-			if b < 0x80 {
-				break
-			}
-		}
-		fieldNum := int32(wire >> 3)
-		wireType := int(wire & 0x7)
-		if wireType == 4 {
-			return fmt.Errorf("proto: KeyValue: wiretype end group for non-group")
-		}
-		if fieldNum <= 0 {
-			return fmt.Errorf("proto: KeyValue: illegal tag %d (wire type %d)", fieldNum, wire)
-		}
-		switch fieldNum {
-		case 1:
-			if wireType != 2 {
-				return fmt.Errorf("proto: wrong wireType = %d for field Key", wireType)
-			}
-			var byteLen int
-			for shift := uint(0); ; shift += 7 {
-				if shift >= 64 {
-					return ErrIntOverflowKv
-				}
-				if iNdEx >= l {
-					return io.ErrUnexpectedEOF
-				}
-				b := data[iNdEx]
-				iNdEx++
-				byteLen |= (int(b) & 0x7F) << shift
-				if b < 0x80 {
-					break
-				}
-			}
-			if byteLen < 0 {
-				return ErrInvalidLengthKv
-			}
-			postIndex := iNdEx + byteLen
-			if postIndex > l {
-				return io.ErrUnexpectedEOF
-			}
-			m.Key = append(m.Key[:0], data[iNdEx:postIndex]...)
-			if m.Key == nil {
-				m.Key = []byte{}
-			}
-			iNdEx = postIndex
-		case 2:
-			if wireType != 0 {
-				return fmt.Errorf("proto: wrong wireType = %d for field CreateRevision", wireType)
-			}
-			m.CreateRevision = 0
-			for shift := uint(0); ; shift += 7 {
-				if shift >= 64 {
-					return ErrIntOverflowKv
-				}
-				if iNdEx >= l {
-					return io.ErrUnexpectedEOF
-				}
-				b := data[iNdEx]
-				iNdEx++
-				m.CreateRevision |= (int64(b) & 0x7F) << shift
-				if b < 0x80 {
-					break
-				}
-			}
-		case 3:
-			if wireType != 0 {
-				return fmt.Errorf("proto: wrong wireType = %d for field ModRevision", wireType)
-			}
-			m.ModRevision = 0
-			for shift := uint(0); ; shift += 7 {
-				if shift >= 64 {
-					return ErrIntOverflowKv
-				}
-				if iNdEx >= l {
-					return io.ErrUnexpectedEOF
-				}
-				b := data[iNdEx]
-				iNdEx++
-				m.ModRevision |= (int64(b) & 0x7F) << shift
-				if b < 0x80 {
-					break
-				}
-			}
-		case 4:
-			if wireType != 0 {
-				return fmt.Errorf("proto: wrong wireType = %d for field Version", wireType)
-			}
-			m.Version = 0
-			for shift := uint(0); ; shift += 7 {
-				if shift >= 64 {
-					return ErrIntOverflowKv
-				}
-				if iNdEx >= l {
-					return io.ErrUnexpectedEOF
-				}
-				b := data[iNdEx]
-				iNdEx++
-				m.Version |= (int64(b) & 0x7F) << shift
-				if b < 0x80 {
-					break
-				}
-			}
-		case 5:
-			if wireType != 2 {
-				return fmt.Errorf("proto: wrong wireType = %d for field Value", wireType)
-			}
-			var byteLen int
-			for shift := uint(0); ; shift += 7 {
-				if shift >= 64 {
-					return ErrIntOverflowKv
-				}
-				if iNdEx >= l {
-					return io.ErrUnexpectedEOF
-				}
-				b := data[iNdEx]
-				iNdEx++
-				byteLen |= (int(b) & 0x7F) << shift
-				if b < 0x80 {
-					break
-				}
-			}
-			if byteLen < 0 {
-				return ErrInvalidLengthKv
-			}
-			postIndex := iNdEx + byteLen
-			if postIndex > l {
-				return io.ErrUnexpectedEOF
-			}
-			m.Value = append(m.Value[:0], data[iNdEx:postIndex]...)
-			if m.Value == nil {
-				m.Value = []byte{}
-			}
-			iNdEx = postIndex
-		case 6:
-			if wireType != 0 {
-				return fmt.Errorf("proto: wrong wireType = %d for field Lease", wireType)
-			}
-			m.Lease = 0
-			for shift := uint(0); ; shift += 7 {
-				if shift >= 64 {
-					return ErrIntOverflowKv
-				}
-				if iNdEx >= l {
-					return io.ErrUnexpectedEOF
-				}
-				b := data[iNdEx]
-				iNdEx++
-				m.Lease |= (int64(b) & 0x7F) << shift
-				if b < 0x80 {
-					break
-				}
-			}
-		default:
-			iNdEx = preIndex
-			skippy, err := skipKv(data[iNdEx:])
-			if err != nil {
-				return err
-			}
-			if skippy < 0 {
-				return ErrInvalidLengthKv
-			}
-			if (iNdEx + skippy) > l {
-				return io.ErrUnexpectedEOF
-			}
-			iNdEx += skippy
-		}
-	}
-
-	if iNdEx > l {
-		return io.ErrUnexpectedEOF
-	}
-	return nil
-}
-func (m *Event) Unmarshal(data []byte) error {
-	l := len(data)
-	iNdEx := 0
-	for iNdEx < l {
-		preIndex := iNdEx
-		var wire uint64
-		for shift := uint(0); ; shift += 7 {
-			if shift >= 64 {
-				return ErrIntOverflowKv
-			}
-			if iNdEx >= l {
-				return io.ErrUnexpectedEOF
-			}
-			b := data[iNdEx]
-			iNdEx++
-			wire |= (uint64(b) & 0x7F) << shift
-			if b < 0x80 {
-				break
-			}
-		}
-		fieldNum := int32(wire >> 3)
-		wireType := int(wire & 0x7)
-		if wireType == 4 {
-			return fmt.Errorf("proto: Event: wiretype end group for non-group")
-		}
-		if fieldNum <= 0 {
-			return fmt.Errorf("proto: Event: illegal tag %d (wire type %d)", fieldNum, wire)
-		}
-		switch fieldNum {
-		case 1:
-			if wireType != 0 {
-				return fmt.Errorf("proto: wrong wireType = %d for field Type", wireType)
-			}
-			m.Type = 0
-			for shift := uint(0); ; shift += 7 {
-				if shift >= 64 {
-					return ErrIntOverflowKv
-				}
-				if iNdEx >= l {
-					return io.ErrUnexpectedEOF
-				}
-				b := data[iNdEx]
-				iNdEx++
-				m.Type |= (Event_EventType(b) & 0x7F) << shift
-				if b < 0x80 {
-					break
-				}
-			}
-		case 2:
-			if wireType != 2 {
-				return fmt.Errorf("proto: wrong wireType = %d for field Kv", wireType)
-			}
-			var msglen int
-			for shift := uint(0); ; shift += 7 {
-				if shift >= 64 {
-					return ErrIntOverflowKv
-				}
-				if iNdEx >= l {
-					return io.ErrUnexpectedEOF
-				}
-				b := data[iNdEx]
-				iNdEx++
-				msglen |= (int(b) & 0x7F) << shift
-				if b < 0x80 {
-					break
-				}
-			}
-			if msglen < 0 {
-				return ErrInvalidLengthKv
-			}
-			postIndex := iNdEx + msglen
-			if postIndex > l {
-				return io.ErrUnexpectedEOF
-			}
-			if m.Kv == nil {
-				m.Kv = &KeyValue{}
-			}
-			if err := m.Kv.Unmarshal(data[iNdEx:postIndex]); err != nil {
-				return err
-			}
-			iNdEx = postIndex
-		default:
-			iNdEx = preIndex
-			skippy, err := skipKv(data[iNdEx:])
-			if err != nil {
-				return err
-			}
-			if skippy < 0 {
-				return ErrInvalidLengthKv
-			}
-			if (iNdEx + skippy) > l {
-				return io.ErrUnexpectedEOF
-			}
-			iNdEx += skippy
-		}
-	}
-
-	if iNdEx > l {
-		return io.ErrUnexpectedEOF
-	}
-	return nil
-}
-func skipKv(data []byte) (n int, err error) {
-	l := len(data)
-	iNdEx := 0
-	for iNdEx < l {
-		var wire uint64
-		for shift := uint(0); ; shift += 7 {
-			if shift >= 64 {
-				return 0, ErrIntOverflowKv
-			}
-			if iNdEx >= l {
-				return 0, io.ErrUnexpectedEOF
-			}
-			b := data[iNdEx]
-			iNdEx++
-			wire |= (uint64(b) & 0x7F) << shift
-			if b < 0x80 {
-				break
-			}
-		}
-		wireType := int(wire & 0x7)
-		switch wireType {
-		case 0:
-			for shift := uint(0); ; shift += 7 {
-				if shift >= 64 {
-					return 0, ErrIntOverflowKv
-				}
-				if iNdEx >= l {
-					return 0, io.ErrUnexpectedEOF
-				}
-				iNdEx++
-				if data[iNdEx-1] < 0x80 {
-					break
-				}
-			}
-			return iNdEx, nil
-		case 1:
-			iNdEx += 8
-			return iNdEx, nil
-		case 2:
-			var length int
-			for shift := uint(0); ; shift += 7 {
-				if shift >= 64 {
-					return 0, ErrIntOverflowKv
-				}
-				if iNdEx >= l {
-					return 0, io.ErrUnexpectedEOF
-				}
-				b := data[iNdEx]
-				iNdEx++
-				length |= (int(b) & 0x7F) << shift
-				if b < 0x80 {
-					break
-				}
-			}
-			iNdEx += length
-			if length < 0 {
-				return 0, ErrInvalidLengthKv
-			}
-			return iNdEx, nil
-		case 3:
-			for {
-				var innerWire uint64
-				var start int = iNdEx
-				for shift := uint(0); ; shift += 7 {
-					if shift >= 64 {
-						return 0, ErrIntOverflowKv
-					}
-					if iNdEx >= l {
-						return 0, io.ErrUnexpectedEOF
-					}
-					b := data[iNdEx]
-					iNdEx++
-					innerWire |= (uint64(b) & 0x7F) << shift
-					if b < 0x80 {
-						break
-					}
-				}
-				innerWireType := int(innerWire & 0x7)
-				if innerWireType == 4 {
-					break
-				}
-				next, err := skipKv(data[start:])
-				if err != nil {
-					return 0, err
-				}
-				iNdEx = start + next
-			}
-			return iNdEx, nil
-		case 4:
-			return iNdEx, nil
-		case 5:
-			iNdEx += 4
-			return iNdEx, nil
-		default:
-			return 0, fmt.Errorf("proto: illegal wireType %d", wireType)
-		}
-	}
-	panic("unreachable")
-}
-
-var (
-	ErrInvalidLengthKv = fmt.Errorf("proto: negative length found during unmarshaling")
-	ErrIntOverflowKv   = fmt.Errorf("proto: integer overflow")
-)
-
-var fileDescriptorKv = []byte{
-	// 288 bytes of a gzipped FileDescriptorProto
-	0x1f, 0x8b, 0x08, 0x00, 0x00, 0x09, 0x6e, 0x88, 0x02, 0xff, 0xe2, 0xe2, 0xc8, 0x2e, 0xd3, 0x2b,
-	0x28, 0xca, 0x2f, 0xc9, 0x17, 0xe2, 0x2c, 0x2e, 0xc9, 0x2f, 0x4a, 0x4c, 0x4f, 0x2d, 0x48, 0x92,
-	0x12, 0x49, 0xcf, 0x4f, 0xcf, 0x07, 0x8b, 0xea, 0x83, 0x58, 0x10, 0x05, 0x4a, 0xeb, 0x18, 0xb9,
-	0x38, 0xbc, 0x53, 0x2b, 0xc3, 0x12, 0x73, 0x4a, 0x53, 0x85, 0x04, 0xb8, 0x98, 0xb3, 0x53, 0x2b,
-	0x25, 0x18, 0x15, 0x18, 0x35, 0x78, 0x82, 0x40, 0x4c, 0x21, 0x75, 0x2e, 0xfe, 0xe4, 0xa2, 0xd4,
-	0xc4, 0x92, 0xd4, 0xf8, 0xa2, 0xd4, 0xb2, 0xcc, 0xe2, 0xcc, 0xfc, 0x3c, 0x09, 0x26, 0xa0, 0x2c,
-	0x73, 0x10, 0x1f, 0x44, 0x38, 0x08, 0x2a, 0x2a, 0xa4, 0xc8, 0xc5, 0x93, 0x9b, 0x9f, 0x82, 0x50,
-	0xc5, 0x0c, 0x56, 0xc5, 0x0d, 0x14, 0x83, 0x2b, 0x91, 0xe0, 0x62, 0x2f, 0x4b, 0x2d, 0x02, 0xcb,
-	0xb2, 0x80, 0x65, 0x61, 0x5c, 0x21, 0x11, 0x2e, 0xd6, 0x32, 0x90, 0x03, 0x24, 0x58, 0xc1, 0x36,
-	0x43, 0x38, 0x20, 0xd1, 0x9c, 0xd4, 0xc4, 0xe2, 0x54, 0x09, 0x36, 0xb0, 0x6a, 0x08, 0x47, 0xa9,
-	0x8b, 0x91, 0x8b, 0xd5, 0xb5, 0x2c, 0x35, 0xaf, 0x44, 0x48, 0x8f, 0x8b, 0xa5, 0xa4, 0xb2, 0x20,
-	0x15, 0xec, 0x5c, 0x3e, 0x23, 0x29, 0x3d, 0xb8, 0x57, 0xf5, 0xc0, 0xf2, 0x10, 0x32, 0x04, 0xa8,
-	0x22, 0x08, 0xac, 0x4e, 0x48, 0x99, 0x8b, 0x29, 0xbb, 0x0c, 0xec, 0x7c, 0x6e, 0x23, 0x61, 0x24,
-	0xd5, 0x30, 0xef, 0x07, 0x01, 0xa5, 0x95, 0x74, 0xb8, 0x38, 0xe1, 0xfa, 0x84, 0xd8, 0xb9, 0x98,
-	0x03, 0x42, 0x43, 0x04, 0x18, 0x84, 0xb8, 0xb8, 0xd8, 0x5c, 0x5c, 0x7d, 0x5c, 0x43, 0x5c, 0x05,
-	0x18, 0x41, 0x6c, 0xd7, 0x88, 0x00, 0xcf, 0x20, 0x57, 0x01, 0x26, 0x27, 0x91, 0x13, 0x0f, 0xe5,
-	0x18, 0x2e, 0x00, 0xf1, 0x89, 0x47, 0x72, 0x8c, 0x17, 0x80, 0xf8, 0x01, 0x10, 0x27, 0xb1, 0x81,
-	0x83, 0xd6, 0x18, 0x10, 0x00, 0x00, 0xff, 0xff, 0x15, 0x18, 0xeb, 0x9b, 0x87, 0x01, 0x00, 0x00,
-}
diff --git a/storage/storagepb/kv.proto b/storage/storagepb/kv.proto
deleted file mode 100644
index 7e5e50a..0000000
--- a/storage/storagepb/kv.proto
+++ /dev/null
@@ -1,47 +0,0 @@
-syntax = "proto3";
-package storagepb;
-
-import "gogoproto/gogo.proto";
-
-option (gogoproto.marshaler_all) = true;
-option (gogoproto.sizer_all) = true;
-option (gogoproto.unmarshaler_all) = true;
-option (gogoproto.goproto_getters_all) = false;
-option (gogoproto.goproto_enum_prefix_all) = false;
-
-message KeyValue {
-  // key is the key in bytes. An empty key is not allowed.
-  bytes key = 1;
-  // create_revision is the revision of last creation on this key.
-  int64 create_revision = 2;
-  // mod_revision is the revision of last modification on this key.
-  int64 mod_revision = 3;
-  // version is the version of the key. A deletion resets
-  // the version to zero and any modification of the key
-  // increases its version.
-  int64 version = 4;
-  // value is the value held by the key, in bytes.
-  bytes value = 5;
-  // lease is the ID of the lease that attached to key.
-  // When the attached lease expires, the key will be deleted.
-  // If lease is 0, then no lease is attached to the key.
-  int64 lease = 6;
-}
-
-message Event {
-  enum EventType {
-    PUT = 0;
-    DELETE = 1;
-    EXPIRE = 2;
-  }
-  // type is the kind of event. If type is a PUT, it indicates
-  // new data has been stored to the key. If type is a DELETE,
-  // it indicates the key was deleted.
-  EventType type = 1;
-  // kv holds the KeyValue for the event.
-  // A PUT event contains current kv pair.
-  // A PUT event with kv.Version=1 indicates the creation of a key.
-  // A DELETE/EXPIRE event contains the deleted key with
-  // its modification revision set to the revision of deletion.
-  KeyValue kv = 2;
-}
diff --git a/storage/watchable_store.go b/storage/watchable_store.go
deleted file mode 100644
index 818fcaf..0000000
--- a/storage/watchable_store.go
+++ /dev/null
@@ -1,386 +0,0 @@
-// Copyright 2015 CoreOS, Inc.
-//
-// Licensed under the Apache License, Version 2.0 (the "License");
-// you may not use this file except in compliance with the License.
-// You may obtain a copy of the License at
-//
-//     http://www.apache.org/licenses/LICENSE-2.0
-//
-// Unless required by applicable law or agreed to in writing, software
-// distributed under the License is distributed on an "AS IS" BASIS,
-// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-// See the License for the specific language governing permissions and
-// limitations under the License.
-
-package storage
-
-import (
-	"log"
-	"sync"
-	"time"
-
-	"github.com/coreos/etcd/lease"
-	"github.com/coreos/etcd/storage/backend"
-	"github.com/coreos/etcd/storage/storagepb"
-)
-
-const (
-	// chanBufLen is the length of the buffered chan
-	// for sending out watched events.
-	// TODO: find a good buf value. 1024 is just a random one that
-	// seems to be reasonable.
-	chanBufLen = 1024
-)
-
-type watchable interface {
-	watch(key, end []byte, startRev int64, id WatchID, ch chan<- WatchResponse) (*watcher, cancelFunc)
-	progress(w *watcher)
-	rev() int64
-}
-
-type watchableStore struct {
-	mu sync.Mutex
-
-	*store
-
-	// contains all unsynced watchers that needs to sync with events that have happened
-	unsynced watcherGroup
-
-	// contains all synced watchers that are in sync with the progress of the store.
-	// The key of the map is the key that the watcher watches on.
-	synced watcherGroup
-
-	stopc chan struct{}
-	wg    sync.WaitGroup
-}
-
-// cancelFunc updates unsynced and synced maps when running
-// cancel operations.
-type cancelFunc func()
-
-func New(b backend.Backend, le lease.Lessor, ig ConsistentIndexGetter) ConsistentWatchableKV {
-	return newWatchableStore(b, le, ig)
-}
-
-func newWatchableStore(b backend.Backend, le lease.Lessor, ig ConsistentIndexGetter) *watchableStore {
-	s := &watchableStore{
-		store:    NewStore(b, le, ig),
-		unsynced: newWatcherGroup(),
-		synced:   newWatcherGroup(),
-		stopc:    make(chan struct{}),
-	}
-	if s.le != nil {
-		// use this store as the deleter so revokes trigger watch events
-		s.le.SetRangeDeleter(s)
-	}
-	s.wg.Add(1)
-	go s.syncWatchersLoop()
-	return s
-}
-
-func (s *watchableStore) Put(key, value []byte, lease lease.LeaseID) (rev int64) {
-	s.mu.Lock()
-	defer s.mu.Unlock()
-
-	rev = s.store.Put(key, value, lease)
-	changes := s.store.getChanges()
-	if len(changes) != 1 {
-		log.Panicf("unexpected len(changes) != 1 after put")
-	}
-
-	ev := storagepb.Event{
-		Type: storagepb.PUT,
-		Kv:   &changes[0],
-	}
-	s.notify(rev, []storagepb.Event{ev})
-	return rev
-}
-
-func (s *watchableStore) DeleteRange(key, end []byte) (n, rev int64) {
-	s.mu.Lock()
-	defer s.mu.Unlock()
-
-	n, rev = s.store.DeleteRange(key, end)
-	changes := s.store.getChanges()
-
-	if len(changes) != int(n) {
-		log.Panicf("unexpected len(changes) != n after deleteRange")
-	}
-
-	if n == 0 {
-		return n, rev
-	}
-
-	evs := make([]storagepb.Event, n)
-	for i, change := range changes {
-		evs[i] = storagepb.Event{
-			Type: storagepb.DELETE,
-			Kv:   &change}
-		evs[i].Kv.ModRevision = rev
-	}
-	s.notify(rev, evs)
-	return n, rev
-}
-
-func (s *watchableStore) TxnBegin() int64 {
-	s.mu.Lock()
-	return s.store.TxnBegin()
-}
-
-func (s *watchableStore) TxnEnd(txnID int64) error {
-	err := s.store.TxnEnd(txnID)
-	if err != nil {
-		return err
-	}
-
-	changes := s.getChanges()
-	if len(changes) == 0 {
-		s.mu.Unlock()
-		return nil
-	}
-
-	rev := s.store.Rev()
-	evs := make([]storagepb.Event, len(changes))
-	for i, change := range changes {
-		switch change.CreateRevision {
-		case 0:
-			evs[i] = storagepb.Event{
-				Type: storagepb.DELETE,
-				Kv:   &changes[i]}
-			evs[i].Kv.ModRevision = rev
-		default:
-			evs[i] = storagepb.Event{
-				Type: storagepb.PUT,
-				Kv:   &changes[i]}
-		}
-	}
-
-	s.notify(rev, evs)
-	s.mu.Unlock()
-
-	return nil
-}
-
-func (s *watchableStore) Close() error {
-	close(s.stopc)
-	s.wg.Wait()
-	return s.store.Close()
-}
-
-func (s *watchableStore) NewWatchStream() WatchStream {
-	watchStreamGauge.Inc()
-	return &watchStream{
-		watchable: s,
-		ch:        make(chan WatchResponse, chanBufLen),
-		cancels:   make(map[WatchID]cancelFunc),
-		watchers:  make(map[WatchID]*watcher),
-	}
-}
-
-func (s *watchableStore) watch(key, end []byte, startRev int64, id WatchID, ch chan<- WatchResponse) (*watcher, cancelFunc) {
-	s.mu.Lock()
-	defer s.mu.Unlock()
-
-	wa := &watcher{
-		key: key,
-		end: end,
-		cur: startRev,
-		id:  id,
-		ch:  ch,
-	}
-
-	s.store.mu.Lock()
-	synced := startRev > s.store.currentRev.main || startRev == 0
-	if synced {
-		wa.cur = s.store.currentRev.main + 1
-		if startRev > wa.cur {
-			wa.cur = startRev
-		}
-	}
-	s.store.mu.Unlock()
-	if synced {
-		s.synced.add(wa)
-	} else {
-		slowWatcherGauge.Inc()
-		s.unsynced.add(wa)
-	}
-	watcherGauge.Inc()
-
-	cancel := cancelFunc(func() {
-		s.mu.Lock()
-		defer s.mu.Unlock()
-		// remove references of the watcher
-		if s.unsynced.delete(wa) {
-			slowWatcherGauge.Dec()
-			watcherGauge.Dec()
-			return
-		}
-
-		if s.synced.delete(wa) {
-			watcherGauge.Dec()
-		}
-		// If we cannot find it, it should have finished watch.
-	})
-
-	return wa, cancel
-}
-
-// syncWatchersLoop syncs the watcher in the unsynced map every 100ms.
-func (s *watchableStore) syncWatchersLoop() {
-	defer s.wg.Done()
-
-	for {
-		s.mu.Lock()
-		s.syncWatchers()
-		s.mu.Unlock()
-
-		select {
-		case <-time.After(100 * time.Millisecond):
-		case <-s.stopc:
-			return
-		}
-	}
-}
-
-// syncWatchers periodically syncs unsynced watchers by: Iterate all unsynced
-// watchers to get the minimum revision within its range, skipping the
-// watcher if its current revision is behind the compact revision of the
-// store. And use this minimum revision to get all key-value pairs. Then send
-// those events to watchers.
-func (s *watchableStore) syncWatchers() {
-	s.store.mu.Lock()
-	defer s.store.mu.Unlock()
-
-	if s.unsynced.size() == 0 {
-		return
-	}
-
-	// in order to find key-value pairs from unsynced watchers, we need to
-	// find min revision index, and these revisions can be used to
-	// query the backend store of key-value pairs
-	curRev := s.store.currentRev.main
-	compactionRev := s.store.compactMainRev
-	minRev := s.unsynced.scanMinRev(curRev, compactionRev)
-	minBytes, maxBytes := newRevBytes(), newRevBytes()
-	revToBytes(revision{main: minRev}, minBytes)
-	revToBytes(revision{main: curRev + 1}, maxBytes)
-
-	// UnsafeRange returns keys and values. And in boltdb, keys are revisions.
-	// values are actual key-value pairs in backend.
-	tx := s.store.b.BatchTx()
-	tx.Lock()
-	revs, vs := tx.UnsafeRange(keyBucketName, minBytes, maxBytes, 0)
-	evs := kvsToEvents(&s.unsynced, revs, vs)
-	tx.Unlock()
-
-	wb := newWatcherBatch(&s.unsynced, evs)
-
-	for w, eb := range wb {
-		select {
-		// s.store.Rev also uses Lock, so just return directly
-		case w.ch <- WatchResponse{WatchID: w.id, Events: eb.evs, Revision: s.store.currentRev.main}:
-			pendingEventsGauge.Add(float64(len(eb.evs)))
-		default:
-			// TODO: handle the full unsynced watchers.
-			// continue to process other watchers for now, the full ones
-			// will be processed next time and hopefully it will not be full.
-			continue
-		}
-		if eb.moreRev != 0 {
-			w.cur = eb.moreRev
-			continue
-		}
-		w.cur = curRev
-		s.synced.add(w)
-		s.unsynced.delete(w)
-	}
-
-	// bring all un-notified watchers to synced.
-	for w := range s.unsynced.watchers {
-		if !wb.contains(w) {
-			w.cur = curRev
-			s.synced.add(w)
-			s.unsynced.delete(w)
-		}
-	}
-
-	slowWatcherGauge.Set(float64(s.unsynced.size()))
-}
-
-// kvsToEvents gets all events for the watchers from all key-value pairs
-func kvsToEvents(wg *watcherGroup, revs, vals [][]byte) (evs []storagepb.Event) {
-	for i, v := range vals {
-		var kv storagepb.KeyValue
-		if err := kv.Unmarshal(v); err != nil {
-			log.Panicf("storage: cannot unmarshal event: %v", err)
-		}
-
-		if !wg.contains(string(kv.Key)) {
-			continue
-		}
-
-		ty := storagepb.PUT
-		if isTombstone(revs[i]) {
-			ty = storagepb.DELETE
-			// patch in mod revision so watchers won't skip
-			kv.ModRevision = bytesToRev(revs[i]).main
-		}
-		evs = append(evs, storagepb.Event{Kv: &kv, Type: ty})
-	}
-	return evs
-}
-
-// notify notifies the fact that given event at the given rev just happened to
-// watchers that watch on the key of the event.
-func (s *watchableStore) notify(rev int64, evs []storagepb.Event) {
-	for w, eb := range newWatcherBatch(&s.synced, evs) {
-		if eb.revs != 1 {
-			panic("unexpected multiple revisions in notification")
-		}
-		select {
-		case w.ch <- WatchResponse{WatchID: w.id, Events: eb.evs, Revision: s.Rev()}:
-			pendingEventsGauge.Add(float64(len(eb.evs)))
-		default:
-			// move slow watcher to unsynced
-			w.cur = rev
-			s.unsynced.add(w)
-			s.synced.delete(w)
-			slowWatcherGauge.Inc()
-		}
-	}
-}
-
-func (s *watchableStore) rev() int64 { return s.store.Rev() }
-
-func (s *watchableStore) progress(w *watcher) {
-	s.mu.Lock()
-	defer s.mu.Unlock()
-
-	if _, ok := s.synced.watchers[w]; ok {
-		select {
-		case w.ch <- WatchResponse{WatchID: w.id, Revision: s.rev()}:
-		default:
-			// If the ch is full, this watcher is receiving events.
-			// We do not need to send progress at all.
-		}
-	}
-}
-
-type watcher struct {
-	// the watcher key
-	key []byte
-	// end indicates the end of the range to watch.
-	// If end is set, the watcher is on a range.
-	end []byte
-
-	// cur is the current watcher revision of a unsynced watcher.
-	// cur will be updated for unsynced watcher while it is catching up.
-	// cur is startRev of a synced watcher.
-	// cur will not be updated for synced watcher.
-	cur int64
-	id  WatchID
-
-	// a chan to send out the watch response.
-	// The chan might be shared with other watchers.
-	ch chan<- WatchResponse
-}
diff --git a/storage/watchable_store_bench_test.go b/storage/watchable_store_bench_test.go
deleted file mode 100644
index 03aa33e..0000000
--- a/storage/watchable_store_bench_test.go
+++ /dev/null
@@ -1,127 +0,0 @@
-// Copyright 2015 CoreOS, Inc.
-//
-// Licensed under the Apache License, Version 2.0 (the "License");
-// you may not use this file except in compliance with the License.
-// You may obtain a copy of the License at
-//
-//     http://www.apache.org/licenses/LICENSE-2.0
-//
-// Unless required by applicable law or agreed to in writing, software
-// distributed under the License is distributed on an "AS IS" BASIS,
-// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-// See the License for the specific language governing permissions and
-// limitations under the License.
-
-package storage
-
-import (
-	"math/rand"
-	"os"
-	"testing"
-
-	"github.com/coreos/etcd/lease"
-	"github.com/coreos/etcd/storage/backend"
-)
-
-// Benchmarks on cancel function performance for unsynced watchers
-// in a WatchableStore. It creates k*N watchers to populate unsynced
-// with a reasonably large number of watchers. And measures the time it
-// takes to cancel N watchers out of k*N watchers. The performance is
-// expected to differ depending on the unsynced member implementation.
-// TODO: k is an arbitrary constant. We need to figure out what factor
-// we should put to simulate the real-world use cases.
-func BenchmarkWatchableStoreUnsyncedCancel(b *testing.B) {
-	be, tmpPath := backend.NewDefaultTmpBackend()
-	s := NewStore(be, &lease.FakeLessor{}, nil)
-
-	// manually create watchableStore instead of newWatchableStore
-	// because newWatchableStore periodically calls syncWatchersLoop
-	// method to sync watchers in unsynced map. We want to keep watchers
-	// in unsynced for this benchmark.
-	ws := &watchableStore{
-		store:    s,
-		unsynced: newWatcherGroup(),
-
-		// to make the test not crash from assigning to nil map.
-		// 'synced' doesn't get populated in this test.
-		synced: newWatcherGroup(),
-	}
-
-	defer func() {
-		ws.store.Close()
-		os.Remove(tmpPath)
-	}()
-
-	// Put a key so that we can spawn watchers on that key
-	// (testKey in this test). This increases the rev to 1,
-	// and later we can we set the watcher's startRev to 1,
-	// and force watchers to be in unsynced.
-	testKey := []byte("foo")
-	testValue := []byte("bar")
-	s.Put(testKey, testValue, lease.NoLease)
-
-	w := ws.NewWatchStream()
-
-	const k int = 2
-	benchSampleN := b.N
-	watcherN := k * benchSampleN
-
-	watchIDs := make([]WatchID, watcherN)
-	for i := 0; i < watcherN; i++ {
-		// non-0 value to keep watchers in unsynced
-		watchIDs[i] = w.Watch(testKey, nil, 1)
-	}
-
-	// random-cancel N watchers to make it not biased towards
-	// data structures with an order, such as slice.
-	ix := rand.Perm(watcherN)
-
-	b.ResetTimer()
-	b.ReportAllocs()
-
-	// cancel N watchers
-	for _, idx := range ix[:benchSampleN] {
-		if err := w.Cancel(watchIDs[idx]); err != nil {
-			b.Error(err)
-		}
-	}
-}
-
-func BenchmarkWatchableStoreSyncedCancel(b *testing.B) {
-	be, tmpPath := backend.NewDefaultTmpBackend()
-	s := newWatchableStore(be, &lease.FakeLessor{}, nil)
-
-	defer func() {
-		s.store.Close()
-		os.Remove(tmpPath)
-	}()
-
-	// Put a key so that we can spawn watchers on that key
-	testKey := []byte("foo")
-	testValue := []byte("bar")
-	s.Put(testKey, testValue, lease.NoLease)
-
-	w := s.NewWatchStream()
-
-	// put 1 million watchers on the same key
-	const watcherN = 1000000
-
-	watchIDs := make([]WatchID, watcherN)
-	for i := 0; i < watcherN; i++ {
-		// 0 for startRev to keep watchers in synced
-		watchIDs[i] = w.Watch(testKey, nil, 0)
-	}
-
-	// randomly cancel watchers to make it not biased towards
-	// data structures with an order, such as slice.
-	ix := rand.Perm(watcherN)
-
-	b.ResetTimer()
-	b.ReportAllocs()
-
-	for _, idx := range ix {
-		if err := w.Cancel(watchIDs[idx]); err != nil {
-			b.Error(err)
-		}
-	}
-}
diff --git a/storage/watchable_store_test.go b/storage/watchable_store_test.go
deleted file mode 100644
index f75acea..0000000
--- a/storage/watchable_store_test.go
+++ /dev/null
@@ -1,426 +0,0 @@
-// Copyright 2015 CoreOS, Inc.
-//
-// Licensed under the Apache License, Version 2.0 (the "License");
-// you may not use this file except in compliance with the License.
-// You may obtain a copy of the License at
-//
-//     http://www.apache.org/licenses/LICENSE-2.0
-//
-// Unless required by applicable law or agreed to in writing, software
-// distributed under the License is distributed on an "AS IS" BASIS,
-// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-// See the License for the specific language governing permissions and
-// limitations under the License.
-
-package storage
-
-import (
-	"bytes"
-	"os"
-	"reflect"
-	"testing"
-	"time"
-
-	"github.com/coreos/etcd/lease"
-	"github.com/coreos/etcd/storage/backend"
-	"github.com/coreos/etcd/storage/storagepb"
-)
-
-func TestWatch(t *testing.T) {
-	b, tmpPath := backend.NewDefaultTmpBackend()
-	s := newWatchableStore(b, &lease.FakeLessor{}, nil)
-
-	defer func() {
-		s.store.Close()
-		os.Remove(tmpPath)
-	}()
-
-	testKey := []byte("foo")
-	testValue := []byte("bar")
-	s.Put(testKey, testValue, lease.NoLease)
-
-	w := s.NewWatchStream()
-	w.Watch(testKey, nil, 0)
-
-	if !s.synced.contains(string(testKey)) {
-		// the key must have had an entry in synced
-		t.Errorf("existence = false, want true")
-	}
-}
-
-func TestNewWatcherCancel(t *testing.T) {
-	b, tmpPath := backend.NewDefaultTmpBackend()
-	s := newWatchableStore(b, &lease.FakeLessor{}, nil)
-
-	defer func() {
-		s.store.Close()
-		os.Remove(tmpPath)
-	}()
-	testKey := []byte("foo")
-	testValue := []byte("bar")
-	s.Put(testKey, testValue, lease.NoLease)
-
-	w := s.NewWatchStream()
-	wt := w.Watch(testKey, nil, 0)
-
-	if err := w.Cancel(wt); err != nil {
-		t.Error(err)
-	}
-
-	if s.synced.contains(string(testKey)) {
-		// the key shoud have been deleted
-		t.Errorf("existence = true, want false")
-	}
-}
-
-// TestCancelUnsynced tests if running CancelFunc removes watchers from unsynced.
-func TestCancelUnsynced(t *testing.T) {
-	b, tmpPath := backend.NewDefaultTmpBackend()
-
-	// manually create watchableStore instead of newWatchableStore
-	// because newWatchableStore automatically calls syncWatchers
-	// method to sync watchers in unsynced map. We want to keep watchers
-	// in unsynced to test if syncWatchers works as expected.
-	s := &watchableStore{
-		store:    NewStore(b, &lease.FakeLessor{}, nil),
-		unsynced: newWatcherGroup(),
-
-		// to make the test not crash from assigning to nil map.
-		// 'synced' doesn't get populated in this test.
-		synced: newWatcherGroup(),
-	}
-
-	defer func() {
-		s.store.Close()
-		os.Remove(tmpPath)
-	}()
-
-	// Put a key so that we can spawn watchers on that key.
-	// (testKey in this test). This increases the rev to 1,
-	// and later we can we set the watcher's startRev to 1,
-	// and force watchers to be in unsynced.
-	testKey := []byte("foo")
-	testValue := []byte("bar")
-	s.Put(testKey, testValue, lease.NoLease)
-
-	w := s.NewWatchStream()
-
-	// arbitrary number for watchers
-	watcherN := 100
-
-	// create watcherN of watch ids to cancel
-	watchIDs := make([]WatchID, watcherN)
-	for i := 0; i < watcherN; i++ {
-		// use 1 to keep watchers in unsynced
-		watchIDs[i] = w.Watch(testKey, nil, 1)
-	}
-
-	for _, idx := range watchIDs {
-		if err := w.Cancel(idx); err != nil {
-			t.Error(err)
-		}
-	}
-
-	// After running CancelFunc
-	//
-	// unsynced should be empty
-	// because cancel removes watcher from unsynced
-	if size := s.unsynced.size(); size != 0 {
-		t.Errorf("unsynced size = %d, want 0", size)
-	}
-}
-
-// TestSyncWatchers populates unsynced watcher map and tests syncWatchers
-// method to see if it correctly sends events to channel of unsynced watchers
-// and moves these watchers to synced.
-func TestSyncWatchers(t *testing.T) {
-	b, tmpPath := backend.NewDefaultTmpBackend()
-
-	s := &watchableStore{
-		store:    NewStore(b, &lease.FakeLessor{}, nil),
-		unsynced: newWatcherGroup(),
-		synced:   newWatcherGroup(),
-	}
-
-	defer func() {
-		s.store.Close()
-		os.Remove(tmpPath)
-	}()
-
-	testKey := []byte("foo")
-	testValue := []byte("bar")
-	s.Put(testKey, testValue, lease.NoLease)
-
-	w := s.NewWatchStream()
-
-	// arbitrary number for watchers
-	watcherN := 100
-
-	for i := 0; i < watcherN; i++ {
-		// specify rev as 1 to keep watchers in unsynced
-		w.Watch(testKey, nil, 1)
-	}
-
-	// Before running s.syncWatchers() synced should be empty because we manually
-	// populate unsynced only
-	sws := s.synced.watcherSetByKey(string(testKey))
-	uws := s.unsynced.watcherSetByKey(string(testKey))
-
-	if len(sws) != 0 {
-		t.Fatalf("synced[string(testKey)] size = %d, want 0", len(sws))
-	}
-	// unsynced should not be empty because we manually populated unsynced only
-	if len(uws) != watcherN {
-		t.Errorf("unsynced size = %d, want %d", len(uws), watcherN)
-	}
-
-	// this should move all unsynced watchers to synced ones
-	s.syncWatchers()
-
-	sws = s.synced.watcherSetByKey(string(testKey))
-	uws = s.unsynced.watcherSetByKey(string(testKey))
-
-	// After running s.syncWatchers(), synced should not be empty because syncwatchers
-	// populates synced in this test case
-	if len(sws) != watcherN {
-		t.Errorf("synced[string(testKey)] size = %d, want %d", len(sws), watcherN)
-	}
-
-	// unsynced should be empty because syncwatchers is expected to move all watchers
-	// from unsynced to synced in this test case
-	if len(uws) != 0 {
-		t.Errorf("unsynced size = %d, want 0", len(uws))
-	}
-
-	for w := range sws {
-		if w.cur != s.Rev() {
-			t.Errorf("w.cur = %d, want %d", w.cur, s.Rev())
-		}
-	}
-
-	if len(w.(*watchStream).ch) != watcherN {
-		t.Errorf("watched event size = %d, want %d", len(w.(*watchStream).ch), watcherN)
-	}
-
-	evs := (<-w.(*watchStream).ch).Events
-	if len(evs) != 1 {
-		t.Errorf("len(evs) got = %d, want = 1", len(evs))
-	}
-	if evs[0].Type != storagepb.PUT {
-		t.Errorf("got = %v, want = %v", evs[0].Type, storagepb.PUT)
-	}
-	if !bytes.Equal(evs[0].Kv.Key, testKey) {
-		t.Errorf("got = %s, want = %s", evs[0].Kv.Key, testKey)
-	}
-	if !bytes.Equal(evs[0].Kv.Value, testValue) {
-		t.Errorf("got = %s, want = %s", evs[0].Kv.Value, testValue)
-	}
-}
-
-// TestWatchCompacted tests a watcher that watches on a compacted revision.
-func TestWatchCompacted(t *testing.T) {
-	b, tmpPath := backend.NewDefaultTmpBackend()
-	s := newWatchableStore(b, &lease.FakeLessor{}, nil)
-
-	defer func() {
-		s.store.Close()
-		os.Remove(tmpPath)
-	}()
-	testKey := []byte("foo")
-	testValue := []byte("bar")
-
-	maxRev := 10
-	compactRev := int64(5)
-	for i := 0; i < maxRev; i++ {
-		s.Put(testKey, testValue, lease.NoLease)
-	}
-	_, err := s.Compact(compactRev)
-	if err != nil {
-		t.Fatalf("failed to compact kv (%v)", err)
-	}
-
-	w := s.NewWatchStream()
-	wt := w.Watch(testKey, nil, compactRev-1)
-
-	select {
-	case resp := <-w.Chan():
-		if resp.WatchID != wt {
-			t.Errorf("resp.WatchID = %x, want %x", resp.WatchID, wt)
-		}
-		if resp.CompactRevision == 0 {
-			t.Errorf("resp.Compacted = %v, want %v", resp.CompactRevision, compactRev)
-		}
-	case <-time.After(1 * time.Second):
-		t.Fatalf("failed to receive response (timeout)")
-	}
-}
-
-func TestWatchFutureRev(t *testing.T) {
-	b, tmpPath := backend.NewDefaultTmpBackend()
-	s := newWatchableStore(b, &lease.FakeLessor{}, nil)
-
-	defer func() {
-		s.store.Close()
-		os.Remove(tmpPath)
-	}()
-
-	testKey := []byte("foo")
-	testValue := []byte("bar")
-
-	w := s.NewWatchStream()
-	wrev := int64(10)
-	w.Watch(testKey, nil, wrev)
-
-	for i := 0; i < 10; i++ {
-		rev := s.Put(testKey, testValue, lease.NoLease)
-		if rev >= wrev {
-			break
-		}
-	}
-
-	select {
-	case resp := <-w.Chan():
-		if resp.Revision != wrev {
-			t.Fatalf("rev = %d, want %d", resp.Revision, wrev)
-		}
-		if len(resp.Events) != 1 {
-			t.Fatalf("failed to get events from the response")
-		}
-		if resp.Events[0].Kv.ModRevision != wrev {
-			t.Fatalf("kv.rev = %d, want %d", resp.Events[0].Kv.ModRevision, wrev)
-		}
-	case <-time.After(time.Second):
-		t.Fatal("failed to receive event in 1 second.")
-	}
-}
-
-// TestWatchBatchUnsynced tests batching on unsynced watchers
-func TestWatchBatchUnsynced(t *testing.T) {
-	b, tmpPath := backend.NewDefaultTmpBackend()
-	s := newWatchableStore(b, &lease.FakeLessor{}, nil)
-
-	oldMaxRevs := watchBatchMaxRevs
-	defer func() {
-		watchBatchMaxRevs = oldMaxRevs
-		s.store.Close()
-		os.Remove(tmpPath)
-	}()
-	batches := 3
-	watchBatchMaxRevs = 4
-
-	v := []byte("foo")
-	for i := 0; i < watchBatchMaxRevs*batches; i++ {
-		s.Put(v, v, lease.NoLease)
-	}
-
-	w := s.NewWatchStream()
-	w.Watch(v, nil, 1)
-	for i := 0; i < batches; i++ {
-		if resp := <-w.Chan(); len(resp.Events) != watchBatchMaxRevs {
-			t.Fatalf("len(events) = %d, want %d", len(resp.Events), watchBatchMaxRevs)
-		}
-	}
-
-	s.store.mu.Lock()
-	defer s.store.mu.Unlock()
-	if size := s.synced.size(); size != 1 {
-		t.Errorf("synced size = %d, want 1", size)
-	}
-}
-
-func TestNewMapwatcherToEventMap(t *testing.T) {
-	k0, k1, k2 := []byte("foo0"), []byte("foo1"), []byte("foo2")
-	v0, v1, v2 := []byte("bar0"), []byte("bar1"), []byte("bar2")
-
-	ws := []*watcher{{key: k0}, {key: k1}, {key: k2}}
-
-	evs := []storagepb.Event{
-		{
-			Type: storagepb.PUT,
-			Kv:   &storagepb.KeyValue{Key: k0, Value: v0},
-		},
-		{
-			Type: storagepb.PUT,
-			Kv:   &storagepb.KeyValue{Key: k1, Value: v1},
-		},
-		{
-			Type: storagepb.PUT,
-			Kv:   &storagepb.KeyValue{Key: k2, Value: v2},
-		},
-	}
-
-	tests := []struct {
-		sync []*watcher
-		evs  []storagepb.Event
-
-		wwe map[*watcher][]storagepb.Event
-	}{
-		// no watcher in sync, some events should return empty wwe
-		{
-			nil,
-			evs,
-			map[*watcher][]storagepb.Event{},
-		},
-
-		// one watcher in sync, one event that does not match the key of that
-		// watcher should return empty wwe
-		{
-			[]*watcher{ws[2]},
-			evs[:1],
-			map[*watcher][]storagepb.Event{},
-		},
-
-		// one watcher in sync, one event that matches the key of that
-		// watcher should return wwe with that matching watcher
-		{
-			[]*watcher{ws[1]},
-			evs[1:2],
-			map[*watcher][]storagepb.Event{
-				ws[1]: evs[1:2],
-			},
-		},
-
-		// two watchers in sync that watches two different keys, one event
-		// that matches the key of only one of the watcher should return wwe
-		// with the matching watcher
-		{
-			[]*watcher{ws[0], ws[2]},
-			evs[2:],
-			map[*watcher][]storagepb.Event{
-				ws[2]: evs[2:],
-			},
-		},
-
-		// two watchers in sync that watches the same key, two events that
-		// match the keys should return wwe with those two watchers
-		{
-			[]*watcher{ws[0], ws[1]},
-			evs[:2],
-			map[*watcher][]storagepb.Event{
-				ws[0]: evs[:1],
-				ws[1]: evs[1:2],
-			},
-		},
-	}
-
-	for i, tt := range tests {
-		wg := newWatcherGroup()
-		for _, w := range tt.sync {
-			wg.add(w)
-		}
-
-		gwe := newWatcherBatch(&wg, tt.evs)
-		if len(gwe) != len(tt.wwe) {
-			t.Errorf("#%d: len(gwe) got = %d, want = %d", i, len(gwe), len(tt.wwe))
-		}
-		// compare gwe and tt.wwe
-		for w, eb := range gwe {
-			if len(eb.evs) != len(tt.wwe[w]) {
-				t.Errorf("#%d: len(eb.evs) got = %d, want = %d", i, len(eb.evs), len(tt.wwe[w]))
-			}
-			if !reflect.DeepEqual(eb.evs, tt.wwe[w]) {
-				t.Errorf("#%d: reflect.DeepEqual events got = %v, want = true", i, false)
-			}
-		}
-	}
-}
diff --git a/storage/watcher.go b/storage/watcher.go
deleted file mode 100644
index 6449c45..0000000
--- a/storage/watcher.go
+++ /dev/null
@@ -1,156 +0,0 @@
-// Copyright 2015 CoreOS, Inc.
-//
-// Licensed under the Apache License, Version 2.0 (the "License");
-// you may not use this file except in compliance with the License.
-// You may obtain a copy of the License at
-//
-//     http://www.apache.org/licenses/LICENSE-2.0
-//
-// Unless required by applicable law or agreed to in writing, software
-// distributed under the License is distributed on an "AS IS" BASIS,
-// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-// See the License for the specific language governing permissions and
-// limitations under the License.
-
-package storage
-
-import (
-	"errors"
-	"sync"
-
-	"github.com/coreos/etcd/storage/storagepb"
-)
-
-var (
-	ErrWatcherNotExist = errors.New("storage: watcher does not exist")
-)
-
-type WatchID int64
-
-type WatchStream interface {
-	// Watch creates a watcher. The watcher watches the events happening or
-	// happened on the given key or range [key, end) from the given startRev.
-	//
-	// The whole event history can be watched unless compacted.
-	// If `startRev` <=0, watch observes events after currentRev.
-	//
-	// The returned `id` is the ID of this watcher. It appears as WatchID
-	// in events that are sent to the created watcher through stream channel.
-	//
-	Watch(key, end []byte, startRev int64) WatchID
-
-	// Chan returns a chan. All watch response will be sent to the returned chan.
-	Chan() <-chan WatchResponse
-
-	// RequestProgress requests the progress of the watcher with given ID. The response
-	// will only be sent if the watcher is currently synced.
-	// The responses will be sent through the WatchRespone Chan attached
-	// with this stream to ensure correct ordering.
-	// The responses contains no events. The revision in the response is the progress
-	// of the watchers since the watcher is currently synced.
-	RequestProgress(id WatchID)
-
-	// Cancel cancels a watcher by giving its ID. If watcher does not exist, an error will be
-	// returned.
-	Cancel(id WatchID) error
-
-	// Close closes Chan and release all related resources.
-	Close()
-
-	// Rev returns the current revision of the KV the stream watches on.
-	Rev() int64
-}
-
-type WatchResponse struct {
-	// WatchID is the WatchID of the watcher this response sent to.
-	WatchID WatchID
-
-	// Events contains all the events that needs to send.
-	Events []storagepb.Event
-
-	// Revision is the revision of the KV when the watchResponse is created.
-	// For a normal response, the revision should be the same as the last
-	// modified revision inside Events. For a delayed response to a unsynced
-	// watcher, the revision is greater than the last modified revision
-	// inside Events.
-	Revision int64
-
-	// CompactRevision is set when the watcher is cancelled due to compaction.
-	CompactRevision int64
-}
-
-// watchStream contains a collection of watchers that share
-// one streaming chan to send out watched events and other control events.
-type watchStream struct {
-	watchable watchable
-	ch        chan WatchResponse
-
-	mu sync.Mutex // guards fields below it
-	// nextID is the ID pre-allocated for next new watcher in this stream
-	nextID   WatchID
-	closed   bool
-	cancels  map[WatchID]cancelFunc
-	watchers map[WatchID]*watcher
-}
-
-// Watch creates a new watcher in the stream and returns its WatchID.
-// TODO: return error if ws is closed?
-func (ws *watchStream) Watch(key, end []byte, startRev int64) WatchID {
-	ws.mu.Lock()
-	defer ws.mu.Unlock()
-	if ws.closed {
-		return -1
-	}
-
-	id := ws.nextID
-	ws.nextID++
-
-	w, c := ws.watchable.watch(key, end, startRev, id, ws.ch)
-
-	ws.cancels[id] = c
-	ws.watchers[id] = w
-	return id
-}
-
-func (ws *watchStream) Chan() <-chan WatchResponse {
-	return ws.ch
-}
-
-func (ws *watchStream) Cancel(id WatchID) error {
-	cancel, ok := ws.cancels[id]
-	if !ok {
-		return ErrWatcherNotExist
-	}
-	cancel()
-	delete(ws.cancels, id)
-	delete(ws.watchers, id)
-	return nil
-}
-
-func (ws *watchStream) Close() {
-	ws.mu.Lock()
-	defer ws.mu.Unlock()
-
-	for _, cancel := range ws.cancels {
-		cancel()
-	}
-	ws.closed = true
-	close(ws.ch)
-	watchStreamGauge.Dec()
-}
-
-func (ws *watchStream) Rev() int64 {
-	ws.mu.Lock()
-	defer ws.mu.Unlock()
-	return ws.watchable.rev()
-}
-
-func (ws *watchStream) RequestProgress(id WatchID) {
-	ws.mu.Lock()
-	w, ok := ws.watchers[id]
-	ws.mu.Unlock()
-	if !ok {
-		return
-	}
-	ws.watchable.progress(w)
-}
diff --git a/storage/watcher_bench_test.go b/storage/watcher_bench_test.go
deleted file mode 100644
index 97d748f..0000000
--- a/storage/watcher_bench_test.go
+++ /dev/null
@@ -1,38 +0,0 @@
-// Copyright 2015 CoreOS, Inc.
-//
-// Licensed under the Apache License, Version 2.0 (the "License");
-// you may not use this file except in compliance with the License.
-// You may obtain a copy of the License at
-//
-//     http://www.apache.org/licenses/LICENSE-2.0
-//
-// Unless required by applicable law or agreed to in writing, software
-// distributed under the License is distributed on an "AS IS" BASIS,
-// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-// See the License for the specific language governing permissions and
-// limitations under the License.
-
-package storage
-
-import (
-	"fmt"
-	"testing"
-
-	"github.com/coreos/etcd/lease"
-	"github.com/coreos/etcd/storage/backend"
-)
-
-func BenchmarkKVWatcherMemoryUsage(b *testing.B) {
-	be, tmpPath := backend.NewDefaultTmpBackend()
-	watchable := newWatchableStore(be, &lease.FakeLessor{}, nil)
-
-	defer cleanup(watchable, be, tmpPath)
-
-	w := watchable.NewWatchStream()
-
-	b.ReportAllocs()
-	b.StartTimer()
-	for i := 0; i < b.N; i++ {
-		w.Watch([]byte(fmt.Sprint("foo", i)), nil, 0)
-	}
-}
diff --git a/storage/watcher_group.go b/storage/watcher_group.go
deleted file mode 100644
index f18852d..0000000
--- a/storage/watcher_group.go
+++ /dev/null
@@ -1,267 +0,0 @@
-// Copyright 2016 CoreOS, Inc.
-//
-// Licensed under the Apache License, Version 2.0 (the "License");
-// you may not use this file except in compliance with the License.
-// You may obtain a copy of the License at
-//
-//     http://www.apache.org/licenses/LICENSE-2.0
-//
-// Unless required by applicable law or agreed to in writing, software
-// distributed under the License is distributed on an "AS IS" BASIS,
-// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-// See the License for the specific language governing permissions and
-// limitations under the License.
-
-package storage
-
-import (
-	"math"
-
-	"github.com/coreos/etcd/pkg/adt"
-	"github.com/coreos/etcd/storage/storagepb"
-)
-
-var (
-	// watchBatchMaxRevs is the maximum distinct revisions that
-	// may be sent to an unsynced watcher at a time. Declared as
-	// var instead of const for testing purposes.
-	watchBatchMaxRevs = 1000
-)
-
-type eventBatch struct {
-	// evs is a batch of revision-ordered events
-	evs []storagepb.Event
-	// revs is the minimum unique revisions observed for this batch
-	revs int
-	// moreRev is first revision with more events following this batch
-	moreRev int64
-}
-
-func (eb *eventBatch) add(ev storagepb.Event) {
-	if eb.revs > watchBatchMaxRevs {
-		// maxed out batch size
-		return
-	}
-
-	if len(eb.evs) == 0 {
-		// base case
-		eb.revs = 1
-		eb.evs = append(eb.evs, ev)
-		return
-	}
-
-	// revision accounting
-	ebRev := eb.evs[len(eb.evs)-1].Kv.ModRevision
-	evRev := ev.Kv.ModRevision
-	if evRev > ebRev {
-		eb.revs++
-		if eb.revs > watchBatchMaxRevs {
-			eb.moreRev = evRev
-			return
-		}
-	}
-
-	eb.evs = append(eb.evs, ev)
-}
-
-type watcherBatch map[*watcher]*eventBatch
-
-func (wb watcherBatch) add(w *watcher, ev storagepb.Event) {
-	eb := wb[w]
-	if eb == nil {
-		eb = &eventBatch{}
-		wb[w] = eb
-	}
-	eb.add(ev)
-}
-
-func (wb watcherBatch) contains(w *watcher) bool {
-	_, ok := wb[w]
-	return ok
-}
-
-// newWatcherBatch maps watchers to their matched events. It enables quick
-// events look up by watcher.
-func newWatcherBatch(wg *watcherGroup, evs []storagepb.Event) watcherBatch {
-	wb := make(watcherBatch)
-	for _, ev := range evs {
-		for w := range wg.watcherSetByKey(string(ev.Kv.Key)) {
-			if ev.Kv.ModRevision >= w.cur {
-				// don't double notify
-				wb.add(w, ev)
-			}
-		}
-	}
-	return wb
-}
-
-type watcherSet map[*watcher]struct{}
-
-func (w watcherSet) add(wa *watcher) {
-	if _, ok := w[wa]; ok {
-		panic("add watcher twice!")
-	}
-	w[wa] = struct{}{}
-}
-
-func (w watcherSet) union(ws watcherSet) {
-	for wa := range ws {
-		w.add(wa)
-	}
-}
-
-func (w watcherSet) delete(wa *watcher) {
-	if _, ok := w[wa]; !ok {
-		panic("removing missing watcher!")
-	}
-	delete(w, wa)
-}
-
-type watcherSetByKey map[string]watcherSet
-
-func (w watcherSetByKey) add(wa *watcher) {
-	set := w[string(wa.key)]
-	if set == nil {
-		set = make(watcherSet)
-		w[string(wa.key)] = set
-	}
-	set.add(wa)
-}
-
-func (w watcherSetByKey) delete(wa *watcher) bool {
-	k := string(wa.key)
-	if v, ok := w[k]; ok {
-		if _, ok := v[wa]; ok {
-			delete(v, wa)
-			if len(v) == 0 {
-				// remove the set; nothing left
-				delete(w, k)
-			}
-			return true
-		}
-	}
-	return false
-}
-
-// watcherGroup is a collection of watchers organized by their ranges
-type watcherGroup struct {
-	// keyWatchers has the watchers that watch on a single key
-	keyWatchers watcherSetByKey
-	// ranges has the watchers that watch a range; it is sorted by interval
-	ranges adt.IntervalTree
-	// watchers is the set of all watchers
-	watchers watcherSet
-}
-
-func newWatcherGroup() watcherGroup {
-	return watcherGroup{
-		keyWatchers: make(watcherSetByKey),
-		watchers:    make(watcherSet),
-	}
-}
-
-// add puts a watcher in the group.
-func (wg *watcherGroup) add(wa *watcher) {
-	wg.watchers.add(wa)
-	if wa.end == nil {
-		wg.keyWatchers.add(wa)
-		return
-	}
-
-	// interval already registered?
-	ivl := adt.NewStringAffineInterval(string(wa.key), string(wa.end))
-	if iv := wg.ranges.Find(ivl); iv != nil {
-		iv.Val.(watcherSet).add(wa)
-		return
-	}
-
-	// not registered, put in interval tree
-	ws := make(watcherSet)
-	ws.add(wa)
-	wg.ranges.Insert(ivl, ws)
-}
-
-// contains is whether the given key has a watcher in the group.
-func (wg *watcherGroup) contains(key string) bool {
-	_, ok := wg.keyWatchers[key]
-	return ok || wg.ranges.Contains(adt.NewStringAffinePoint(key))
-}
-
-// size gives the number of unique watchers in the group.
-func (wg *watcherGroup) size() int { return len(wg.watchers) }
-
-// delete removes a watcher from the group.
-func (wg *watcherGroup) delete(wa *watcher) bool {
-	if _, ok := wg.watchers[wa]; !ok {
-		return false
-	}
-	wg.watchers.delete(wa)
-	if wa.end == nil {
-		wg.keyWatchers.delete(wa)
-		return true
-	}
-
-	ivl := adt.NewStringAffineInterval(string(wa.key), string(wa.end))
-	iv := wg.ranges.Find(ivl)
-	if iv == nil {
-		return false
-	}
-
-	ws := iv.Val.(watcherSet)
-	delete(ws, wa)
-	if len(ws) == 0 {
-		// remove interval missing watchers
-		if ok := wg.ranges.Delete(ivl); !ok {
-			panic("could not remove watcher from interval tree")
-		}
-	}
-
-	return true
-}
-
-func (wg *watcherGroup) scanMinRev(curRev int64, compactRev int64) int64 {
-	minRev := int64(math.MaxInt64)
-	for w := range wg.watchers {
-		if w.cur > curRev {
-			panic("watcher current revision should not exceed current revision")
-		}
-		if w.cur < compactRev {
-			select {
-			case w.ch <- WatchResponse{WatchID: w.id, CompactRevision: compactRev}:
-				wg.delete(w)
-			default:
-				// retry next time
-			}
-			continue
-		}
-		if minRev > w.cur {
-			minRev = w.cur
-		}
-	}
-	return minRev
-}
-
-// watcherSetByKey gets the set of watchers that receive events on the given key.
-func (wg *watcherGroup) watcherSetByKey(key string) watcherSet {
-	wkeys := wg.keyWatchers[key]
-	wranges := wg.ranges.Stab(adt.NewStringAffinePoint(key))
-
-	// zero-copy cases
-	switch {
-	case len(wranges) == 0:
-		// no need to merge ranges or copy; reuse single-key set
-		return wkeys
-	case len(wranges) == 0 && len(wkeys) == 0:
-		return nil
-	case len(wranges) == 1 && len(wkeys) == 0:
-		return wranges[0].Val.(watcherSet)
-	}
-
-	// copy case
-	ret := make(watcherSet)
-	ret.union(wg.keyWatchers[key])
-	for _, item := range wranges {
-		ret.union(item.Val.(watcherSet))
-	}
-	return ret
-}
diff --git a/storage/watcher_test.go b/storage/watcher_test.go
deleted file mode 100644
index e87d1c8..0000000
--- a/storage/watcher_test.go
+++ /dev/null
@@ -1,246 +0,0 @@
-// Copyright 2015 CoreOS, Inc.
-//
-// Licensed under the Apache License, Version 2.0 (the "License");
-// you may not use this file except in compliance with the License.
-// You may obtain a copy of the License at
-//
-//     http://www.apache.org/licenses/LICENSE-2.0
-//
-// Unless required by applicable law or agreed to in writing, software
-// distributed under the License is distributed on an "AS IS" BASIS,
-// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-// See the License for the specific language governing permissions and
-// limitations under the License.
-
-package storage
-
-import (
-	"bytes"
-	"os"
-	"reflect"
-	"testing"
-	"time"
-
-	"github.com/coreos/etcd/lease"
-	"github.com/coreos/etcd/storage/backend"
-)
-
-// TestWatcherWatchID tests that each watcher provides unique watchID,
-// and the watched event attaches the correct watchID.
-func TestWatcherWatchID(t *testing.T) {
-	b, tmpPath := backend.NewDefaultTmpBackend()
-	s := WatchableKV(newWatchableStore(b, &lease.FakeLessor{}, nil))
-	defer cleanup(s, b, tmpPath)
-
-	w := s.NewWatchStream()
-	defer w.Close()
-
-	idm := make(map[WatchID]struct{})
-
-	for i := 0; i < 10; i++ {
-		id := w.Watch([]byte("foo"), nil, 0)
-		if _, ok := idm[id]; ok {
-			t.Errorf("#%d: id %d exists", i, id)
-		}
-		idm[id] = struct{}{}
-
-		s.Put([]byte("foo"), []byte("bar"), lease.NoLease)
-
-		resp := <-w.Chan()
-		if resp.WatchID != id {
-			t.Errorf("#%d: watch id in event = %d, want %d", i, resp.WatchID, id)
-		}
-
-		if err := w.Cancel(id); err != nil {
-			t.Error(err)
-		}
-	}
-
-	s.Put([]byte("foo2"), []byte("bar"), lease.NoLease)
-
-	// unsynced watchers
-	for i := 10; i < 20; i++ {
-		id := w.Watch([]byte("foo2"), nil, 1)
-		if _, ok := idm[id]; ok {
-			t.Errorf("#%d: id %d exists", i, id)
-		}
-		idm[id] = struct{}{}
-
-		resp := <-w.Chan()
-		if resp.WatchID != id {
-			t.Errorf("#%d: watch id in event = %d, want %d", i, resp.WatchID, id)
-		}
-
-		if err := w.Cancel(id); err != nil {
-			t.Error(err)
-		}
-	}
-}
-
-// TestWatcherWatchPrefix tests if Watch operation correctly watches
-// and returns events with matching prefixes.
-func TestWatcherWatchPrefix(t *testing.T) {
-	b, tmpPath := backend.NewDefaultTmpBackend()
-	s := WatchableKV(newWatchableStore(b, &lease.FakeLessor{}, nil))
-	defer cleanup(s, b, tmpPath)
-
-	w := s.NewWatchStream()
-	defer w.Close()
-
-	idm := make(map[WatchID]struct{})
-
-	val := []byte("bar")
-	keyWatch, keyEnd, keyPut := []byte("foo"), []byte("fop"), []byte("foobar")
-
-	for i := 0; i < 10; i++ {
-		id := w.Watch(keyWatch, keyEnd, 0)
-		if _, ok := idm[id]; ok {
-			t.Errorf("#%d: unexpected duplicated id %x", i, id)
-		}
-		idm[id] = struct{}{}
-
-		s.Put(keyPut, val, lease.NoLease)
-
-		resp := <-w.Chan()
-		if resp.WatchID != id {
-			t.Errorf("#%d: watch id in event = %d, want %d", i, resp.WatchID, id)
-		}
-
-		if err := w.Cancel(id); err != nil {
-			t.Errorf("#%d: unexpected cancel error %v", i, err)
-		}
-
-		if len(resp.Events) != 1 {
-			t.Errorf("#%d: len(resp.Events) got = %d, want = 1", i, len(resp.Events))
-		}
-		if len(resp.Events) == 1 {
-			if !bytes.Equal(resp.Events[0].Kv.Key, keyPut) {
-				t.Errorf("#%d: resp.Events got = %s, want = %s", i, resp.Events[0].Kv.Key, keyPut)
-			}
-		}
-	}
-
-	keyWatch1, keyEnd1, keyPut1 := []byte("foo1"), []byte("foo2"), []byte("foo1bar")
-	s.Put(keyPut1, val, lease.NoLease)
-
-	// unsynced watchers
-	for i := 10; i < 15; i++ {
-		id := w.Watch(keyWatch1, keyEnd1, 1)
-		if _, ok := idm[id]; ok {
-			t.Errorf("#%d: id %d exists", i, id)
-		}
-		idm[id] = struct{}{}
-
-		resp := <-w.Chan()
-		if resp.WatchID != id {
-			t.Errorf("#%d: watch id in event = %d, want %d", i, resp.WatchID, id)
-		}
-
-		if err := w.Cancel(id); err != nil {
-			t.Error(err)
-		}
-
-		if len(resp.Events) != 1 {
-			t.Errorf("#%d: len(resp.Events) got = %d, want = 1", i, len(resp.Events))
-		}
-		if len(resp.Events) == 1 {
-			if !bytes.Equal(resp.Events[0].Kv.Key, keyPut1) {
-				t.Errorf("#%d: resp.Events got = %s, want = %s", i, resp.Events[0].Kv.Key, keyPut1)
-			}
-		}
-	}
-}
-
-// TestWatchStreamCancelWatcherByID ensures cancel calls the cancel func of the watcher
-// with given id inside watchStream.
-func TestWatchStreamCancelWatcherByID(t *testing.T) {
-	b, tmpPath := backend.NewDefaultTmpBackend()
-	s := WatchableKV(newWatchableStore(b, &lease.FakeLessor{}, nil))
-	defer cleanup(s, b, tmpPath)
-
-	w := s.NewWatchStream()
-	defer w.Close()
-
-	id := w.Watch([]byte("foo"), nil, 0)
-
-	tests := []struct {
-		cancelID WatchID
-		werr     error
-	}{
-		// no error should be returned when cancel the created watcher.
-		{id, nil},
-		// not exist error should be returned when cancel again.
-		{id, ErrWatcherNotExist},
-		// not exist error should be returned when cancel a bad id.
-		{id + 1, ErrWatcherNotExist},
-	}
-
-	for i, tt := range tests {
-		gerr := w.Cancel(tt.cancelID)
-
-		if gerr != tt.werr {
-			t.Errorf("#%d: err = %v, want %v", i, gerr, tt.werr)
-		}
-	}
-
-	if l := len(w.(*watchStream).cancels); l != 0 {
-		t.Errorf("cancels = %d, want 0", l)
-	}
-}
-
-// TestWatcherRequestProgress ensures synced watcher can correctly
-// report its correct progress.
-func TestWatcherRequestProgress(t *testing.T) {
-	b, tmpPath := backend.NewDefaultTmpBackend()
-
-	// manually create watchableStore instead of newWatchableStore
-	// because newWatchableStore automatically calls syncWatchers
-	// method to sync watchers in unsynced map. We want to keep watchers
-	// in unsynced to test if syncWatchers works as expected.
-	s := &watchableStore{
-		store:    NewStore(b, &lease.FakeLessor{}, nil),
-		unsynced: newWatcherGroup(),
-		synced:   newWatcherGroup(),
-	}
-
-	defer func() {
-		s.store.Close()
-		os.Remove(tmpPath)
-	}()
-
-	testKey := []byte("foo")
-	notTestKey := []byte("bad")
-	testValue := []byte("bar")
-	s.Put(testKey, testValue, lease.NoLease)
-
-	w := s.NewWatchStream()
-
-	badID := WatchID(1000)
-	w.RequestProgress(badID)
-	select {
-	case resp := <-w.Chan():
-		t.Fatalf("unexpected %+v", resp)
-	default:
-	}
-
-	id := w.Watch(notTestKey, nil, 1)
-	w.RequestProgress(id)
-	select {
-	case resp := <-w.Chan():
-		t.Fatalf("unexpected %+v", resp)
-	default:
-	}
-
-	s.syncWatchers()
-
-	w.RequestProgress(id)
-	wrs := WatchResponse{WatchID: 0, Revision: 2}
-	select {
-	case resp := <-w.Chan():
-		if !reflect.DeepEqual(resp, wrs) {
-			t.Fatalf("got %+v, expect %+v", resp, wrs)
-		}
-	case <-time.After(time.Second):
-		t.Fatal("failed to receive progress")
-	}
-}
diff --git a/test b/test
index 6c48758..5268377 100755
--- a/test
+++ b/test
@@ -28,7 +28,7 @@ ln -s ${PWD}/cmd/vendor $GOPATH/src
 
 # Hack: gofmt ./ will recursively check the .git directory. So use *.go for gofmt.
 PKGS=`ls pkg/*/*go  | cut -f1,2 -d/ | sort | uniq`
-TESTABLE_AND_FORMATTABLE="client clientv3 discovery error etcdctl/ctlv2 etcdctl/ctlv3 etcdmain etcdserver etcdserver/auth etcdserver/api/v2http etcdserver/api/v2http/httptypes $PKGS proxy/httpproxy proxy/tcpproxy raft snap storage storage/backend store version wal rafthttp"
+TESTABLE_AND_FORMATTABLE="client clientv3 discovery error etcdctl/ctlv2 etcdctl/ctlv3 etcdmain etcdserver etcdserver/auth etcdserver/api/v2http etcdserver/api/v2http/httptypes $PKGS proxy/httpproxy proxy/tcpproxy raft snap mvcc mvcc/backend store version wal rafthttp"
 FORMATTABLE="$TESTABLE_AND_FORMATTABLE *.go etcdctl/ integration clientv3/integration e2e alarm"
 
 # user has not provided PKG override
@@ -153,7 +153,7 @@ function dep_tests {
 	echo "Checking package dependencies..."
 	# don't pull in etcdserver package
 	pushd clientv3 >/dev/null
-	badpkg="(etcdserver|storage)"
+	badpkg="(etcdserver|mvcc)"
 	deps=`go list -f '{{ .Deps }}'  | sed 's/ /\n/g' | egrep "${badpkg}" | egrep -v "${badpkg}/" || echo ""`
 	popd >/dev/null
 	if [ ! -z "$deps" ]; then
diff --git a/tools/benchmark/cmd/mvcc-put.go b/tools/benchmark/cmd/mvcc-put.go
new file mode 100644
index 0000000..448d2b1
--- /dev/null
+++ b/tools/benchmark/cmd/mvcc-put.go
@@ -0,0 +1,150 @@
+// Copyright 2015 Nippon Telegraph and Telephone Corporation.
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//     http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+package cmd
+
+import (
+	"crypto/rand"
+	"fmt"
+	"os"
+	"runtime/pprof"
+	"time"
+
+	"github.com/coreos/etcd/lease"
+	"github.com/spf13/cobra"
+)
+
+// mvccPutCmd represents a storage put performance benchmarking tool
+var mvccPutCmd = &cobra.Command{
+	Use:   "put",
+	Short: "Benchmark put performance of storage",
+
+	Run: mvccPutFunc,
+}
+
+var (
+	totalNrKeys    int
+	storageKeySize int
+	valueSize      int
+	txn            bool
+)
+
+func init() {
+	mvccCmd.AddCommand(mvccPutCmd)
+
+	mvccPutCmd.Flags().IntVar(&totalNrKeys, "total", 100, "a total number of keys to put")
+	mvccPutCmd.Flags().IntVar(&storageKeySize, "key-size", 64, "a size of key (Byte)")
+	mvccPutCmd.Flags().IntVar(&valueSize, "value-size", 64, "a size of value (Byte)")
+	mvccPutCmd.Flags().BoolVar(&txn, "txn", false, "put a key in transaction or not")
+
+	// TODO: after the PR https://github.com/spf13/cobra/pull/220 is merged, the below pprof related flags should be moved to RootCmd
+	mvccPutCmd.Flags().StringVar(&cpuProfPath, "cpuprofile", "", "the path of file for storing cpu profile result")
+	mvccPutCmd.Flags().StringVar(&memProfPath, "memprofile", "", "the path of file for storing heap profile result")
+
+}
+
+func createBytesSlice(bytesN, sliceN int) [][]byte {
+	rs := make([][]byte, sliceN)
+	for i := range rs {
+		rs[i] = make([]byte, bytesN)
+		if _, err := rand.Read(rs[i]); err != nil {
+			panic(err)
+		}
+	}
+	return rs
+}
+
+func mvccPutFunc(cmd *cobra.Command, args []string) {
+	if cpuProfPath != "" {
+		f, err := os.Create(cpuProfPath)
+		if err != nil {
+			fmt.Fprintln(os.Stderr, "Failed to create a file for storing cpu profile result: ", err)
+			os.Exit(1)
+		}
+
+		err = pprof.StartCPUProfile(f)
+		if err != nil {
+			fmt.Fprintln(os.Stderr, "Failed to start cpu profile: ", err)
+			os.Exit(1)
+		}
+		defer pprof.StopCPUProfile()
+	}
+
+	if memProfPath != "" {
+		f, err := os.Create(memProfPath)
+		if err != nil {
+			fmt.Fprintln(os.Stderr, "Failed to create a file for storing heap profile result: ", err)
+			os.Exit(1)
+		}
+
+		defer func() {
+			err := pprof.WriteHeapProfile(f)
+			if err != nil {
+				fmt.Fprintln(os.Stderr, "Failed to write heap profile result: ", err)
+				// can do nothing for handling the error
+			}
+		}()
+	}
+
+	keys := createBytesSlice(storageKeySize, totalNrKeys)
+	vals := createBytesSlice(valueSize, totalNrKeys)
+
+	latencies := make([]time.Duration, totalNrKeys)
+
+	minLat := time.Duration(1<<63 - 1)
+	maxLat := time.Duration(0)
+
+	for i := 0; i < totalNrKeys; i++ {
+		begin := time.Now()
+
+		if txn {
+			id := s.TxnBegin()
+			if _, err := s.TxnPut(id, keys[i], vals[i], lease.NoLease); err != nil {
+				fmt.Fprintln(os.Stderr, "txn put error:", err)
+				os.Exit(1)
+			}
+			s.TxnEnd(id)
+		} else {
+			s.Put(keys[i], vals[i], lease.NoLease)
+		}
+
+		end := time.Now()
+
+		lat := end.Sub(begin)
+		latencies[i] = lat
+		if maxLat < lat {
+			maxLat = lat
+		}
+		if lat < minLat {
+			minLat = lat
+		}
+	}
+
+	total := time.Duration(0)
+
+	for _, lat := range latencies {
+		total += lat
+	}
+
+	fmt.Printf("total: %v\n", total)
+	fmt.Printf("average: %v\n", total/time.Duration(totalNrKeys))
+	fmt.Printf("rate: %4.4f\n", float64(totalNrKeys)/total.Seconds())
+	fmt.Printf("minimum latency: %v\n", minLat)
+	fmt.Printf("maximum latency: %v\n", maxLat)
+
+	// TODO: Currently this benchmark doesn't use the common histogram infrastructure.
+	// This is because an accuracy of the infrastructure isn't suitable for measuring
+	// performance of kv storage:
+	// https://github.com/coreos/etcd/pull/4070#issuecomment-167954149
+}
diff --git a/tools/benchmark/cmd/mvcc.go b/tools/benchmark/cmd/mvcc.go
new file mode 100644
index 0000000..ddd3e2c
--- /dev/null
+++ b/tools/benchmark/cmd/mvcc.go
@@ -0,0 +1,59 @@
+// Copyright 2015 Nippon Telegraph and Telephone Corporation.
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//     http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+package cmd
+
+import (
+	"os"
+	"time"
+
+	"github.com/coreos/etcd/lease"
+	"github.com/coreos/etcd/mvcc"
+	"github.com/coreos/etcd/mvcc/backend"
+	"github.com/spf13/cobra"
+)
+
+var (
+	batchInterval int
+	batchLimit    int
+
+	s mvcc.KV
+)
+
+func initMVCC() {
+	be := backend.New("mvcc-bench", time.Duration(batchInterval), batchLimit)
+	s = mvcc.NewStore(be, &lease.FakeLessor{}, nil)
+	os.Remove("mvcc-bench") // boltDB has an opened fd, so removing the file is ok
+}
+
+// mvccCmd represents the MVCC storage benchmarking tools
+var mvccCmd = &cobra.Command{
+	Use:   "mvcc",
+	Short: "Benchmark mvcc",
+	Long: `storage subcommand is a set of various benchmark tools for MVCC storage subsystem of etcd.
+Actual benchmarks are implemented as its subcommands.`,
+
+	PersistentPreRun: mvccPreRun,
+}
+
+func init() {
+	RootCmd.AddCommand(mvccCmd)
+
+	mvccCmd.PersistentFlags().IntVar(&batchInterval, "batch-interval", 100, "Interval of batching (milliseconds)")
+	mvccCmd.PersistentFlags().IntVar(&batchLimit, "batch-limit", 10000, "A limit of batched transaction")
+}
+
+func mvccPreRun(cmd *cobra.Command, args []string) {
+	initMVCC()
+}
diff --git a/tools/benchmark/cmd/storage-put.go b/tools/benchmark/cmd/storage-put.go
deleted file mode 100644
index b971fd6..0000000
--- a/tools/benchmark/cmd/storage-put.go
+++ /dev/null
@@ -1,150 +0,0 @@
-// Copyright 2015 Nippon Telegraph and Telephone Corporation.
-//
-// Licensed under the Apache License, Version 2.0 (the "License");
-// you may not use this file except in compliance with the License.
-// You may obtain a copy of the License at
-//
-//     http://www.apache.org/licenses/LICENSE-2.0
-//
-// Unless required by applicable law or agreed to in writing, software
-// distributed under the License is distributed on an "AS IS" BASIS,
-// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-// See the License for the specific language governing permissions and
-// limitations under the License.
-
-package cmd
-
-import (
-	"crypto/rand"
-	"fmt"
-	"os"
-	"runtime/pprof"
-	"time"
-
-	"github.com/coreos/etcd/lease"
-	"github.com/spf13/cobra"
-)
-
-// storagePutCmd represents a storage put performance benchmarking tool
-var storagePutCmd = &cobra.Command{
-	Use:   "put",
-	Short: "Benchmark put performance of storage",
-
-	Run: storagePutFunc,
-}
-
-var (
-	totalNrKeys    int
-	storageKeySize int
-	valueSize      int
-	txn            bool
-)
-
-func init() {
-	storageCmd.AddCommand(storagePutCmd)
-
-	storagePutCmd.Flags().IntVar(&totalNrKeys, "total", 100, "a total number of keys to put")
-	storagePutCmd.Flags().IntVar(&storageKeySize, "key-size", 64, "a size of key (Byte)")
-	storagePutCmd.Flags().IntVar(&valueSize, "value-size", 64, "a size of value (Byte)")
-	storagePutCmd.Flags().BoolVar(&txn, "txn", false, "put a key in transaction or not")
-
-	// TODO: after the PR https://github.com/spf13/cobra/pull/220 is merged, the below pprof related flags should be moved to RootCmd
-	storagePutCmd.Flags().StringVar(&cpuProfPath, "cpuprofile", "", "the path of file for storing cpu profile result")
-	storagePutCmd.Flags().StringVar(&memProfPath, "memprofile", "", "the path of file for storing heap profile result")
-
-}
-
-func createBytesSlice(bytesN, sliceN int) [][]byte {
-	rs := make([][]byte, sliceN)
-	for i := range rs {
-		rs[i] = make([]byte, bytesN)
-		if _, err := rand.Read(rs[i]); err != nil {
-			panic(err)
-		}
-	}
-	return rs
-}
-
-func storagePutFunc(cmd *cobra.Command, args []string) {
-	if cpuProfPath != "" {
-		f, err := os.Create(cpuProfPath)
-		if err != nil {
-			fmt.Fprintln(os.Stderr, "Failed to create a file for storing cpu profile result: ", err)
-			os.Exit(1)
-		}
-
-		err = pprof.StartCPUProfile(f)
-		if err != nil {
-			fmt.Fprintln(os.Stderr, "Failed to start cpu profile: ", err)
-			os.Exit(1)
-		}
-		defer pprof.StopCPUProfile()
-	}
-
-	if memProfPath != "" {
-		f, err := os.Create(memProfPath)
-		if err != nil {
-			fmt.Fprintln(os.Stderr, "Failed to create a file for storing heap profile result: ", err)
-			os.Exit(1)
-		}
-
-		defer func() {
-			err := pprof.WriteHeapProfile(f)
-			if err != nil {
-				fmt.Fprintln(os.Stderr, "Failed to write heap profile result: ", err)
-				// can do nothing for handling the error
-			}
-		}()
-	}
-
-	keys := createBytesSlice(storageKeySize, totalNrKeys)
-	vals := createBytesSlice(valueSize, totalNrKeys)
-
-	latencies := make([]time.Duration, totalNrKeys)
-
-	minLat := time.Duration(1<<63 - 1)
-	maxLat := time.Duration(0)
-
-	for i := 0; i < totalNrKeys; i++ {
-		begin := time.Now()
-
-		if txn {
-			id := s.TxnBegin()
-			if _, err := s.TxnPut(id, keys[i], vals[i], lease.NoLease); err != nil {
-				fmt.Fprintln(os.Stderr, "txn put error:", err)
-				os.Exit(1)
-			}
-			s.TxnEnd(id)
-		} else {
-			s.Put(keys[i], vals[i], lease.NoLease)
-		}
-
-		end := time.Now()
-
-		lat := end.Sub(begin)
-		latencies[i] = lat
-		if maxLat < lat {
-			maxLat = lat
-		}
-		if lat < minLat {
-			minLat = lat
-		}
-	}
-
-	total := time.Duration(0)
-
-	for _, lat := range latencies {
-		total += lat
-	}
-
-	fmt.Printf("total: %v\n", total)
-	fmt.Printf("average: %v\n", total/time.Duration(totalNrKeys))
-	fmt.Printf("rate: %4.4f\n", float64(totalNrKeys)/total.Seconds())
-	fmt.Printf("minimum latency: %v\n", minLat)
-	fmt.Printf("maximum latency: %v\n", maxLat)
-
-	// TODO: Currently this benchmark doesn't use the common histogram infrastructure.
-	// This is because an accuracy of the infrastructure isn't suitable for measuring
-	// performance of kv storage:
-	// https://github.com/coreos/etcd/pull/4070#issuecomment-167954149
-}
diff --git a/tools/benchmark/cmd/storage.go b/tools/benchmark/cmd/storage.go
deleted file mode 100644
index 0420cdb..0000000
--- a/tools/benchmark/cmd/storage.go
+++ /dev/null
@@ -1,59 +0,0 @@
-// Copyright 2015 Nippon Telegraph and Telephone Corporation.
-//
-// Licensed under the Apache License, Version 2.0 (the "License");
-// you may not use this file except in compliance with the License.
-// You may obtain a copy of the License at
-//
-//     http://www.apache.org/licenses/LICENSE-2.0
-//
-// Unless required by applicable law or agreed to in writing, software
-// distributed under the License is distributed on an "AS IS" BASIS,
-// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-// See the License for the specific language governing permissions and
-// limitations under the License.
-
-package cmd
-
-import (
-	"os"
-	"time"
-
-	"github.com/coreos/etcd/lease"
-	"github.com/coreos/etcd/storage"
-	"github.com/coreos/etcd/storage/backend"
-	"github.com/spf13/cobra"
-)
-
-var (
-	batchInterval int
-	batchLimit    int
-
-	s storage.KV
-)
-
-func initStorage() {
-	be := backend.New("storage-bench", time.Duration(batchInterval), batchLimit)
-	s = storage.NewStore(be, &lease.FakeLessor{}, nil)
-	os.Remove("storage-bench") // boltDB has an opened fd, so removing the file is ok
-}
-
-// storageCmd represents the storage benchmarking tools
-var storageCmd = &cobra.Command{
-	Use:   "storage",
-	Short: "Benchmark storage",
-	Long: `storage subcommand is a set of various benchmark tools for storage subsystem of etcd.
-Actual benchmarks are implemented as its subcommands.`,
-
-	PersistentPreRun: storagePreRun,
-}
-
-func init() {
-	RootCmd.AddCommand(storageCmd)
-
-	storageCmd.PersistentFlags().IntVar(&batchInterval, "batch-interval", 100, "Interval of batching (milliseconds)")
-	storageCmd.PersistentFlags().IntVar(&batchLimit, "batch-limit", 10000, "A limit of batched transaction")
-}
-
-func storagePreRun(cmd *cobra.Command, args []string) {
-	initStorage()
-}
diff --git a/wal/walpb/record.pb.go b/wal/walpb/record.pb.go
index e164fdc..508bd45 100644
--- a/wal/walpb/record.pb.go
+++ b/wal/walpb/record.pb.go
@@ -501,16 +501,16 @@ var (
 )
 
 var fileDescriptorRecord = []byte{
-	// 175 bytes of a gzipped FileDescriptorProto
+	// 161 bytes of a gzipped FileDescriptorProto
 	0x1f, 0x8b, 0x08, 0x00, 0x00, 0x09, 0x6e, 0x88, 0x02, 0xff, 0xe2, 0xe2, 0x29, 0x4a, 0x4d, 0xce,
 	0x2f, 0x4a, 0xd1, 0x2b, 0x28, 0xca, 0x2f, 0xc9, 0x17, 0x62, 0x2d, 0x4f, 0xcc, 0x29, 0x48, 0x92,
-	0x12, 0x49, 0xcf, 0x4f, 0xcf, 0x07, 0x8b, 0xe8, 0x83, 0x58, 0x10, 0x49, 0x25, 0x3f, 0x2e, 0xb6,
-	0x20, 0xb0, 0x62, 0x21, 0x09, 0x2e, 0x96, 0x92, 0xca, 0x82, 0x54, 0x09, 0x46, 0x05, 0x46, 0x0d,
-	0x66, 0x27, 0x96, 0x13, 0xf7, 0xe4, 0x19, 0x82, 0xc0, 0x22, 0x42, 0x62, 0x5c, 0xcc, 0xc9, 0x45,
-	0xc9, 0x12, 0x4c, 0x40, 0x09, 0x5e, 0xa8, 0x04, 0x48, 0x40, 0x48, 0x88, 0x8b, 0x25, 0x25, 0xb1,
-	0x24, 0x51, 0x82, 0x19, 0x28, 0xc1, 0x13, 0x04, 0x66, 0x2b, 0x39, 0x70, 0x71, 0x04, 0xe7, 0x25,
-	0x16, 0x14, 0x67, 0xe4, 0x97, 0x08, 0x49, 0x71, 0xb1, 0x66, 0xe6, 0xa5, 0xa4, 0x56, 0x80, 0x8d,
-	0x64, 0x81, 0xea, 0x84, 0x08, 0x81, 0x6d, 0x4b, 0x2d, 0xca, 0x05, 0x1b, 0xca, 0x02, 0xb7, 0x0d,
-	0x28, 0xe2, 0x24, 0x70, 0xe2, 0xa1, 0x1c, 0xc3, 0x89, 0x47, 0x72, 0x8c, 0x17, 0x80, 0xf8, 0x01,
-	0x10, 0x03, 0x02, 0x00, 0x00, 0xff, 0xff, 0xe5, 0x20, 0xf5, 0xbc, 0xcf, 0x00, 0x00, 0x00,
+	0x12, 0x49, 0xcf, 0x4f, 0xcf, 0x07, 0x8b, 0xe8, 0x83, 0x58, 0x10, 0x49, 0x25, 0x5b, 0x2e, 0xb6,
+	0x20, 0xb0, 0x62, 0x21, 0x21, 0x2e, 0x96, 0x92, 0xca, 0x82, 0x54, 0x09, 0x46, 0x05, 0x46, 0x0d,
+	0x66, 0x27, 0x96, 0x13, 0xf7, 0xe4, 0x19, 0x84, 0x04, 0xb9, 0x98, 0x93, 0x8b, 0x92, 0x25, 0x98,
+	0x80, 0x42, 0xbc, 0x50, 0x21, 0x1e, 0x2e, 0x96, 0x94, 0xc4, 0x92, 0x44, 0x09, 0x66, 0xa0, 0x18,
+	0x8f, 0x92, 0x31, 0x17, 0x47, 0x70, 0x5e, 0x62, 0x41, 0x71, 0x46, 0x7e, 0x89, 0x90, 0x30, 0x17,
+	0x6b, 0x66, 0x5e, 0x4a, 0x6a, 0x05, 0xd8, 0x04, 0x16, 0xa8, 0x72, 0x90, 0xa9, 0xa9, 0x45, 0xb9,
+	0x60, 0x23, 0xa0, 0x62, 0x4e, 0x02, 0x27, 0x1e, 0xca, 0x31, 0x9c, 0x78, 0x24, 0xc7, 0x78, 0x01,
+	0x88, 0x1f, 0x00, 0x31, 0x20, 0x00, 0x00, 0xff, 0xff, 0x73, 0xbe, 0x84, 0x03, 0xb1, 0x00, 0x00,
+	0x00,
 }
