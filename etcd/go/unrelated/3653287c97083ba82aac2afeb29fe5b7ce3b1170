commit 3653287c97083ba82aac2afeb29fe5b7ce3b1170
Author: Blake Mizerany <blake.mizerany@gmail.com>
Date:   Wed Aug 13 16:09:50 2014 -0700

    Revert "Documentation -> doc"
    
    This reverts commit 9c179224aa04f0f25c52dd7b66576c3faf56cfc9.

diff --git a/Documentation/api.md b/Documentation/api.md
new file mode 100644
index 0000000..ab2a83a
--- /dev/null
+++ b/Documentation/api.md
@@ -0,0 +1,1314 @@
+# etcd API
+
+## Running a Single Machine Cluster
+
+These examples will use a single machine cluster to show you the basics of the etcd REST API.
+Let's start etcd:
+
+```sh
+./bin/etcd -data-dir machine0 -name machine0
+```
+
+This will bring up etcd listening on default ports (4001 for client communication and 7001 for server-to-server communication).
+The `-data-dir machine0` argument tells etcd to write machine configuration, logs and snapshots to the `./machine0/` directory.
+The `-name machine0` tells the rest of the cluster that this machine is named machine0.
+
+## Getting the etcd version
+
+The etcd version of a specific instance can be obtained from the `/version` endpoint.
+
+```sh
+curl -L http://127.0.0.1:4001/version
+```
+
+## Key Space Operations
+
+The primary API of etcd is a hierarchical key space.
+The key space consists of directories and keys which are generically referred to as "nodes".
+
+
+### Setting the value of a key
+
+Let's set the first key-value pair in the datastore.
+In this case the key is `/message` and the value is `Hello world`.
+
+```sh
+curl -L http://127.0.0.1:4001/v2/keys/message -XPUT -d value="Hello world"
+```
+
+```json
+{
+    "action": "set",
+    "node": {
+        "createdIndex": 2,
+        "key": "/message",
+        "modifiedIndex": 2,
+        "value": "Hello world"
+    }
+}
+```
+
+The response object contains several attributes:
+
+1. `action`: the action of the request that was just made.
+The request attempted to modify `node.value` via a `PUT` HTTP request, thus the value of action is `set`.
+
+2. `node.key`: the HTTP path to which the request was made.
+We set `/message` to `Hello world`, so the key field is `/message`.
+etcd uses a file-system-like structure to represent the key-value pairs, therefore all keys start with `/`.
+
+3. `node.value`: the value of the key after resolving the request.
+In this case, a successful request was made that attempted to change the node's value to `Hello world`.
+
+4. `node.createdIndex`: an index is a unique, monotonically-incrementing integer created for each change to etcd.
+This specific index reflects the point in the etcd state machine at which a given key was created.
+You may notice that in this example the index is `2` even though it is the first request you sent to the server.
+This is because there are internal commands that also change the state behind the scenes, like adding and syncing servers.
+
+5. `node.modifiedIndex`: like `node.createdIndex`, this attribute is also an etcd index.
+Actions that cause the value to change include `set`, `delete`, `update`, `create`, `compareAndSwap` and `compareAndDelete`.
+Since the `get` and `watch` commands do not change state in the store, they do not change the value of `node.modifiedIndex`.
+
+
+### Response Headers
+
+etcd includes a few HTTP headers in responses that provide global information about the etcd cluster that serviced a request:
+
+```
+X-Etcd-Index: 35
+X-Raft-Index: 5398
+X-Raft-Term: 0
+```
+
+- `X-Etcd-Index` is the current etcd index as explained above.
+- `X-Raft-Index` is similar to the etcd index but is for the underlying raft protocol
+- `X-Raft-Term` is an integer that will increase whenever an etcd master election happens in the cluster. If this number is increasing rapidly, you may need to tune the election timeout. See the [tuning][tuning] section for details.
+
+[tuning]: #tuning
+
+
+### Get the value of a key
+
+We can get the value that we just set in `/message` by issuing a `GET` request:
+
+```sh
+curl -L http://127.0.0.1:4001/v2/keys/message
+```
+
+```json
+{
+    "action": "get",
+    "node": {
+        "createdIndex": 2,
+        "key": "/message",
+        "modifiedIndex": 2,
+        "value": "Hello world"
+    }
+}
+```
+
+
+### Changing the value of a key
+
+You can change the value of `/message` from `Hello world` to `Hello etcd` with another `PUT` request to the key:
+
+```sh
+curl -L http://127.0.0.1:4001/v2/keys/message -XPUT -d value="Hello etcd"
+```
+
+```json
+{
+    "action": "set",
+    "node": {
+        "createdIndex": 3,
+        "key": "/message",
+        "modifiedIndex": 3,
+        "value": "Hello etcd"
+    },
+    "prevNode": {
+    	"createdIndex": 2,
+    	"key": "/message",
+    	"value": "Hello world",
+    	"modifiedIndex": 2
+    }
+}
+```
+
+Here we introduce a new field: `prevNode`. The `prevNode` field represents what the state of a given node was before resolving the request at hand. The `prevNode` field follows the same format as the `node`, and is omitted in the event that there was no previous state for a given node.
+
+### Deleting a key
+
+You can remove the `/message` key with a `DELETE` request:
+
+```sh
+curl -L http://127.0.0.1:4001/v2/keys/message -XDELETE
+```
+
+```json
+{
+    "action": "delete",
+    "node": {
+        "createdIndex": 3,
+        "key": "/message",
+        "modifiedIndex": 4
+    },
+    "prevNode": {
+    	"key": "/message",
+    	"value": "Hello etcd",
+    	"modifiedIndex": 3,
+    	"createdIndex": 3
+    }
+}
+```
+
+
+### Using key TTL
+
+Keys in etcd can be set to expire after a specified number of seconds.
+You can do this by setting a TTL (time to live) on the key when sending a `PUT` request:
+
+```sh
+curl -L http://127.0.0.1:4001/v2/keys/foo -XPUT -d value=bar -d ttl=5
+```
+
+```json
+{
+    "action": "set",
+    "node": {
+        "createdIndex": 5,
+        "expiration": "2013-12-04T12:01:21.874888581-08:00",
+        "key": "/foo",
+        "modifiedIndex": 5,
+        "ttl": 5,
+        "value": "bar"
+    }
+}
+```
+
+Note the two new fields in response:
+
+1. The `expiration` is the time at which this key will expire and be deleted.
+
+2. The `ttl` is the specified time to live for the key, in seconds.
+
+_NOTE_: Keys can only be expired by a cluster leader, so if a machine gets disconnected from the cluster, its keys will not expire until it rejoins.
+
+Now you can try to get the key by sending a `GET` request:
+
+```sh
+curl -L http://127.0.0.1:4001/v2/keys/foo
+```
+
+If the TTL has expired, the key will have been deleted, and you will be returned a 100.
+
+```json
+{
+    "cause": "/foo",
+    "errorCode": 100,
+    "index": 6,
+    "message": "Key Not Found"
+}
+```
+
+The TTL could be unset to avoid expiration through update operation:
+
+```sh
+curl -L http://127.0.0.1:4001/v2/keys/foo -XPUT -d value=bar -d ttl= -d prevExist=true
+```
+
+```json
+{
+    "action": "update",
+    "node": {
+        "createdIndex": 5,
+        "key": "/foo",
+        "modifiedIndex": 6,
+        "value": "bar"
+    },
+    "prevNode": {
+        "createdIndex": 5,
+        "expiration": "2013-12-04T12:01:21.874888581-08:00",
+        "key": "/foo",
+        "modifiedIndex": 5,
+        "ttl": 3,
+        "value": "bar"
+    }
+}
+```
+
+
+### Waiting for a change
+
+We can watch for a change on a key and receive a notification by using long polling.
+This also works for child keys by passing `recursive=true` in curl.
+
+In one terminal, we send a `GET` with `wait=true` :
+
+```sh
+curl -L http://127.0.0.1:4001/v2/keys/foo?wait=true
+```
+
+Now we are waiting for any changes at path `/foo`.
+
+In another terminal, we set a key `/foo` with value `bar`:
+
+```sh
+curl -L http://127.0.0.1:4001/v2/keys/foo -XPUT -d value=bar
+```
+
+The first terminal should get the notification and return with the same response as the set request:
+
+```json
+{
+    "action": "set",
+    "node": {
+        "createdIndex": 7,
+        "key": "/foo",
+        "modifiedIndex": 7,
+        "value": "bar"
+    },
+    "prevNode": {
+        "createdIndex": 6,
+        "key": "/foo",
+        "modifiedIndex": 6,
+        "value": "bar"
+    }
+}
+```
+
+However, the watch command can do more than this.
+Using the index, we can watch for commands that have happened in the past.
+This is useful for ensuring you don't miss events between watch commands.
+
+Let's try to watch for the set command of index 7 again:
+
+```sh
+curl -L 'http://127.0.0.1:4001/v2/keys/foo?wait=true&waitIndex=7'
+```
+
+The watch command returns immediately with the same response as previously.
+
+
+### Atomically Creating In-Order Keys
+
+Using `POST` on a directory, you can create keys with key names that are created in-order.
+This can be used in a variety of useful patterns, like implementing queues of keys which need to be processed in strict order.
+An example use case is the [locking module][lockmod] which uses it to ensure clients get fair access to a mutex.
+
+Creating an in-order key is easy:
+
+```sh
+curl http://127.0.0.1:4001/v2/keys/queue -XPOST -d value=Job1
+```
+
+```json
+{
+    "action": "create",
+    "node": {
+        "createdIndex": 6,
+        "key": "/queue/6",
+        "modifiedIndex": 6,
+        "value": "Job1"
+    }
+}
+```
+
+If you create another entry some time later, it is guaranteed to have a key name that is greater than the previous key.
+Also note the key names use the global etcd index, so the next key can be more than `previous + 1`.
+
+```sh
+curl http://127.0.0.1:4001/v2/keys/queue -XPOST -d value=Job2
+```
+
+```json
+{
+    "action": "create",
+    "node": {
+        "createdIndex": 29,
+        "key": "/queue/29",
+        "modifiedIndex": 29,
+        "value": "Job2"
+    }
+}
+```
+
+To enumerate the in-order keys as a sorted list, use the "sorted" parameter.
+
+```sh
+curl -s 'http://127.0.0.1:4001/v2/keys/queue?recursive=true&sorted=true'
+```
+
+```json
+{
+    "action": "get",
+    "node": {
+        "createdIndex": 2,
+        "dir": true,
+        "key": "/queue",
+        "modifiedIndex": 2,
+        "nodes": [
+            {
+                "createdIndex": 2,
+                "key": "/queue/2",
+                "modifiedIndex": 2,
+                "value": "Job1"
+            },
+            {
+                "createdIndex": 3,
+                "key": "/queue/3",
+                "modifiedIndex": 3,
+                "value": "Job2"
+            }
+        ]
+    }
+}
+```
+
+[lockmod]: #lock
+
+
+### Using a directory TTL
+
+Like keys, directories in etcd can be set to expire after a specified number of seconds.
+You can do this by setting a TTL (time to live) on a directory when it is created with a `PUT`:
+
+```sh
+curl -L http://127.0.0.1:4001/v2/keys/dir -XPUT -d ttl=30 -d dir=true
+```
+
+```json
+{
+    "action": "set",
+    "node": {
+        "createdIndex": 17,
+        "dir": true,
+        "expiration": "2013-12-11T10:37:33.689275857-08:00",
+        "key": "/dir",
+        "modifiedIndex": 17,
+        "ttl": 30
+    }
+}
+```
+
+The directory's TTL can be refreshed by making an update.
+You can do this by making a PUT with `prevExist=true` and a new TTL.
+
+```sh
+curl -L http://127.0.0.1:4001/v2/keys/dir -XPUT -d ttl=30 -d dir=true -d prevExist=true
+```
+
+Keys that are under this directory work as usual, but when the directory expires, a watcher on a key under the directory will get an expire event:
+
+```sh
+curl 'http://127.0.0.1:4001/v2/keys/dir/asdf?consistent=true&wait=true'
+```
+
+```json
+{
+	"action": "expire",
+	"node": {
+		"createdIndex": 8,
+		"key": "/dir",
+		"modifiedIndex": 15
+	},
+	"prevNode": {
+		"createdIndex": 8,
+		"key": "/dir",
+		"dir":true,
+		"modifiedIndex": 17,
+		"expiration": "2013-12-11T10:39:35.689275857-08:00"
+	}
+}
+```
+
+
+### Atomic Compare-and-Swap
+
+etcd can be used as a centralized coordination service in a cluster, and `CompareAndSwap` (CAS) is the most basic operation used to build a distributed lock service.
+
+This command will set the value of a key only if the client-provided conditions are equal to the current conditions.
+
+The current comparable conditions are:
+
+1. `prevValue` - checks the previous value of the key.
+
+2. `prevIndex` - checks the previous modifiedIndex of the key.
+
+3. `prevExist` - checks existence of the key: if `prevExist` is true, it is an `update` request; if prevExist is `false`, it is a `create` request.
+
+Here is a simple example.
+Let's create a key-value pair first: `foo=one`.
+
+```sh
+curl -L http://127.0.0.1:4001/v2/keys/foo -XPUT -d value=one
+```
+
+Now let's try some invalid `CompareAndSwap` commands.
+
+Trying to set this existing key with `prevExist=false` fails as expected:
+```sh
+curl -L http://127.0.0.1:4001/v2/keys/foo?prevExist=false -XPUT -d value=three
+```
+
+The error code explains the problem:
+
+```json
+{
+    "cause": "/foo",
+    "errorCode": 105,
+    "index": 39776,
+    "message": "Already exists"
+}
+```
+
+Now let's provide a `prevValue` parameter:
+
+```sh
+curl -L http://127.0.0.1:4001/v2/keys/foo?prevValue=two -XPUT -d value=three
+```
+
+This will try to compare the previous value of the key and the previous value we provided. If they are equal, the value of the key will change to three.
+
+```json
+{
+    "cause": "[two != one]",
+    "errorCode": 101,
+    "index": 8,
+    "message": "Test Failed"
+}
+```
+
+which means `CompareAndSwap` failed. `cause` explains why the test failed.
+Note: the condition prevIndex=0 always passes.
+
+Let's try a valid condition:
+
+```sh
+curl -L http://127.0.0.1:4001/v2/keys/foo?prevValue=one -XPUT -d value=two
+```
+
+The response should be:
+
+```json
+{
+    "action": "compareAndSwap",
+    "node": {
+        "createdIndex": 8,
+        "key": "/foo",
+        "modifiedIndex": 9,
+        "value": "two"
+    },
+    "prevNode": {
+    	"createdIndex": 8,
+    	"key": "/foo",
+    	"modifiedIndex": 8,
+    	"value": "one"
+    }
+}
+```
+
+We successfully changed the value from "one" to "two" since we gave the correct previous value.
+
+### Atomic Compare-and-Delete
+
+This command will delete a key only if the client-provided conditions are equal to the current conditions.
+
+The current comparable conditions are:
+
+1. `prevValue` - checks the previous value of the key.
+
+2. `prevIndex` - checks the previous modifiedIndex of the key.
+
+Here is a simple example. Let's first create a key: `foo=one`.
+
+```sh
+curl -L http://127.0.0.1:4001/v2/keys/foo -XPUT -d value=one
+```
+
+Now let's try some `CompareAndDelete` commands.
+
+Trying to delete the key with `prevValue=two` fails as expected:
+```sh
+curl -L http://127.0.0.1:4001/v2/keys/foo?prevValue=two -XDELETE
+```
+
+The error code explains the problem:
+
+```json
+{
+	"errorCode": 101,
+	"message": "Compare failed",
+	"cause": "[two != one]",
+	"index": 8
+}
+```
+
+As does a `CompareAndDelete` with a mismatched `prevIndex`:
+
+```sh
+curl -L http://127.0.0.1:4001/v2/keys/foo?prevIndex=1 -XDELETE
+```
+
+```json
+{
+	"errorCode": 101,
+	"message": "Compare failed",
+	"cause": "[1 != 8]",
+	"index": 8
+}
+```
+
+And now a valid `prevValue` condition:
+
+```sh
+curl -L http://127.0.0.1:4001/v2/keys/foo?prevValue=one -XDELETE
+```
+
+The successful response will look something like:
+
+```json
+{
+	"action": "compareAndDelete",
+	"node": {
+		"key": "/foo",
+		"modifiedIndex": 9,
+		"createdIndex": 8
+	},
+	"prevNode": {
+		"key": "/foo",
+		"value": "one",
+		"modifiedIndex": 8,
+		"createdIndex": 8
+	}
+}
+```
+
+### Creating Directories
+
+In most cases, directories for a key are automatically created.
+But there are cases where you will want to create a directory or remove one.
+
+Creating a directory is just like a key except you cannot provide a value and must add the `dir=true` parameter.
+
+```sh
+curl -L http://127.0.0.1:4001/v2/keys/dir -XPUT -d dir=true
+```
+```json
+{
+    "action": "set",
+    "node": {
+        "createdIndex": 30,
+        "dir": true,
+        "key": "/dir",
+        "modifiedIndex": 30
+    }
+}
+```
+
+
+### Listing a directory
+
+In etcd we can store two types of things: keys and directories.
+Keys store a single string value.
+Directories store a set of keys and/or other directories.
+
+In this example, let's first create some keys:
+
+We already have `/foo=two` so now we'll create another one called `/foo_dir/foo` with the value of `bar`:
+
+```sh
+curl -L http://127.0.0.1:4001/v2/keys/foo_dir/foo -XPUT -d value=bar
+```
+
+```json
+{
+    "action": "set",
+    "node": {
+        "createdIndex": 2,
+        "key": "/foo_dir/foo",
+        "modifiedIndex": 2,
+        "value": "bar"
+    }
+}
+```
+
+Now we can list the keys under root `/`:
+
+```sh
+curl -L http://127.0.0.1:4001/v2/keys/
+```
+
+We should see the response as an array of items:
+
+```json
+{
+    "action": "get",
+    "node": {
+        "dir": true,
+        "key": "/",
+        "nodes": [
+            {
+                "createdIndex": 2,
+                "dir": true,
+                "key": "/foo_dir",
+                "modifiedIndex": 2
+            }
+        ]
+    }
+}
+```
+
+Here we can see `/foo` is a key-value pair under `/` and `/foo_dir` is a directory.
+We can also recursively get all the contents under a directory by adding `recursive=true`.
+
+```sh
+curl -L http://127.0.0.1:4001/v2/keys/?recursive=true
+```
+
+```json
+{
+    "action": "get",
+    "node": {
+        "dir": true,
+        "key": "/",
+        "nodes": [
+            {
+                "createdIndex": 2,
+                "dir": true,
+                "key": "/foo_dir",
+                "modifiedIndex": 2,
+                "nodes": [
+                    {
+                        "createdIndex": 2,
+                        "key": "/foo_dir/foo",
+                        "modifiedIndex": 2,
+                        "value": "bar"
+                    }
+                ]
+            }
+        ]
+    }
+}
+```
+
+
+### Deleting a Directory
+
+Now let's try to delete the directory `/foo_dir`.
+
+You can remove an empty directory using the `DELETE` verb and the `dir=true` parameter.
+
+```sh
+curl -L 'http://127.0.0.1:4001/v2/keys/foo_dir?dir=true' -XDELETE
+```
+```json
+{
+    "action": "delete",
+    "node": {
+        "createdIndex": 30,
+        "dir": true,
+        "key": "/foo_dir",
+        "modifiedIndex": 31
+    },
+    "prevNode": {
+    	"createdIndex": 30,
+    	"key": "/foo_dir",
+    	"dir": true,
+    	"modifiedIndex": 30
+    }
+}
+```
+
+To delete a directory that holds keys, you must add `recursive=true`.
+
+```sh
+curl -L http://127.0.0.1:4001/v2/keys/dir?recursive=true -XDELETE
+```
+
+```json
+{
+    "action": "delete",
+    "node": {
+        "createdIndex": 10,
+        "dir": true,
+        "key": "/dir",
+        "modifiedIndex": 11
+    },
+    "prevNode": {
+    	"createdIndex": 10,
+    	"dir": true,
+    	"key": "/dir",
+    	"modifiedIndex": 10
+    }
+}
+```
+
+
+### Creating a hidden node
+
+We can create a hidden key-value pair or directory by add a `_` prefix.
+The hidden item will not be listed when sending a `GET` request for a directory.
+
+First we'll add a hidden key named `/_message`:
+
+```sh
+curl -L http://127.0.0.1:4001/v2/keys/_message -XPUT -d value="Hello hidden world"
+```
+
+```json
+{
+    "action": "set",
+    "node": {
+        "createdIndex": 3,
+        "key": "/_message",
+        "modifiedIndex": 3,
+        "value": "Hello hidden world"
+    }
+}
+```
+
+Next we'll add a regular key named `/message`:
+
+```sh
+curl -L http://127.0.0.1:4001/v2/keys/message -XPUT -d value="Hello world"
+```
+
+```json
+{
+    "action": "set",
+    "node": {
+        "createdIndex": 4,
+        "key": "/message",
+        "modifiedIndex": 4,
+        "value": "Hello world"
+    }
+}
+```
+
+Now let's try to get a listing of keys under the root directory, `/`:
+
+```sh
+curl -L http://127.0.0.1:4001/v2/keys/
+```
+
+```json
+{
+    "action": "get",
+    "node": {
+        "dir": true,
+        "key": "/",
+        "nodes": [
+            {
+                "createdIndex": 2,
+                "dir": true,
+                "key": "/foo_dir",
+                "modifiedIndex": 2
+            },
+            {
+                "createdIndex": 4,
+                "key": "/message",
+                "modifiedIndex": 4,
+                "value": "Hello world"
+            }
+        ]
+    }
+}
+```
+
+Here we see the `/message` key but our hidden `/_message` key is not returned.
+
+### Setting a key from a file
+
+You can also use etcd to store small configuration files, json documents, XML documents, etc directly.
+For example you can use curl to upload a simple text file and encode it:
+
+```
+echo "Hello\nWorld" > afile.txt
+curl -L http://127.0.0.1:4001/v2/keys/afile -XPUT --data-urlencode value@afile.txt
+```
+
+```json
+{
+    "action": "get",
+    "node": {
+        "createdIndex": 2,
+        "key": "/afile",
+        "modifiedIndex": 2,
+        "value": "Hello\nWorld\n"
+    }
+}
+```
+
+### Read Consistency
+
+#### Read from the Master
+
+Followers in a cluster can be behind the leader in their copy of the keyspace.
+If your application wants or needs the most up-to-date version of a key then it should ensure it reads from the current leader.
+By using the `consistent=true` flag in your GET requests, etcd will make sure you are talking to the current master.
+
+As an example of how a machine can be behind the leader let's start with a three machine cluster: L, F1, and F2.
+A client makes a write to L and F1 acknowledges the request.
+The client is told the write was successful and the keyspace is updated.
+Meanwhile F2 has partitioned from the network and will have an out-of-date version of the keyspace until the partition resolves.
+Since F2 missed the most recent write, a client reading from F2 will have an out-of-date version of the keyspace.
+
+Implementation notes on `consistent=true`: If the leader you are talking to is
+partitioned it will be unable to determine if it is not currently the master.
+In a later version we will provide a mechanism to set an upperbound of time
+that the current master can be unable to contact the quorom and still serve
+reads.
+
+### Read Linearization
+
+If you want a read that is fully linearized you can use a `quorum=true` GET.
+The read will take a very similar path to a write and will have a similar
+speed. If you are unsure if you need this feature feel free to email etcd-dev
+for advice.
+
+## Lock Module (*Deprecated and Removed*)
+
+The lock module is used to serialize access to resources used by clients.
+Multiple clients can attempt to acquire a lock but only one can have it at a time.
+Once the lock is released, the next client waiting for the lock will receive it.
+
+**Warning:** This module is deprecated and removed at v0.4. See [Modules][modules] for more details.
+
+
+### Acquiring a Lock
+
+To acquire a lock, simply send a `POST` request to the lock module with the lock name and TTL:
+
+```sh
+curl -L http://127.0.0.1:4001/mod/v2/lock/mylock -XPOST -d ttl=20
+```
+
+You will receive the lock index when you acquire the lock:
+
+```
+2
+```
+
+If the TTL is not specified or is not a number then you'll receive the following error:
+
+```json
+{
+    "errorCode": 202,
+    "message": "The given TTL in POST form is not a number",
+    "cause": "Acquire",
+}
+```
+
+If you specify a timeout that is not a number then you'll receive the following error:
+
+```json
+{
+    "errorCode": 205,
+    "message": "The given timeout in POST form is not a number",
+    "cause": "Acquire",
+}
+```
+
+
+### Renewing a Lock
+
+To extend the TTL of an already acquired lock, simply repeat your original request but with a `PUT` and the lock index instead:
+
+```sh
+curl -L http://127.0.0.1:4001/mod/v2/lock/mylock -XPUT -d index=5 -d ttl=20
+```
+
+If the index or value is not specified then you'll receive the following error:
+
+```json
+{
+    "errorCode": 207,
+    "message": "Index or value is required",
+    "cause": "Renew",
+}
+```
+
+If the index or value does not exist then you'll receive the following error with a `404 Not Found` HTTP code:
+
+```json
+{
+    "errorCode": 100,
+    "message": "Key not found",
+    "index": 1
+}
+```
+
+If the TTL is not specified or is not a number then you'll receive the following error:
+
+```json
+{
+    "errorCode": 202,
+    "message": "The given TTL in POST form is not a number",
+    "cause": "Renew",
+}
+```
+
+
+### Releasing a Lock
+
+When the client is finished with the lock, simply send a `DELETE` request to release the lock:
+
+```sh
+curl -L http://127.0.0.1:4001/mod/v2/lock/mylock?index=5 -XDELETE
+```
+
+If the index or value is not specified then you'll receive the following error:
+
+```json
+{
+    "errorCode": 207,
+    "message": "Index or value is required",
+    "cause": "Release",
+}
+```
+
+If the index and value are both specified then you'll receive the following error:
+
+```json
+{
+    "errorCode": 208,
+    "message": "Index and value cannot both be specified",
+    "cause": "Release",
+}
+```
+
+If the index or value does not exist then you'll receive the following error with a `404 Not Found` HTTP code:
+
+```json
+{
+    "errorCode": 100,
+    "message": "Key not found",
+    "index": 1
+}
+```
+
+
+### Retrieving a Lock
+
+To determine the current value or index of a lock, send a `GET` request to the lock.
+You can specify a `field` of `index` or `value`.
+The default is `value`.
+
+```sh
+curl -L http://127.0.0.1:4001/mod/v2/lock/mylock?field=index
+```
+
+Will return the current index:
+
+```sh
+2
+```
+
+If you specify a field other than `index` or `value` then you'll receive the following error:
+
+```json
+{
+    "errorCode": 209,
+    "message": "Invalid field",
+    "cause": "Get",
+}
+```
+
+
+## Leader Module (*Deprecated*)
+
+The leader module wraps the lock module to provide a simple interface for electing a single leader in a cluster.
+
+**Warning:** This module is deprecated at v0.4. See [Modules][modules] for more details.
+[modules]: https://github.com/coreos/etcd/blob/master/Documentation/modules.md
+
+
+### Setting the Leader
+
+A client can attempt to become leader by sending a `PUT` request to the leader module with the name of the leader to elect:
+
+```sh
+curl -L http://127.0.0.1:4001/mod/v2/leader/myclustername -XPUT -d ttl=300 -d name=foo.mydomain.com
+```
+
+You will receive a successful `200` HTTP response code when the leader is elected.
+
+If the name is not specified then you'll receive the following error:
+
+```json
+{
+    "errorCode": 206,
+    "message": "Name is required in POST form",
+    "cause": "Set",
+}
+```
+
+You can also receive any errors specified by the Lock module.
+
+
+### Retrieving the Current Leader
+
+A client can check to determine if there is a current leader by sending a `GET` request to the leader module:
+
+```sh
+curl -L http://127.0.0.1:4001/mod/v2/leader/myclustername
+```
+
+You will receive the name of the current leader:
+
+```sh
+foo.mydomain.com
+```
+
+
+### Relinquishing Leadership
+
+A client can give up leadership by sending a `DELETE` request with the leader name:
+
+```sh
+curl -L http://127.0.0.1:4001/mod/v2/leader/myclustername?name=foo.mydomain.com -XDELETE
+```
+
+If the name is not specified then you'll receive the following error:
+
+```json
+{
+    "errorCode": 206,
+    "message": "Name is required in POST form",
+    "cause": "Set",
+}
+```
+
+
+## Statistics
+
+An etcd cluster keeps track of a number of statistics including latency, bandwidth and uptime.
+These statistics are used in the `/mod/dashboard` endpoint to generate tables and graphs about the cluster state.
+
+
+### Leader Statistics
+
+The leader has a view of the entire cluster and keeps track of two interesting statistics: latency to each peer in the cluster, and the number of failed and successful Raft RPC requests.
+You can grab these statistics from the `/v2/stats/leader` endpoint:
+
+```sh
+curl -L http://127.0.0.1:4001/v2/stats/leader
+```
+
+```json
+{
+    "followers": {
+        "etcd-node1": {
+            "counts": {
+                "fail": 1212,
+                "success": 4163176
+            },
+            "latency": {
+                "average": 2.7206299430775007,
+                "current": 1.486487,
+                "maximum": 2018.410279,
+                "minimum": 1.011763,
+                "standardDeviation": 6.246990702203536
+            }
+        },
+        "etcd-node3": {
+            "counts": {
+                "fail": 1378,
+                "success": 4164598
+            },
+            "latency": {
+                "average": 2.707100125761001,
+                "current": 1.666258,
+                "maximum": 1409.054765,
+                "minimum": 0.998415,
+                "standardDeviation": 5.910089773061448
+            }
+        }
+    },
+    "leader": "etcd-node2"
+}
+```
+
+
+### Self Statistics
+
+Each node keeps a number of internal statistics:
+
+- `leaderInfo.leader`: name of the current leader machine
+- `leaderInfo.uptime`: amount of time the leader has been leader
+- `name`: this machine's name
+- `recvAppendRequestCnt`: number of append requests this node has processed
+- `recvBandwidthRate`: number of bytes per second this node is receiving (follower only)
+- `recvPkgRate`: number of requests per second this node is receiving (follower only)
+- `sendAppendRequestCnt`: number of requests that this node has sent
+- `sendBandwidthRate`: number of bytes per second this node is receiving (leader only). This value is undefined on single machine clusters.
+- `sendPkgRate`: number of requests per second this node is receiving (leader only). This value is undefined on single machine clusters.
+- `state`: either leader or follower
+- `startTime`: the time when this node was started
+
+This is an example response from a follower machine:
+
+```sh
+curl -L http://127.0.0.1:4001/v2/stats/self
+```
+
+```json
+{
+    "leaderInfo": {
+        "leader": "machine1",
+        "uptime": "1m18.544996775s"
+    },
+    "name": "machine0",
+    "recvAppendRequestCnt": 5871307,
+    "recvBandwidthRate": 630.3121596542599,
+    "recvPkgRate": 19.272654323628185,
+    "sendAppendRequestCnt": 3175763,
+    "startTime": "2014-01-01T15:26:24.96569404Z",
+    "state": "follower"
+}
+```
+
+And this is an example response from a leader machine:
+
+```sh
+curl -L http://127.0.0.1:4001/v2/stats/self
+```
+
+```json
+{
+    "leaderInfo": {
+        "leader": "machine0",
+        "uptime": "24.648619798s"
+    },
+    "name": "machine0",
+    "recvAppendRequestCnt": 5901116,
+    "sendAppendRequestCnt": 3212344,
+    "sendBandwidthRate": 1254.3151237301615,
+    "sendPkgRate": 38.71342974475808,
+    "startTime": "2014-01-01T15:26:24.96569404Z",
+    "state": "leader"
+}
+```
+
+
+### Store Statistics
+
+The store statistics include information about the operations that this node has handled.
+
+Operations that modify the store's state like create, delete, set and update are seen by the entire cluster and the number will increase on all nodes.
+Operations like get and watch are node local and will only be seen on this node.
+
+```sh
+curl -L http://127.0.0.1:4001/v2/stats/store
+```
+
+```json
+{
+    "compareAndSwapFail": 0,
+    "compareAndSwapSuccess": 0,
+    "createFail": 0,
+    "createSuccess": 2,
+    "deleteFail": 0,
+    "deleteSuccess": 0,
+    "expireCount": 0,
+    "getsFail": 4,
+    "getsSuccess": 75,
+    "setsFail": 2,
+    "setsSuccess": 4,
+    "updateFail": 0,
+    "updateSuccess": 0,
+    "watchers": 0
+}
+```
+
+## Cluster Config
+
+The configuration endpoint manages shared cluster wide properties.
+
+### Set Cluster Config
+
+```sh
+curl -L http://127.0.0.1:7001/v2/admin/config -XPUT -d '{"activeSize":3, "removeDelay":1800,"syncInterval":5}'
+```
+
+```json
+{
+    "activeSize": 3,
+    "removeDelay": 1800,
+    "syncInterval":5
+}
+```
+
+`activeSize` is the maximum number of peers that can join the cluster and participate in the consensus protocol.
+
+The size of cluster is controlled to be around a certain number. If it is not, standby-mode instances will join or peer-mode instances will be removed to make it happen.
+
+`removeDelay` indicates the minimum time that a machine has been observed to be unresponsive before it is removed from the cluster.
+
+### Get Cluster Config
+
+```sh
+curl -L http://127.0.0.1:7001/v2/admin/config
+```
+
+```json
+{
+    "activeSize": 3,
+    "removeDelay": 1800,
+    "syncInterval":5
+}
+```
+
+## Remove Machines
+
+At times you may want to manually remove a machine. Using the machines endpoint
+you can find and remove machines.
+
+First, list all the machines in the cluster.
+
+```sh
+curl -L http://127.0.0.1:7001/v2/admin/machines
+```
+```json
+[
+    {
+        "clientURL": "http://127.0.0.1:4001",
+        "name": "peer1",
+        "peerURL": "http://127.0.0.1:7001",
+        "state": "leader"
+    },
+    {
+        "clientURL": "http://127.0.0.1:4002",
+        "name": "peer2",
+        "peerURL": "http://127.0.0.1:7002",
+        "state": "follower"
+    },
+    {
+        "clientURL": "http://127.0.0.1:4003",
+        "name": "peer3",
+        "peerURL": "http://127.0.0.1:7003",
+        "state": "follower"
+    }
+]
+```
+
+Then take a closer look at the machine you want to remove.
+
+```sh
+curl -L http://127.0.0.1:7001/v2/admin/machines/peer2
+```
+
+```json
+{
+    "clientURL": "http://127.0.0.1:4002",
+    "name": "peer2",
+    "peerURL": "http://127.0.0.1:7002",
+    "state": "follower"
+}
+```
+
+And finally remove it.
+
+```sh
+curl -L -XDELETE http://127.0.0.1:7001/v2/admin/machines/peer2
+```
diff --git a/Documentation/clients-matrix.md b/Documentation/clients-matrix.md
new file mode 100644
index 0000000..fa6659a
--- /dev/null
+++ b/Documentation/clients-matrix.md
@@ -0,0 +1,42 @@
+# Client libraries support matrix for etcd
+
+As etcd features support is really uneven between client libraries, a compatibility matrix can be important.
+
+## v2 clients
+
+The v2 API has a lot of features, we will categorize them in a few categories:
+- **Language**: The language in which the client library was written.
+- **HTTPS Auth**: Support for SSL-certificate based authentication
+- **Reconnect**: If the client is able to reconnect automatically to another server if one fails.
+- **Mod/Lock**: Support for the locking module
+- **Mod/Leader**: Support for the leader election module
+- **GET,PUT,POST,DEL Features**: Support for all the modifiers when calling the etcd server with said HTTP method.
+
+### Supported features matrix
+
+**Legend**
+**F**: Full support **G**: Good support **B**: Basic support
+**Y**: Feature supported  **-**: Feature not supported
+
+Sorted alphabetically on language/name
+
+|Client |**Language**|**HTTPS Auth**|**Re-connect**|**GET**|**PUT**|**POST**|**DEL**|**Mod Lock**|**Mod Leader**|
+| --- | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | 
+|[etcd-api](https://github.com/jdarcy/etcd-api)                   |C      |-|Y|B|G|-|B|-|-|
+|[etcdcpp](https://github.com/edwardcapriolo/etcdcpp)             |C++    |-|-|F|F|G|-|-|-|
+|[cetcd](https://github.com/dwwoelfel/cetcd)                      |Clojure|-|-|F|F|-|G|-|-|
+|[clj-etcd](https://github.com/rthomas/clj-etcd)                  |Clojure|-|-|G|G|-|B|-|-|
+|[etcd-clojure](https://github.com/aterreno/etcd-clojure)         |Clojure|-|-|F|F|F|F|-|-|
+|[go-etcd](https://github.com/coreos/go-etcd)                     |go     |Y|Y|F|F|F|F|-|-|
+|[etcd4j](https://github.com/jurmous/etcd4j)                      |java   |Y|Y|F|F|F|F|-|-|
+|[jetcd](https://github.com/diwakergupta/jetcd)                   |java   |Y|-|B|B|-|B|-|-|
+|[jetcd](https://github.com/justinsb/jetcd)                       |java   |-|-|B|B|-|B|-|-|
+|[Etcd.jl](https://github.com/forio/Etcd.jl)                      |Julia  |-|-|F|F|F|F|Y|Y|
+|[etcetera](https://github.com/drusellers/etcetera)               |.net   |-|-|F|F|F|F|-|-|
+|[node-etcd](https://github.com/stianeikeland/node-etcd)          |nodejs |Y|-|F|F|-|F|-|-|
+|[nodejs-etcd](https://github.com/lavagetto/nodejs-etcd)          |nodejs |Y|-|F|F|F|F|-|-|
+|[p5-etcd](https://metacpan.org/release/Etcd)                     |perl   |-|-|F|F|F|F|-|-|
+|[python-etcd](https://github.com/jplana/python-etcd)             |python |Y|Y|F|F|F|F|Y|-|
+|[python-etcd-client](https://github.com/dsoprea/PythonEtcdClient)|python |Y|Y|F|F|F|F|Y|Y|
+|[txetcd](https://github.com/russellhaering/txetcd)               |python |-|-|G|G|F|G|-|-|
+|[etcd-ruby](https://github.com/ranjib/etcd-ruby)                 |ruby   |-|-|F|F|F|F|-|-|
diff --git a/Documentation/cluster-discovery.md b/Documentation/cluster-discovery.md
new file mode 100644
index 0000000..8f53c9d
--- /dev/null
+++ b/Documentation/cluster-discovery.md
@@ -0,0 +1,60 @@
+# Cluster Discovery
+
+## Overview
+
+Starting an etcd cluster requires that each node knows another in the cluster. If you are trying to bring up a cluster all at once, say using a cloud formation, you also need to coordinate who will be the initial cluster leader. The discovery protocol helps you by providing an automated way to discover other existing peers in a cluster.
+
+For more information on how etcd can locate the cluster, see the [finding the cluster][cluster-finding] documentation.
+
+Please note - at least 3 nodes are required for [cluster availability][optimal-cluster-size].
+
+[cluster-finding]: https://github.com/coreos/etcd/blob/master/Documentation/design/cluster-finding.md
+[optimal-cluster-size]: https://github.com/coreos/etcd/blob/master/Documentation/optimal-cluster-size.md
+
+## Using discovery.etcd.io
+
+### Create a Discovery URL
+
+To use the discovery API, you must first create a unique discovery URL for your etcd cluster. Visit [https://discovery.etcd.io/new](https://discovery.etcd.io/new) to create a new discovery URL.
+
+You can inspect the list of peers by viewing `https://discovery.etcd.io/<cluster id>`.
+
+### Start etcd With the Discovery Flag
+
+Specify the `-discovery` flag when you start each etcd instance. The list of existing peers in the cluster will be downloaded and configured. If the instance is the first peer, it will start as the leader of the cluster.
+
+Here's a full example:
+
+```
+URL=$(curl https://discovery.etcd.io/new)
+./etcd -name instance1 -peer-addr 10.1.2.3:7001 -addr 10.1.2.3:4001 -discovery $URL
+./etcd -name instance2 -peer-addr 10.1.2.4:7001 -addr 10.1.2.4:4001 -discovery $URL
+./etcd -name instance3 -peer-addr 10.1.2.5:7001 -addr 10.1.2.5:4001 -discovery $URL
+```
+
+## Running Your Own Discovery Endpoint
+
+The discovery API communicates with a separate etcd cluster to store and retrieve the list of peers. CoreOS provides [https://discovery.etcd.io](https://discovery.etcd.io) as a free service, but you can easily run your own etcd cluster for this purpose. Here's an example using an etcd cluster located at `10.10.10.10:4001`:
+
+```
+URL="http://10.10.10.10:4001/v2/keys/testcluster"
+./etcd -name instance1 -peer-addr 10.1.2.3:7001 -addr 10.1.2.3:4001 -discovery $URL
+./etcd -name instance2 -peer-addr 10.1.2.4:7001 -addr 10.1.2.4:4001 -discovery $URL
+./etcd -name instance3 -peer-addr 10.1.2.5:7001 -addr 10.1.2.5:4001 -discovery $URL
+```
+
+If you're interested in how to discovery API works behind the scenes, read about the [Discovery Protocol](https://github.com/coreos/etcd/blob/master/Documentation/discovery-protocol.md).
+
+## Setting Peer Addresses Correctly
+
+The Discovery API submits the `-peer-addr` of each etcd instance to the configured Discovery endpoint. It's important to select an address that *all* peers in the cluster can communicate with. For example, if you're located in two regions of a cloud provider, configuring a private `10.x` address will not work between the two regions, and communication will not be possible between all peers.
+
+## Stale Peers
+
+The discovery API will automatically clean up the address of a stale peer that is no longer part of the cluster. The TTL for this process is a week, which should be long enough to handle any extremely long outage you may encounter. There is no harm in having stale peers in the list until they are cleaned up, since an etcd instance only needs to connect to one valid peer in the cluster to join.
+
+## Lifetime of a Discovery URL
+
+A discovery URL identifies a single etcd cluster. Do not re-use discovery URLs for new clusters.
+
+When a machine starts with a new discovery URL the discovery URL will be activated and record the machine's metadata. If you destroy the whole cluster and attempt to bring the cluster back up with the same discovery URL it will fail. This is intentional because all of the registered machines are gone including their logs so there is nothing to recover the killed cluster.
diff --git a/Documentation/clustering.md b/Documentation/clustering.md
new file mode 100644
index 0000000..5487bc2
--- /dev/null
+++ b/Documentation/clustering.md
@@ -0,0 +1,169 @@
+## Clustering
+
+### Example cluster of three machines
+
+Let's explore the use of etcd clustering.
+We use Raft as the underlying distributed protocol which provides consistency and persistence of the data across all of the etcd instances.
+
+Let start by creating 3 new etcd instances.
+
+We use `-peer-addr` to specify server port and `-addr` to specify client port and `-data-dir` to specify the directory to store the log and info of the machine in the cluster:
+
+```sh
+./etcd -peer-addr 127.0.0.1:7001 -addr 127.0.0.1:4001 -data-dir machines/machine1 -name machine1
+```
+
+**Note:** If you want to run etcd on an external IP address and still have access locally, you'll need to add `-bind-addr 0.0.0.0` so that it will listen on both external and localhost addresses.
+A similar argument `-peer-bind-addr` is used to setup the listening address for the server port.
+
+Let's join two more machines to this cluster using the `-peers` argument. A single connection to any peer will allow a new machine to join, but multiple can be specified for greater resiliency.
+
+```sh
+./etcd -peer-addr 127.0.0.1:7002 -addr 127.0.0.1:4002 -peers 127.0.0.1:7001,127.0.0.1:7003 -data-dir machines/machine2 -name machine2
+./etcd -peer-addr 127.0.0.1:7003 -addr 127.0.0.1:4003 -peers 127.0.0.1:7001,127.0.0.1:7002 -data-dir machines/machine3 -name machine3
+```
+
+We can retrieve a list of machines in the cluster using the HTTP API:
+
+```sh
+curl -L http://127.0.0.1:4001/v2/machines
+```
+
+We should see there are three machines in the cluster
+
+```
+http://127.0.0.1:4001, http://127.0.0.1:4002, http://127.0.0.1:4003
+```
+
+The machine list is also available via the main key API:
+
+```sh
+curl -L http://127.0.0.1:4001/v2/keys/_etcd/machines
+```
+
+```json
+{
+    "action": "get",
+    "node": {
+        "createdIndex": 1,
+        "dir": true,
+        "key": "/_etcd/machines",
+        "modifiedIndex": 1,
+        "nodes": [
+            {
+                "createdIndex": 1,
+                "key": "/_etcd/machines/machine1",
+                "modifiedIndex": 1,
+                "value": "raft=http://127.0.0.1:7001&etcd=http://127.0.0.1:4001"
+            },
+            {
+                "createdIndex": 2,
+                "key": "/_etcd/machines/machine2",
+                "modifiedIndex": 2,
+                "value": "raft=http://127.0.0.1:7002&etcd=http://127.0.0.1:4002"
+            },
+            {
+                "createdIndex": 3,
+                "key": "/_etcd/machines/machine3",
+                "modifiedIndex": 3,
+                "value": "raft=http://127.0.0.1:7003&etcd=http://127.0.0.1:4003"
+            }
+        ]
+    }
+}
+```
+
+We can also get the current leader in the cluster:
+
+```
+curl -L http://127.0.0.1:4001/v2/leader
+```
+
+The first server we set up should still be the leader unless it has died during these commands.
+
+```
+http://127.0.0.1:7001
+```
+
+Now we can do normal SET and GET operations on keys as we explored earlier.
+
+```sh
+curl -L http://127.0.0.1:4001/v2/keys/foo -XPUT -d value=bar
+```
+
+```json
+{
+    "action": "set",
+    "node": {
+        "createdIndex": 4,
+        "key": "/foo",
+        "modifiedIndex": 4,
+        "value": "bar"
+    }
+}
+```
+
+### Rejoining to the Cluster
+
+If one machine disconnects from the cluster, it could rejoin the cluster automatically when the communication is recovered.
+
+If one machine is killed, it could rejoin the cluster when started with old name. If the peer address is changed, etcd will treat the new peer address as the refreshed one, which benefits instance migration, or virtual machine boot with different IP. The peer-address-changing functionality is only supported when the majority of the cluster is alive, because this behavior needs the consensus of the etcd cluster.
+
+**Note:** For now, it is user responsibility to ensure that the machine doesn't join the cluster that has the member with the same name. Or unexpected error will happen. It would be improved sooner or later.
+
+### Killing Nodes in the Cluster
+
+Now if we kill the leader of the cluster, we can get the value from one of the other two machines:
+
+```sh
+curl -L http://127.0.0.1:4002/v2/keys/foo
+```
+
+We can also see that a new leader has been elected:
+
+```
+curl -L http://127.0.0.1:4002/v2/leader
+```
+
+```
+http://127.0.0.1:7002
+```
+
+or
+
+```
+http://127.0.0.1:7003
+```
+
+
+### Testing Persistence
+
+Next we'll kill all the machines to test persistence.
+Type `CTRL-C` on each terminal and then rerun the same command you used to start each machine.
+
+Your request for the `foo` key will return the correct value:
+
+```sh
+curl -L http://127.0.0.1:4002/v2/keys/foo
+```
+
+```json
+{
+    "action": "get",
+    "node": {
+        "createdIndex": 4,
+        "key": "/foo",
+        "modifiedIndex": 4,
+        "value": "bar"
+    }
+}
+```
+
+
+### Using HTTPS between servers
+
+In the previous example we showed how to use SSL client certs for client-to-server communication.
+Etcd can also do internal server-to-server communication using SSL client certs.
+To do this just change the `-*-file` flags to `-peer-*-file`.
+
+If you are using SSL for server-to-server communication, you must use it on all instances of etcd.
diff --git a/Documentation/configuration.md b/Documentation/configuration.md
new file mode 100644
index 0000000..e90f6e9
--- /dev/null
+++ b/Documentation/configuration.md
@@ -0,0 +1,135 @@
+# Etcd Configuration
+
+## Node Configuration
+
+Individual node configuration options can be set in three places:
+
+ 1. Command line flags
+ 2. Environment variables
+ 3. Configuration file
+
+Options set on the command line take precedence over all other sources.
+Options set in environment variables take precedence over options set in
+configuration files.
+
+## Cluster Configuration
+
+Cluster-wide settings are configured via the `/config` admin endpoint and additionally in the configuration file. Values contained in the configuration file will seed the cluster setting with the provided value. After the cluster is running, only the admin endpoint is used.
+
+The full documentation is contained in the [API docs](https://github.com/coreos/etcd/blob/master/Documentation/api.md#cluster-config).
+
+* `activeSize` - the maximum number of peers that can participate in the consensus protocol. Other peers will join as standbys.
+* `removeDelay` - the minimum time in seconds that a machine has been observed to be unresponsive before it is removed from the cluster.
+* `syncInterval` - the amount of time in seconds between cluster sync when it runs in standby mode.
+
+## Command Line Flags
+
+### Required
+
+* `-name` - The node name. Defaults to a UUID.
+
+### Optional
+
+* `-addr` - The advertised public hostname:port for client communication. Defaults to `127.0.0.1:4001`.
+* `-discovery` - A URL to use for discovering the peer list. (i.e `"https://discovery.etcd.io/your-unique-key"`).
+* `-http-read-timeout` - The number of seconds before an HTTP read operation is timed out.
+* `-http-write-timeout` - The number of seconds before an HTTP write operation is timed out.
+* `-bind-addr` - The listening hostname for client communication. Defaults to advertised IP.
+* `-peers` - A comma separated list of peers in the cluster (i.e `"203.0.113.101:7001,203.0.113.102:7001"`).
+* `-peers-file` - The file path containing a comma separated list of peers in the cluster.
+* `-ca-file` - The path of the client CAFile. Enables client cert authentication when present.
+* `-cert-file` - The cert file of the client.
+* `-key-file` - The key file of the client.
+* `-config` - The path of the etcd configuration file. Defaults to `/etc/etcd/etcd.conf`.
+* `-cors` - A comma separated white list of origins for cross-origin resource sharing.
+* `-cpuprofile` - The path to a file to output CPU profile data. Enables CPU profiling when present.
+* `-data-dir` - The directory to store log and snapshot. Defaults to the current working directory.
+* `-max-result-buffer` - The max size of result buffer. Defaults to `1024`.
+* `-max-retry-attempts` - The max retry attempts when trying to join a cluster. Defaults to `3`.
+* `-peer-addr` - The advertised public hostname:port for server communication. Defaults to `127.0.0.1:7001`.
+* `-peer-bind-addr` - The listening hostname for server communication. Defaults to advertised IP.
+* `-peer-ca-file` - The path of the CAFile. Enables client/peer cert authentication when present.
+* `-peer-cert-file` - The cert file of the server.
+* `-peer-key-file` - The key file of the server.
+* `-peer-election-timeout` - The number of milliseconds to wait before the leader is declared unhealthy.
+* `-peer-heartbeat-interval` - The number of milliseconds in between heartbeat requests
+* `-snapshot=false` - Disable log snapshots. Defaults to `true`.
+* `-cluster-active-size` - The expected number of instances participating in the consensus protocol. Only applied if the etcd instance is the first peer in the cluster.
+* `-cluster-remove-delay` - The number of seconds before one node is removed from the cluster since it cannot be connected at all. Only applied if the etcd instance is the first peer in the cluster.
+* `-cluster-sync-interval` - The number of seconds between synchronization for standby-mode instance with the cluster. Only applied if the etcd instance is the first peer in the cluster.
+* `-v` - Enable verbose logging. Defaults to `false`.
+* `-vv` - Enable very verbose logging. Defaults to `false`.
+* `-version` - Print the version and exit.
+
+## Configuration File
+
+The etcd configuration file is written in [TOML](https://github.com/mojombo/toml)
+and read from `/etc/etcd/etcd.conf` by default.
+
+```TOML
+addr = "127.0.0.1:4001"
+bind_addr = "127.0.0.1:4001"
+ca_file = ""
+cert_file = ""
+cors = []
+cpu_profile_file = ""
+data_dir = "."
+discovery = "http://etcd.local:4001/v2/keys/_etcd/registry/examplecluster"
+http_read_timeout = 10
+http_write_timeout = 10
+key_file = ""
+peers = []
+peers_file = ""
+max_cluster_size = 9
+max_result_buffer = 1024
+max_retry_attempts = 3
+name = "default-name"
+snapshot = true
+verbose = false
+very_verbose = false
+
+[peer]
+addr = "127.0.0.1:7001"
+bind_addr = "127.0.0.1:7001"
+ca_file = ""
+cert_file = ""
+key_file = ""
+
+[cluster]
+active_size = 9
+remove_delay = 1800.0
+sync_interval = 5.0
+```
+
+## Environment Variables
+
+ * `ETCD_ADDR`
+ * `ETCD_BIND_ADDR`
+ * `ETCD_CA_FILE`
+ * `ETCD_CERT_FILE`
+ * `ETCD_CORS_ORIGINS`
+ * `ETCD_CONFIG`
+ * `ETCD_CPU_PROFILE_FILE`
+ * `ETCD_DATA_DIR`
+ * `ETCD_DISCOVERY`
+ * `ETCD_CLUSTER_HTTP_READ_TIMEOUT`
+ * `ETCD_CLUSTER_HTTP_WRITE_TIMEOUT`
+ * `ETCD_KEY_FILE`
+ * `ETCD_PEERS`
+ * `ETCD_PEERS_FILE`
+ * `ETCD_MAX_CLUSTER_SIZE`
+ * `ETCD_MAX_RESULT_BUFFER`
+ * `ETCD_MAX_RETRY_ATTEMPTS`
+ * `ETCD_NAME`
+ * `ETCD_SNAPSHOT`
+ * `ETCD_VERBOSE`
+ * `ETCD_VERY_VERBOSE`
+ * `ETCD_PEER_ADDR`
+ * `ETCD_PEER_BIND_ADDR`
+ * `ETCD_PEER_CA_FILE`
+ * `ETCD_PEER_CERT_FILE`
+ * `ETCD_PEER_KEY_FILE`
+ * `ETCD_PEER_ELECTION_TIMEOUT`
+ * `ETCD_CLUSTER_ACTIVE_SIZE`
+ * `ETCD_CLUSTER_REMOVE_DELAY`
+ * `ETCD_CLUSTER_SYNC_INTERVAL`
diff --git a/Documentation/debugging.md b/Documentation/debugging.md
new file mode 100644
index 0000000..1e266b1
--- /dev/null
+++ b/Documentation/debugging.md
@@ -0,0 +1,69 @@
+# Debugging etcd
+
+Diagnosing issues in a distributed application is hard.
+etcd will help as much as it can - just enable these debug features using the CLI flag `-trace=*` or the config option `trace=*`.
+
+## Logging
+
+Log verbosity can be increased to the max using either the `-vvv` CLI flag or the `very_very_verbose=true` config option.
+
+The only supported logging mode is to stdout.
+
+## Metrics
+
+etcd itself can generate a set of metrics.
+These metrics represent many different internal data points that can be helpful when debugging etcd servers.
+
+#### Metrics reference
+
+Each individual metric name is prefixed with `etcd.<NAME>`, where \<NAME\> is the configured name of the etcd server.
+
+* `timer.appendentries.handle`: amount of time a peer takes to process an AppendEntriesRequest from the POV of the peer itself
+* `timer.peer.<PEER>.heartbeat`: amount of time a peer heartbeat operation takes from the POV of the leader that initiated that operation for peer \<PEER\>
+* `timer.command.<COMMAND>`: amount of time a given command took to be processed through the local server's raft state machine. This does not include time waiting on locks.
+
+#### Fetching metrics over HTTP
+
+Once tracing has been enabled on a given etcd server, all metric data is available at the server's `/debug/metrics` HTTP endpoint (i.e. `http://127.0.0.1:4001/debug/metrics`).
+Executing a GET HTTP command against the metrics endpoint will yield the current state of all metrics in the etcd server.
+
+#### Sending metrics to Graphite
+
+etcd supports [Graphite's Carbon plaintext protocol](https://graphite.readthedocs.org/en/latest/feeding-carbon.html#the-plaintext-protocol) - a TCP wire protocol designed for shipping metric data to an aggregator.
+To send metrics to a Graphite endpoint using this protocol, use of the `-graphite-host` CLI flag or the `graphite_host` config option (i.e. `graphite_host=172.17.0.19:2003`).
+
+See an [example graphite deploy script](https://github.com/coreos/etcd/contrib/graphite).
+
+#### Generating additional metrics with Collectd
+
+[Collectd](http://collectd.org/documentation.shtml) gathers metrics from the host running etcd.
+While these aren't metrics generated by etcd itself, it can be invaluable to compare etcd's view of the world to that of a separate process running next to etcd.
+
+See an [example collectd deploy script](https://github.com/coreos/etcd/contrib/collectd).
+
+## Profiling
+
+etcd exposes profiling information from the Go pprof package over HTTP.
+The basic browsable interface is served by etcd at the `/debug/pprof` HTTP endpoint (i.e. `http://127.0.0.1:4001/debug/pprof`).
+For more information on using profiling tools, see http://blog.golang.org/profiling-go-programs.
+
+**NOTE**: In the following examples you need to ensure that the `./bin/etcd` is identical to the `./bin/etcd` that you are targeting (same git hash, arch, platform, etc).
+
+#### Heap memory profile
+
+```
+go tool pprof ./bin/etcd http://127.0.0.1:4001/debug/pprof/heap
+```
+
+#### CPU profile
+
+```
+go tool pprof ./bin/etcd http://127.0.0.1:4001/debug/pprof/profile
+```
+
+#### Blocked goroutine profile
+
+```
+go tool pprof ./bin/etcd http://127.0.0.1:4001/debug/pprof/block
+```
+
diff --git a/Documentation/design/cluster-finding.md b/Documentation/design/cluster-finding.md
new file mode 100644
index 0000000..9cc3baf
--- /dev/null
+++ b/Documentation/design/cluster-finding.md
@@ -0,0 +1,34 @@
+## Cluster Finding Process
+
+Peer discovery uses the following sources in this order: log data in `-data-dir`, `-discovery` and `-peers`.
+
+If log data is provided, etcd will concatenate possible peers from three sources: the log data, the `-discovery` option, and `-peers`. Then it tries to join cluster through them one by one. If all connection attempts fail (which indicates that the majority of the cluster is currently down), it will restart itself based on the log data, which helps the cluster to recover from a full outage.
+
+Without log data, the instance is assumed to be a brand new one. If possible targets are provided by `-discovery` and `-peers`, etcd will make a best effort attempt to join them, and if none is reachable it will exit. Otherwise, if no `-discovery` or `-peers` option is provided, a new cluster will always be started.
+
+This ensures that users can always restart the node safely with the same command (without --force), and etcd will either reconnect to the old cluster if it is still running or recover its cluster from a outage.
+
+## Logical Workflow
+
+Start an etcd machine:
+
+```
+If log data is given:
+	Try to join via peers in previous cluster
+	Try to join via peers found in discover URL
+	Try to join via peers in peer list
+	Restart the previous cluster which is down
+	return
+
+If discover URL is given:
+	Fetch peers through discover URL
+	If Success:
+		Join via peers found
+		return
+
+If peer list is given:
+	Join as follower via peers in peer list
+	return
+
+Start as the leader of a new cluster
+```
diff --git a/Documentation/design/standbys.md b/Documentation/design/standbys.md
new file mode 100644
index 0000000..1a37c2f
--- /dev/null
+++ b/Documentation/design/standbys.md
@@ -0,0 +1,232 @@
+## Standbys
+
+Adding peers in an etcd cluster adds network, CPU, and disk overhead to the leader since each one requires replication.
+Peers primarily provide resiliency in the event of a leader failure but the benefit of more failover nodes decreases as the cluster size increases.
+A lightweight alternative is the standby.
+
+Standbys are a way for an etcd node to forward requests along to the cluster but the standbys are not part of the Raft cluster themselves.
+This provides an easier API for local applications while reducing the overhead required by a regular peer node.
+Standbys also act as standby nodes in the event that a peer node in the cluster has not recovered after a long duration.
+
+
+## Configuration Parameters
+
+There are three configuration parameters used by standbys: active size, remove delay and standby sync interval.
+
+The active size specifies a target size for the number of peers in the cluster.
+If there are not enough peers to meet the active size, standbys will send join requests until the peer count is equal to the active size.
+If there are more peers than the target active size then peers are removed by the leader and will become standbys.
+
+The remove delay specifies how long the cluster should wait before removing a dead peer.
+By default this is 30 minutes.
+If a peer is inactive for 30 minutes then the peer is removed.
+
+The standby sync interval specifies the synchronization interval of standbys with the cluster.
+By default this is 5 seconds.
+After each interval, standbys synchronize information with cluster.
+
+
+## Logical Workflow
+
+### Start a etcd machine
+
+#### Main logic
+
+```
+If find existing standby cluster info:
+  Goto standby loop
+
+Find cluster as required
+If determine to start peer server:
+  Goto peer loop
+Else:
+  Goto standby loop
+
+Peer loop:
+  Start peer mode
+  If running:
+    Wait for stop
+  Goto standby loop
+
+Standby loop:
+  Start standby mode
+  If running:
+    Wait for stop
+  Goto peer loop
+```
+
+
+#### [Cluster finding logic][cluster-finding.md]
+
+
+#### Join request logic:
+
+```
+Fetch machine info
+If cannot match version:
+  return false
+If active size <= peer count:
+  return false
+If it has existed in the cluster:
+  return true
+If join request fails:
+  return false
+return true
+```
+
+**Note**
+1. [TODO] The running mode cannot be determined by log, because the log may be outdated. But the log could be used to estimate its state.
+2. Even if sync cluster fails, it will restart still for recovery from full outage.
+
+
+#### Peer mode start logic
+
+```
+Start raft server
+Start other helper routines
+```
+
+
+#### Peer mode auto stop logic
+
+```
+When removed from the cluster:
+  Stop raft server
+  Stop other helper routines
+```
+
+
+#### Standby mode run logic
+
+```
+Loop:
+  Sleep for some time
+
+  Sync cluster, and write cluster info into disk
+
+  Check active size and send join request if needed
+  If succeed:
+    Clear cluster info from disk
+    Return
+```
+
+
+#### Serve Requests as Standby
+
+Return '404 Page Not Found' always on peer address. This is because peer address is used for raft communication and cluster management, which should not be used in standby mode.
+
+
+Serve requests from client:
+
+```
+Redirect all requests to client URL of leader
+```
+
+**Note**
+1. The leader here implies the one in raft cluster when doing the latest successful synchronization.
+2. [IDEA] We could extend HTTP Redirect to multiple possible targets.
+
+
+### Join Request Handling
+
+```
+If machine has existed in the cluster:
+  Return
+If peer count < active size:
+  Add peer
+  Increase peer count
+```
+
+
+### Remove Request Handling
+
+```
+If machine exists in the cluster:
+  Remove peer
+  Decrease peer count
+```
+
+
+## Cluster Monitor Logic
+
+### Active Size Monitor:
+
+This is only run by current cluster leader.
+
+```
+Loop:
+  Sleep for some time
+
+  If peer count > active size:
+    Remove randomly selected peer
+```
+
+
+### Peer Activity Monitor
+
+This is only run by current cluster leader.
+
+```
+Loop:
+  Sleep for some time
+
+  For each peer:
+    If peer last activity time > remove delay:
+      Remove the peer
+      Goto Loop
+```
+
+
+## Cluster Cases
+
+### Create Cluster with Thousands of Instances
+
+First few machines run in peer mode.
+
+All the others check the status of the cluster and run in standby mode.
+
+
+### Recover from full outage
+
+Machines with log data restart with join failure.
+
+Machines in peer mode recover heartbeat between each other.
+
+Machines in standby mode always sync the cluster. If sync fails, it uses the first address from data log as redirect target.
+
+
+### Kill one peer machine
+
+Leader of the cluster lose the connection with the peer.
+
+When the time exceeds remove delay, it removes the peer from the cluster.
+
+Machine in standby mode finds one available place of the cluster. It sends join request and joins the cluster.
+
+**Note**
+1. [TODO] Machine which was divided from majority and was removed from the cluster will distribute running of the cluster if the new node uses the same name.
+
+
+### Kill one standby machine
+
+No change for the cluster.
+
+
+## Cons
+
+1. New instance cannot join immediately after one peer is kicked out of the cluster, because the leader doesn't know the info about the standby instances.
+
+2. It may introduce join collision
+
+3. Cluster needs a good interval setting to balance the join delay and join collision.
+
+
+## Future Attack Plans
+
+1. Based on heartbeat miss and remove delay, standby could adjust its next check time.
+
+2. Preregister the promotion target when heartbeat miss happens.
+
+3. Get the estimated cluster size from the check happened in the sync interval, and adjust sync interval dynamically.
+
+4. Accept join requests based on active size and alive peers.
diff --git a/Documentation/development-tools.md b/Documentation/development-tools.md
new file mode 100644
index 0000000..0847398
--- /dev/null
+++ b/Documentation/development-tools.md
@@ -0,0 +1,12 @@
+# Development tools
+
+## Vagrant
+
+For fast start you can use Vagrant. `vagrant up` will make etcd build and running on virtual machine. Required Vagrant version is 1.5.0.
+
+Next lets set a single key and then retrieve it:
+
+```
+curl -L http://127.0.0.1:4001/v2/keys/mykey -XPUT -d value="this is awesome"
+curl -L http://127.0.0.1:4001/v2/keys/mykey
+```
diff --git a/Documentation/discovery-protocol.md b/Documentation/discovery-protocol.md
new file mode 100644
index 0000000..03be668
--- /dev/null
+++ b/Documentation/discovery-protocol.md
@@ -0,0 +1,87 @@
+# Discovery Protocol
+
+Starting a new etcd cluster can be painful since each machine needs to know of at least one live machine in the cluster. If you are trying to bring up a new cluster all at once, say using an AWS cloud formation, you also need to coordinate who will be the initial cluster leader. The discovery protocol uses an existing running etcd cluster to start a second etcd cluster.
+
+To use this feature you add the command line flag `-discovery` to your etcd args. In this example we will use `http://example.com/v2/keys/_etcd/registry` as the URL prefix.
+
+## The Protocol
+
+By convention the etcd discovery protocol uses the key prefix `_etcd/registry`. A full URL to the keyspace will be `http://example.com/v2/keys/_etcd/registry`.
+
+### Creating a New Cluster
+
+Generate a unique token that will identify the new cluster. This will be used as a key prefix in the following steps. An easy way to do this is to use uuidgen:
+
+```
+UUID=$(uuidgen)
+```
+
+### Bringing up Machines
+
+Now that you have your cluster ID you can start bringing up machines. Every machine will follow this protocol internally in etcd if given a `-discovery`.
+
+### Registering your Machine
+
+The first thing etcd must do is register your machine. This is done by using the machine name (from the `-name` arg) and posting it with a long TTL to the given key.
+
+```
+curl -X PUT "http://example.com/v2/keys/_etcd/registry/${UUID}/${etcd_machine_name}?ttl=604800" -d value=${peer_addr}
+```
+
+### Discovering Peers
+
+Now that this etcd machine is registered it must discover its peers.
+
+But, the tricky bit of starting a new cluster is that one machine needs to assume the initial role of leader and will have no peers. To figure out if another machine has already started the cluster etcd needs to create the `_state` key and set its value to "started":
+
+```
+curl -X PUT "http://example.com/v2/keys/_etcd/registry/${UUID}/_state?prevExist=false" -d value=started
+```
+
+If this returns a `200 OK` response then this machine is the initial leader and should start with no peers configured. If, however, this returns a `412 Precondition Failed` then you need to find all of the registered peers:
+
+```
+curl -X GET "http://example.com/v2/keys/_etcd/registry/${UUID}?recursive=true"
+```
+
+```
+{
+    "action": "get",
+    "node": {
+        "createdIndex": 11,
+        "dir": true,
+        "key": "/_etcd/registry/9D4258A5-A1D3-4074-8837-31C1E091131D",
+        "modifiedIndex": 11,
+        "nodes": [
+            {
+                "createdIndex": 16,
+                "expiration": "2014-02-03T13:19:57.631253589-08:00",
+                "key": "/_etcd/registry/9D4258A5-A1D3-4074-8837-31C1E091131D/peer1",
+                "modifiedIndex": 16,
+                "ttl": 604765,
+                "value": "127.0.0.1:7001"
+            },
+            {
+                "createdIndex": 17,
+                "expiration": "2014-02-03T13:19:57.631253589-08:00",
+                "key": "/_etcd/registry/9D4258A5-A1D3-4074-8837-31C1E091131D/peer2",
+                "modifiedIndex": 17,
+                "ttl": 604765,
+                "value": "127.0.0.1:7002"
+            }
+        ]
+    }
+}
+```
+
+Using this information you can connect to the rest of the peers in the cluster.
+
+### Heartbeating
+
+At this point etcd will start heart beating to your registration URL. The
+protocol uses a heartbeat so permanently deleted nodes get slowly removed from
+the discovery information cluster.
+
+The heartbeat interval is about once per day and the TTL is one week. This
+should give a sufficiently wide window to protect against a discovery service
+taking a temporary outage yet provide adequate cleanup.
diff --git a/Documentation/errorcode.md b/Documentation/errorcode.md
new file mode 100644
index 0000000..b483973
--- /dev/null
+++ b/Documentation/errorcode.md
@@ -0,0 +1,60 @@
+Error Code
+======
+
+This document describes the error code in **Etcd** project.
+
+It's categorized into four groups:
+
+- Command Related Error
+- Post Form Related Error
+- Raft Related Error
+- Etcd Related Error
+
+Error code corresponding strerror
+------
+
+    const (
+        EcodeKeyNotFound    = 100
+        EcodeTestFailed     = 101
+        EcodeNotFile        = 102
+        EcodeNoMorePeer     = 103
+        EcodeNotDir         = 104
+        EcodeNodeExist      = 105
+        EcodeKeyIsPreserved = 106
+        EcodeRootROnly      = 107
+
+        EcodeValueRequired     = 200
+        EcodePrevValueRequired = 201
+        EcodeTTLNaN            = 202
+        EcodeIndexNaN          = 203
+
+        EcodeRaftInternal = 300
+        EcodeLeaderElect  = 301
+
+        EcodeWatcherCleared = 400
+        EcodeEventIndexCleared = 401
+    )
+
+    // command related errors
+    errors[100] = "Key Not Found"
+    errors[101] = "Test Failed" //test and set
+    errors[102] = "Not A File"
+    errors[103] = "Reached the max number of peers in the cluster"
+    errors[104] = "Not A Directory"
+    errors[105] = "Already exists" // create
+    errors[106] = "The prefix of given key is a keyword in etcd"
+    errors[107] = "Root is read only"
+
+    // Post form related errors
+    errors[200] = "Value is Required in POST form"
+    errors[201] = "PrevValue is Required in POST form"
+    errors[202] = "The given TTL in POST form is not a number"
+    errors[203] = "The given index in POST form is not a number"
+
+    // raft related errors
+    errors[300] = "Raft Internal Error"
+    errors[301] = "During Leader Election"
+
+    // etcd related errors
+    errors[400] = "watcher is cleared due to etcd recovery"
+    errors[401] = "The event in requested index is outdated and cleared"
diff --git a/Documentation/etcd-file-system.md b/Documentation/etcd-file-system.md
new file mode 100644
index 0000000..f4ae09f
--- /dev/null
+++ b/Documentation/etcd-file-system.md
@@ -0,0 +1,101 @@
+#Etcd File System
+
+## Structure
+[TODO]
+![alt text](./img/etcd_fs_structure.jpg "etcd file system structure")
+
+## Node
+In **etcd**, the **node** is the base from which the filesystem is constructed.
+**etcd**'s file system is Unix-like with two kinds of nodes: file and directories.
+
+- A **file node** has data associated with it.
+- A **directory node** has child nodes associated with it.
+
+All nodes, regardless of type, have the following attributes and operations:
+
+### Attributes:
+- **Expiration Time** [optional]
+
+  The node will be deleted when it expires.
+
+- **ACL**
+
+  The path to the node's access control list.
+
+### Operation:
+- **Get** (path, recursive, sorted)
+
+  Get the content of the node
+    - If the node is a file, the data of the file will be returned.
+    - If the node is a directory, the child nodes of the directory will be returned.
+    - If recursive is true, it will recursively get the nodes of the directory.
+    - If sorted is true, the result will be sorted based on the path.
+
+- **Create** (path, value[optional], ttl [optional])
+
+  Create a file. Create operation will help to create intermediate directories with no expiration time.
+    - If the file already exists, create will fail.
+    - If the value is given, set will create a file.
+    - If the value is not given, set will crate a directory.
+    - If ttl is given, the node will be deleted when it expires.
+
+- **Update** (path, value[optional], ttl [optional])
+
+  Update the content of the node.
+    - If the value is given, the value of the key will be updated.
+    - If ttl is given, the expiration time of the node will be updated.
+
+- **Delete** (path, recursive)
+
+  Delete the node of given path.
+    - If the node is a directory:
+    - If recursive is true, the operation will delete all nodes under the directory.
+    - If recursive is false, error will be returned.
+
+- **TestAndSet** (path, prevValue [prevIndex], value, ttl)
+
+  Atomic *test and set* value to a file. If test succeeds, this operation will change the previous value of the file to the given value.
+    - If the prevValue is given, it will test against previous value of
+    the node.
+    - If the prevValue is empty, it will test if the node is not existing.
+    - If the prevValue is not empty, it will test if the prevValue is equal to the current value of the file.
+    - If the prevIndex is given, it will test if the create/last modified index of the node is equal to prevIndex.
+
+- **Renew** (path, ttl)
+
+  Set the node's expiration time to (current time + ttl)
+
+## ACL
+
+### Theory
+Etcd exports a Unix-like file system interface consisting of files and directories, collectively called nodes.
+Each node has various meta-data, including three names of the access control lists used to control reading, writing and changing (change ACL names for the node).
+
+We are storing the ACL names for nodes under a special *ACL* directory.
+Each node has ACL name corresponding to one file within *ACL* dir.
+Unless overridden, a node naturally inherits the ACL names of its parent directory on creation.
+
+For each ACL name, it has three children: *R (Reading)*, *W (Writing)*, *C (Changing)*
+
+Each permission is also a node. Under the node it contains the users who have this permission for the file referring to this ACL name.
+
+### Example
+[TODO]
+### Diagram
+[TODO]
+
+### Interface
+
+Testing permissions:
+
+- (node *Node) get_perm()
+- (node *Node) has_perm(perm string, user string)
+
+Setting/Changing permissions:
+
+- (node *Node) set_perm(perm string)
+- (node *Node) change_ACLname(aclname string)
+
+
+## User Group
+[TODO]
diff --git a/Documentation/img/etcd_fs_structure.jpg b/Documentation/img/etcd_fs_structure.jpg
new file mode 100644
index 0000000..51140ca
Binary files /dev/null and b/Documentation/img/etcd_fs_structure.jpg differ
diff --git a/Documentation/internal-protocol-versioning.md b/Documentation/internal-protocol-versioning.md
new file mode 100644
index 0000000..6df1fd4
--- /dev/null
+++ b/Documentation/internal-protocol-versioning.md
@@ -0,0 +1,61 @@
+# Versioning
+
+Goal: We want to be able to upgrade an individual peer in an etcd cluster to a newer version of etcd.
+The process will take the form of individual followers upgrading to the latest version until the entire cluster is on the new version.
+
+Immediate need: etcd is moving too fast to version the internal API right now.
+But, we need to keep mixed version clusters from being started by a rolling upgrade process (e.g. the CoreOS developer alpha).
+
+Longer term need: Having a mixed version cluster where all peers are not running the exact same version of etcd itself but are able to speak one version of the internal protocol.
+
+Solution: The internal protocol needs to be versioned just as the client protocol is.
+Initially during the 0.\*.\* series of etcd releases we won't allow mixed versions at all.
+
+## Join Control
+
+We will add a version field to the join command.
+But, who decides whether a newly upgraded follower should be able to join a cluster?
+
+### Leader Controlled
+
+If the leader controls the version of followers joining the cluster then it compares its version to the version number presented by the follower in the JoinCommand and rejects the join if the number is less than the leader's version number.
+
+Advantages
+
+- Leader controls all cluster decisions still
+
+Disadvantages
+
+- Follower knows better what versions of the internal protocol it can talk than the leader
+
+
+### Follower Controlled
+
+A newly upgraded follower should be able to figure out the leaders internal version from a defined internal backwards compatible API endpoint and figure out if it can join the cluster.
+If it cannot join the cluster then it simply exits.
+
+Advantages
+
+- The follower is running newer code and knows better if it can talk older protocols
+
+Disadvantages
+
+- This cluster decision isn't made by the leader
+
+## Recommendation
+
+To solve the immediate need and to plan for the future lets do the following:
+
+- Add Version field to JoinCommand
+- Have a joining follower read the Version field of the leader and if its own version doesn't match the leader then sleep for some random interval and retry later to see if the leader has upgraded.
+
+# Research
+
+## Zookeeper versioning
+
+Zookeeper very recently added versioning into the protocol and it doesn't seem to have seen any use yet.
+https://issues.apache.org/jira/browse/ZOOKEEPER-1633
+
+## doozerd
+
+doozerd stores the version number of the peers in the datastore for other clients to check, no decisions are made off of this number currently.
diff --git a/Documentation/libraries-and-tools.md b/Documentation/libraries-and-tools.md
new file mode 100644
index 0000000..23dd861
--- /dev/null
+++ b/Documentation/libraries-and-tools.md
@@ -0,0 +1,99 @@
+## Libraries and Tools
+
+**Tools**
+
+- [etcdctl](https://github.com/coreos/etcdctl) - A command line client for etcd
+- [etcd-dump](https://npmjs.org/package/etcd-dump) - Command line utility for dumping/restoring etcd.
+- [etcd-fs](https://github.com/xetorthio/etcd-fs) - FUSE filesystem for etcd
+- [etcd-browser](https://github.com/henszey/etcd-browser) - A web-based key/value editor for etcd using AngularJS
+
+**Go libraries**
+
+- [go-etcd](https://github.com/coreos/go-etcd) - Supports v2
+
+**Java libraries**
+
+- [justinsb/jetcd](https://github.com/justinsb/jetcd)
+- [diwakergupta/jetcd](https://github.com/diwakergupta/jetcd) - Supports v2
+- [jurmous/etcd4j](https://github.com/jurmous/etcd4j) - Supports v2
+
+**Python libraries**
+
+- [jplana/python-etcd](https://github.com/jplana/python-etcd) - Supports v2
+- [russellhaering/txetcd](https://github.com/russellhaering/txetcd) - a Twisted Python library
+- [cholcombe973/autodock](https://github.com/cholcombe973/autodock) - A docker deployment automation tool
+
+**Node libraries**
+
+- [stianeikeland/node-etcd](https://github.com/stianeikeland/node-etcd) - Supports v2 (w Coffeescript)
+- [lavagetto/nodejs-etcd](https://github.com/lavagetto/nodejs-etcd) - Supports v2
+
+**Ruby libraries**
+
+- [iconara/etcd-rb](https://github.com/iconara/etcd-rb)
+- [jpfuentes2/etcd-ruby](https://github.com/jpfuentes2/etcd-ruby)
+- [ranjib/etcd-ruby](https://github.com/ranjib/etcd-ruby) - Supports v2
+
+**C libraries**
+
+- [jdarcy/etcd-api](https://github.com/jdarcy/etcd-api) - Supports v2
+
+**C++ libraries**
+- [edwardcapriolo/etcdcpp](https://github.com/edwardcapriolo/etcdcpp) - Supports v2
+
+**Clojure libraries**
+
+- [aterreno/etcd-clojure](https://github.com/aterreno/etcd-clojure)
+- [dwwoelfel/cetcd](https://github.com/dwwoelfel/cetcd) - Supports v2
+- [rthomas/clj-etcd](https://github.com/rthomas/clj-etcd) - Supports v2
+
+**Erlang libraries**
+
+- [marshall-lee/etcd.erl](https://github.com/marshall-lee/etcd.erl)
+
+**.Net Libraries**
+
+- [drusellers/etcetera](https://github.com/drusellers/etcetera)
+
+**PHP Libraries**
+
+- [linkorb/etcd-php](https://github.com/linkorb/etcd-php)
+
+**Haskell libraries**
+
+- [wereHamster/etcd-hs](https://github.com/wereHamster/etcd-hs)
+
+A detailed recap of client functionalities can be found in the [clients compatibility matrix][clients-matrix.md].
+
+[clients-matrix.md]: https://github.com/coreos/etcd/blob/master/Documentation/clients-matrix.md
+
+**Chef Integration**
+
+- [coderanger/etcd-chef](https://github.com/coderanger/etcd-chef)
+
+**Chef Cookbook**
+
+- [spheromak/etcd-cookbook](https://github.com/spheromak/etcd-cookbook)
+
+**BOSH Releases**
+
+- [cloudfoundry-community/etcd-boshrelease](https://github.com/cloudfoundry-community/etcd-boshrelease)
+- [cloudfoundry/cf-release](https://github.com/cloudfoundry/cf-release/tree/master/jobs/etcd)
+
+**Projects using etcd**
+
+- [binocarlos/yoda](https://github.com/binocarlos/yoda) - etcd + ZeroMQ
+- [calavera/active-proxy](https://github.com/calavera/active-proxy) - HTTP Proxy configured with etcd
+- [derekchiang/etcdplus](https://github.com/derekchiang/etcdplus) - A set of distributed synchronization primitives built upon etcd
+- [go-discover](https://github.com/flynn/go-discover) - service discovery in Go
+- [gleicon/goreman](https://github.com/gleicon/goreman/tree/etcd) - Branch of the Go Foreman clone with etcd support
+- [garethr/hiera-etcd](https://github.com/garethr/hiera-etcd) - Puppet hiera backend using etcd
+- [mattn/etcd-vim](https://github.com/mattn/etcd-vim) - SET and GET keys from inside vim
+- [mattn/etcdenv](https://github.com/mattn/etcdenv) - "env" shebang with etcd integration
+- [kelseyhightower/confd](https://github.com/kelseyhightower/confd) - Manage local app config files using templates and data from etcd
+- [configdb](https://git.autistici.org/ai/configdb/tree/master) - A REST relational abstraction on top of arbitrary database backends, aimed at storing configs and inventories.
+- [scrz](https://github.com/scrz/scrz) - Container manager, stores configuration in etcd.
+- [fleet](https://github.com/coreos/fleet) - Distributed init system
+- [GoogleCloudPlatform/kubernetes](https://github.com/GoogleCloudPlatform/kubernetes) - Container cluster manager.
+- [mailgun/vulcand](https://github.com/mailgun/vulcand) - HTTP proxy that uses etcd as a configuration backend.
+- [duedil-ltd/discodns](https://github.com/duedil-ltd/discodns) - Simple DNS nameserver using etcd as a database for names and records.
diff --git a/Documentation/modules.md b/Documentation/modules.md
new file mode 100644
index 0000000..08944ce
--- /dev/null
+++ b/Documentation/modules.md
@@ -0,0 +1,118 @@
+## Modules
+
+etcd has a number of modules that are built on top of the core etcd API.
+These modules provide things like dashboards, locks and leader election (removed).
+
+**Warning**: Modules are deprecated from v0.4 until we have a solid base we can apply them back onto.
+For now, we are choosing to focus on raft algorithm and core etcd to make sure that it works correctly and fast.
+And it is time consuming to maintain these modules in this period, given that etcd's API changes from time to time.
+Moreover, the lock module has some unfixed bugs, which may mislead users.
+But we also notice that these modules are popular and useful, and plan to add them back with full functionality as soon as possible.
+
+### Dashboard
+
+An HTML dashboard can be found at `http://127.0.0.1:4001/mod/dashboard/`.
+This dashboard is compiled into the etcd binary and uses the same API as regular etcd clients.
+
+Use the `-cors='*'` flag to allow your browser to request information from the current master as it changes.
+
+### Lock
+
+The Lock module implements a fair lock that can be used when lots of clients want access to a single resource.
+A lock can be associated with a value.
+The value is unique so if a lock tries to request a value that is already queued for a lock then it will find it and watch until that value obtains the lock.
+You may supply a `timeout` which will cancel the lock request if it is not obtained within `timeout` seconds.  If `timeout` is not supplied, it is presumed to be infinite.  If `timeout` is `0`, the lock request will fail if it is not immediately acquired.
+If you lock the same value on a key from two separate curl sessions they'll both return at the same time.
+
+Here's the API:
+
+**Acquire a lock (with no value) for "customer1"**
+
+```sh
+curl -X POST http://127.0.0.1:4001/mod/v2/lock/customer1?ttl=60
+```
+
+**Acquire a lock for "customer1" that is associated with the value "bar"**
+
+```sh
+curl -X POST http://127.0.0.1:4001/mod/v2/lock/customer1?ttl=60 -d value=bar
+```
+
+**Acquire a lock for "customer1" that is associated with the value "bar" only if it is done within 2 seconds**
+
+```sh
+curl -X POST http://127.0.0.1:4001/mod/v2/lock/customer1?ttl=60 -d value=bar -d timeout=2
+```
+
+**Renew the TTL on the "customer1" lock for index 2**
+
+```sh
+curl -X PUT http://127.0.0.1:4001/mod/v2/lock/customer1?ttl=60 -d index=2
+```
+
+**Renew the TTL on the "customer1" lock for value "bar"**
+
+```sh
+curl -X PUT http://127.0.0.1:4001/mod/v2/lock/customer1?ttl=60 -d value=bar
+```
+
+**Retrieve the current value for the "customer1" lock.**
+
+```sh
+curl http://127.0.0.1:4001/mod/v2/lock/customer1
+```
+
+**Retrieve the current index for the "customer1" lock**
+
+```sh
+curl http://127.0.0.1:4001/mod/v2/lock/customer1?field=index
+```
+
+**Delete the "customer1" lock with the index 2**
+
+```sh
+curl -X DELETE http://127.0.0.1:4001/mod/v2/lock/customer1?index=2
+```
+
+**Delete the "customer1" lock with the value "bar"**
+
+```sh
+curl -X DELETE http://127.0.0.1:4001/mod/v2/lock/customer1?value=bar
+```
+
+
+### Leader Election (Deprecated and Removed in 0.4)
+
+The Leader Election module wraps the Lock module to allow clients to come to consensus on a single value.
+This is useful when you want one server to process at a time but allow other servers to fail over.
+The API is similar to the Lock module but is limited to simple strings values.
+
+Here's the API:
+
+**Attempt to set a value for the "order_processing" leader key:**
+
+```sh
+curl -X PUT http://127.0.0.1:4001/mod/v2/leader/order_processing?ttl=60 -d name=myserver1.foo.com
+```
+
+**Retrieve the current value for the "order_processing" leader key:**
+
+```sh
+curl http://127.0.0.1:4001/mod/v2/leader/order_processing
+myserver1.foo.com
+```
+
+**Remove a value from the "order_processing" leader key:**
+
+```sh
+curl -X DELETE http://127.0.0.1:4001/mod/v2/leader/order_processing?name=myserver1.foo.com
+```
+
+If multiple clients attempt to set the value for a key then only one will succeed.
+The other clients will hang until the current value is removed because of TTL or because of a `DELETE` operation.
+Multiple clients can submit the same value and will all be notified when that value succeeds.
+
+To update the TTL of a value simply reissue the same `PUT` command that you used to set the value.
+
+
+
diff --git a/Documentation/optimal-cluster-size.md b/Documentation/optimal-cluster-size.md
new file mode 100644
index 0000000..2aa7c95
--- /dev/null
+++ b/Documentation/optimal-cluster-size.md
@@ -0,0 +1,38 @@
+# Optimal etcd Cluster Size
+
+etcd's Raft consensus algorithm is most efficient in small clusters between 3 and 9 peers. For clusters larger than 9, etcd will select a subset of instances to participate in the algorithm in order to keep it efficient. The end of this document briefly explores how etcd works internally and why these choices have been made.
+
+## Cluster Management
+
+You can manage the active cluster size through the [cluster config API](https://github.com/coreos/etcd/blob/master/Documentation/api.md#cluster-config). `activeSize` represents the etcd peers allowed to actively participate in the consensus algorithm.
+
+If the total number of etcd instances exceeds this number, additional peers are started as [standbys](https://github.com/coreos/etcd/blob/master/Documentation/design/standbys.md), which can be promoted to active participation if one of the existing active instances has failed or been removed.
+
+## Internals of etcd
+
+### Writing to etcd
+
+Writes to an etcd peer are always redirected to the leader of the cluster and distributed to all of the peers immediately. A write is only considered successful when a majority of the peers acknowledge the write.
+
+For example, in a cluster with 5 peers, a write operation is only as fast as the 3rd fastest machine. This is the main reason for keeping the number of active peers below 9. In practice, you only need to worry about write performance in high latency environments such as a cluster spanning multiple data centers.
+
+### Leader Election
+
+The leader election process is similar to writing a key &mdash; a majority of the active peers must acknowledge the new leader before cluster operations can continue. The longer each peer takes to elect a new leader means you have to wait longer before you can write to the cluster again. In low latency environments this process takes milliseconds.
+
+### Odd Active Cluster Size
+
+The other important cluster optimization is to always have an odd active cluster size (i.e. `activeSize`). Adding an odd node to the number of peers doesn't change the size of the majority and therefore doesn't increase the total latency of the majority as described above. But, you gain a higher tolerance for peer failure by adding the extra machine. You can see this in practice when comparing two even and odd sized clusters:
+
+| Active Peers | Majority   | Failure Tolerance |
+|--------------|------------|-------------------|
+| 1 peers      | 1 peers    | None              |
+| 3 peers      | 2 peers    | 1 peer            |
+| 4 peers      | 3 peers    | 1 peer           |
+| 5 peers      | 3 peers    | **2 peers**       |
+| 6 peers      | 4 peers    | 2 peers           |
+| 7 peers      | 4 peers    | **3 peers**       |
+| 8 peers      | 5 peers    | 3 peers           |
+| 9 peers      | 5 peers    | **4 peers**       |
+
+As you can see, adding another peer to bring the number of active peers up to an odd size is always worth it. During a network partition, an odd number of active peers also guarantees that there will almost always be a majority of the cluster that can continue to operate and be the source of truth when the partition ends.
diff --git a/Documentation/platforms/freebsd.md b/Documentation/platforms/freebsd.md
new file mode 100644
index 0000000..13384db
--- /dev/null
+++ b/Documentation/platforms/freebsd.md
@@ -0,0 +1,62 @@
+# FreeBSD
+
+Starting with version 0.1.2 both etcd and etcdctl have been ported to FreeBSD and can
+be installed either via packages or ports system. Their versions have been recently
+updated to 0.2.0 so now you can enjoy using etcd and etcdctl on FreeBSD 10.0 (RC4 as
+of now) and 9.x where they have been tested. They might also work when installed from
+ports on earlier versions of FreeBSD, but your mileage may vary.
+
+## Installation
+
+### Using pkgng package system
+
+1. If you do not have pkg­ng installed, install it with command `pkg` and answering 'Y'
+when asked
+
+2. Update your repository data with `pkg update`
+
+3. Install etcd with `pkg install coreos­etcd coreos­etcdctl`
+
+4. Verify successful installation with `pkg info | grep etcd` and you should get:
+
+```
+r@fbsd­10:/ # pkg info | grep etcd
+coreos­etcd­0.2.0              Highly­available key value store and service discovery
+coreos­etcdctl­0.2.0           Simple commandline client for etcd
+r@fbsd­10:/ #
+```
+
+5. You’re ready to use etcd and etcdctl! For more information about using pkgng, please
+see: http://www.freebsd.org/doc/handbook/pkgng­intro.html
+ 
+### Using ports system
+
+1. If you do not have ports installed, install with with `portsnap fetch extract` (it
+may take some time depending on your hardware and network connection)
+
+2. Build etcd with `cd /usr/ports/devel/etcd && make install clean`, you
+will get an option to build and install documentation and etcdctl with it.
+
+3. If you haven't installed it with etcdctl, and you would like to install it later, you can build it
+with `cd /usr/ports/devel/etcdctl && make install clean`
+
+4. Verify successful installation with `pkg info | grep etcd` and you should get:
+ 
+
+```
+r@fbsd­10:/ # pkg info | grep etcd
+coreos­etcd­0.2.0              Highly­available key value store and service discovery
+coreos­etcdctl­0.2.0           Simple commandline client for etcd
+r@fbsd­10:/ #
+```
+
+5. You’re ready to use etcd and etcdctl! For more information about using ports system,
+please see: https://www.freebsd.org/doc/handbook/ports­using.html
+
+## Issues
+
+If you find any issues with the build/install procedure or you've found a problem that
+you've verified is local to FreeBSD version only (for example, by not being able to
+reproduce it on any other platform, like OSX or Linux), please sent a
+problem report using this page for more
+information: http://www.freebsd.org/send­pr.html
diff --git a/Documentation/production-ready.md b/Documentation/production-ready.md
new file mode 100644
index 0000000..a76e08f
--- /dev/null
+++ b/Documentation/production-ready.md
@@ -0,0 +1,7 @@
+etcd is being used successfully by many companies in production. It is,
+however, under active development and systems like etcd are difficult to get
+correct. If you are comfortable with bleeding-edge software please use etcd and
+provide us with the feedback and testing young software needs.
+
+When the etcd team feels confident removing this warning we will release etcd
+1.0.
diff --git a/Documentation/security.md b/Documentation/security.md
new file mode 100644
index 0000000..30a881d
--- /dev/null
+++ b/Documentation/security.md
@@ -0,0 +1,131 @@
+# Reading and Writing over HTTPS
+
+## Transport Security with HTTPS
+
+Etcd supports SSL/TLS and client cert authentication for clients to server, as well as server to server communication.
+
+First, you need to have a CA cert `clientCA.crt` and signed key pair `client.crt`, `client.key`.
+This site has a good reference for how to generate self-signed key pairs:
+http://www.g-loaded.eu/2005/11/10/be-your-own-ca/
+Or you could use [etcd-ca](https://github.com/coreos/etcd-ca) to generate certs and keys.
+
+For testing you can use the certificates in the `fixtures/ca` directory.
+
+Let's configure etcd to use this keypair:
+
+```sh
+./etcd -f -name machine0 -data-dir machine0 -cert-file=./fixtures/ca/server.crt -key-file=./fixtures/ca/server.key.insecure
+```
+
+There are a few new options we're using:
+
+* `-f` - forces a new machine configuration, even if an existing configuration is found. (WARNING: data loss!)
+* `-cert-file` and `-key-file` specify the location of the cert and key files to be used for for transport layer security between the client and server.
+
+You can now test the configuration using HTTPS:
+
+```sh
+curl --cacert ./fixtures/ca/server-chain.pem https://127.0.0.1:4001/v2/keys/foo -XPUT -d value=bar -v
+```
+
+You should be able to see the handshake succeed.
+
+**OSX 10.9+ Users**: curl 7.30.0 on OSX 10.9+ doesn't understand certificates passed in on the command line.
+Instead you must import the dummy ca.crt directly into the keychain or add the `-k` flag to curl to ignore errors.
+If you want to test without the `-k` flag run `open ./fixtures/ca/ca.crt` and follow the prompts.
+Please remove this certificate after you are done testing!
+If you know of a workaround let us know.
+
+```
+...
+SSLv3, TLS handshake, Finished (20):
+...
+```
+
+And also the response from the etcd server:
+
+```json
+{
+    "action": "set",
+    "key": "/foo",
+    "modifiedIndex": 3,
+    "value": "bar"
+}
+```
+
+
+## Authentication with HTTPS Client Certificates
+
+We can also do authentication using CA certs.
+The clients will provide their cert to the server and the server will check whether the cert is signed by the CA and decide whether to serve the request.
+
+```sh
+./etcd -f -name machine0 -data-dir machine0 -ca-file=./fixtures/ca/ca.crt -cert-file=./fixtures/ca/server.crt -key-file=./fixtures/ca/server.key.insecure
+```
+
+```-ca-file``` is the path to the CA cert.
+
+Try the same request to this server:
+
+```sh
+curl --cacert ./fixtures/ca/server-chain.pem https://127.0.0.1:4001/v2/keys/foo -XPUT -d value=bar -v
+```
+
+The request should be rejected by the server.
+
+```
+...
+routines:SSL3_READ_BYTES:sslv3 alert bad certificate
+...
+```
+
+We need to give the CA signed cert to the server.
+
+```sh
+curl --key ./fixtures/ca/server2.key.insecure --cert ./fixtures/ca/server2.crt --cacert ./fixtures/ca/server-chain.pem -L https://127.0.0.1:4001/v2/keys/foo -XPUT -d value=bar -v
+```
+
+You should able to see:
+
+```
+...
+SSLv3, TLS handshake, CERT verify (15):
+...
+TLS handshake, Finished (20)
+```
+
+And also the response from the server:
+
+```json
+{
+    "action": "set",
+    "node": {
+        "createdIndex": 12,
+        "key": "/foo",
+        "modifiedIndex": 12,
+        "value": "bar"
+    }
+}
+```
+
+### Why SSLv3 alert handshake failure when using SSL client auth?
+
+The `crypto/tls` package of `golang` checks the key usage of the certificate public key before using it.
+To use the certificate public key to do client auth, we need to add `clientAuth` to `Extended Key Usage` when creating the certificate public key.
+
+Here is how to do it:
+
+Add the following section to your openssl.cnf:
+
+```
+[ ssl_client ]
+...
+  extendedKeyUsage = clientAuth
+...
+```
+
+When creating the cert be sure to reference it in the `-extensions` flag:
+
+```
+openssl ca -config openssl.cnf -policy policy_anything -extensions ssl_client -out certs/machine.crt -infiles machine.csr
+```
diff --git a/Documentation/tuning.md b/Documentation/tuning.md
new file mode 100644
index 0000000..c9899a5
--- /dev/null
+++ b/Documentation/tuning.md
@@ -0,0 +1,95 @@
+## Tuning
+
+The default settings in etcd should work well for installations on a local network where the average network latency is low.
+However, when using etcd across multiple data centers or over networks with high latency you may need to tweak the heartbeat interval and election timeout settings.
+
+The network isn't the only source of latency. Each request and response may be impacted by slow disks on both the leader and follower. Each of these timeouts represents the total time from request to successful response from the other machine.
+
+### Time Parameters
+
+The underlying distributed consensus protocol relies on two separate time parameters to ensure that nodes can handoff leadership if one stalls or goes offline.
+The first parameter is called the *Heartbeat Interval*.
+This is the frequency with which the leader will notify followers that it is still the leader.
+etcd batches commands together for higher throughput so this heartbeat interval is also a delay for how long it takes for commands to be committed.
+By default, etcd uses a `50ms` heartbeat interval.
+
+The second parameter is the *Election Timeout*.
+This timeout is how long a follower node will go without hearing a heartbeat before attempting to become leader itself.
+By default, etcd uses a `200ms` election timeout.
+
+Adjusting these values is a trade off.
+Lowering the heartbeat interval will cause individual commands to be committed faster but it will lower the overall throughput of etcd.
+If your etcd instances have low utilization then lowering the heartbeat interval can improve your command response time.
+
+The election timeout should be set based on the heartbeat interval and your network ping time between nodes.
+Election timeouts should be at least 10 times your ping time so it can account for variance in your network.
+For example, if the ping time between your nodes is 10ms then you should have at least a 100ms election timeout.
+
+You should also set your election timeout to at least 4 to 5 times your heartbeat interval to account for variance in leader replication.
+For a heartbeat interval of 50ms you should set your election timeout to at least 200ms - 250ms.
+
+You can override the default values on the command line:
+
+```sh
+# Command line arguments:
+$ etcd -peer-heartbeat-interval=100 -peer-election-timeout=500
+
+# Environment variables:
+$ ETCD_PEER_HEARTBEAT_INTERVAL=100 ETCD_PEER_ELECTION_TIMEOUT=500 etcd
+```
+
+Or you can set the values within the configuration file:
+
+```toml
+[peer]
+heartbeat_interval = 100
+election_timeout = 500
+```
+
+The values are specified in milliseconds.
+
+
+### Snapshots
+
+etcd appends all key changes to a log file.
+This log grows forever and is a complete linear history of every change made to the keys.
+A complete history works well for lightly used clusters but clusters that are heavily used would carry around a large log.
+
+To avoid having a huge log etcd makes periodic snapshots.
+These snapshots provide a way for etcd to compact the log by saving the current state of the system and removing old logs.
+
+### Snapshot Tuning
+
+Creating snapshots can be expensive so they're only created after a given number of changes to etcd.
+By default, snapshots will be made after every 10,000 changes.
+If etcd's memory usage and disk usage are too high, you can lower the snapshot threshold by setting the following on the command line:
+
+```sh
+# Command line arguments:
+$ etcd -snapshot-count=5000
+
+# Environment variables:
+$ ETCD_SNAPSHOT_COUNT=5000 etcd
+```
+
+Or you can change the setting in the configuration file:
+
+```toml
+snapshot_count = 5000
+```
+
+You can also disable snapshotting by adding the following to your command line:
+
+```sh
+# Command line arguments:
+$ etcd -snapshot false
+
+# Environment variables:
+$ ETCD_SNAPSHOT=false etcd
+```
+
+You can also disable snapshotting within the configuration file:
+
+```toml
+snapshot = false
+```
diff --git a/Documentation/upgrade.md b/Documentation/upgrade.md
new file mode 100644
index 0000000..b13f4f6
--- /dev/null
+++ b/Documentation/upgrade.md
@@ -0,0 +1,17 @@
+# Upgrading an Existing Cluster
+
+etcd clusters can be upgraded by doing a rolling upgrade or all at once. We make every effort to test this process, but please be sure to backup your data [by etcd-dump](https://github.com/AaronO/etcd-dump), or make a copy of data directory beforehand.
+
+## Upgrade Process
+
+- Stop the old etcd processes
+- Upgrade the etcd binary
+- Restart the etcd instance using the original --name, --address, --peer-address and --data-dir.
+
+## Rolling Upgrade
+
+During an upgrade, etcd clusters are designed to continue working in a mix of old and new versions. It's recommended to converge on the new version quickly. Using new API features before the entire cluster has been upgraded is only supported as a best effort. Each instance's version can be found with `curl http://127.0.0.1:4001/version`.
+
+## All at Once
+
+If downtime is not an issue, the easiest way to upgrade your cluster is to shutdown all of the etcd instances and restart them with the new binary. The current state of the cluster is saved to disk and will be loaded into the cluster when it restarts.
diff --git a/doc/api.md b/doc/api.md
deleted file mode 100644
index ab2a83a..0000000
--- a/doc/api.md
+++ /dev/null
@@ -1,1314 +0,0 @@
-# etcd API
-
-## Running a Single Machine Cluster
-
-These examples will use a single machine cluster to show you the basics of the etcd REST API.
-Let's start etcd:
-
-```sh
-./bin/etcd -data-dir machine0 -name machine0
-```
-
-This will bring up etcd listening on default ports (4001 for client communication and 7001 for server-to-server communication).
-The `-data-dir machine0` argument tells etcd to write machine configuration, logs and snapshots to the `./machine0/` directory.
-The `-name machine0` tells the rest of the cluster that this machine is named machine0.
-
-## Getting the etcd version
-
-The etcd version of a specific instance can be obtained from the `/version` endpoint.
-
-```sh
-curl -L http://127.0.0.1:4001/version
-```
-
-## Key Space Operations
-
-The primary API of etcd is a hierarchical key space.
-The key space consists of directories and keys which are generically referred to as "nodes".
-
-
-### Setting the value of a key
-
-Let's set the first key-value pair in the datastore.
-In this case the key is `/message` and the value is `Hello world`.
-
-```sh
-curl -L http://127.0.0.1:4001/v2/keys/message -XPUT -d value="Hello world"
-```
-
-```json
-{
-    "action": "set",
-    "node": {
-        "createdIndex": 2,
-        "key": "/message",
-        "modifiedIndex": 2,
-        "value": "Hello world"
-    }
-}
-```
-
-The response object contains several attributes:
-
-1. `action`: the action of the request that was just made.
-The request attempted to modify `node.value` via a `PUT` HTTP request, thus the value of action is `set`.
-
-2. `node.key`: the HTTP path to which the request was made.
-We set `/message` to `Hello world`, so the key field is `/message`.
-etcd uses a file-system-like structure to represent the key-value pairs, therefore all keys start with `/`.
-
-3. `node.value`: the value of the key after resolving the request.
-In this case, a successful request was made that attempted to change the node's value to `Hello world`.
-
-4. `node.createdIndex`: an index is a unique, monotonically-incrementing integer created for each change to etcd.
-This specific index reflects the point in the etcd state machine at which a given key was created.
-You may notice that in this example the index is `2` even though it is the first request you sent to the server.
-This is because there are internal commands that also change the state behind the scenes, like adding and syncing servers.
-
-5. `node.modifiedIndex`: like `node.createdIndex`, this attribute is also an etcd index.
-Actions that cause the value to change include `set`, `delete`, `update`, `create`, `compareAndSwap` and `compareAndDelete`.
-Since the `get` and `watch` commands do not change state in the store, they do not change the value of `node.modifiedIndex`.
-
-
-### Response Headers
-
-etcd includes a few HTTP headers in responses that provide global information about the etcd cluster that serviced a request:
-
-```
-X-Etcd-Index: 35
-X-Raft-Index: 5398
-X-Raft-Term: 0
-```
-
-- `X-Etcd-Index` is the current etcd index as explained above.
-- `X-Raft-Index` is similar to the etcd index but is for the underlying raft protocol
-- `X-Raft-Term` is an integer that will increase whenever an etcd master election happens in the cluster. If this number is increasing rapidly, you may need to tune the election timeout. See the [tuning][tuning] section for details.
-
-[tuning]: #tuning
-
-
-### Get the value of a key
-
-We can get the value that we just set in `/message` by issuing a `GET` request:
-
-```sh
-curl -L http://127.0.0.1:4001/v2/keys/message
-```
-
-```json
-{
-    "action": "get",
-    "node": {
-        "createdIndex": 2,
-        "key": "/message",
-        "modifiedIndex": 2,
-        "value": "Hello world"
-    }
-}
-```
-
-
-### Changing the value of a key
-
-You can change the value of `/message` from `Hello world` to `Hello etcd` with another `PUT` request to the key:
-
-```sh
-curl -L http://127.0.0.1:4001/v2/keys/message -XPUT -d value="Hello etcd"
-```
-
-```json
-{
-    "action": "set",
-    "node": {
-        "createdIndex": 3,
-        "key": "/message",
-        "modifiedIndex": 3,
-        "value": "Hello etcd"
-    },
-    "prevNode": {
-    	"createdIndex": 2,
-    	"key": "/message",
-    	"value": "Hello world",
-    	"modifiedIndex": 2
-    }
-}
-```
-
-Here we introduce a new field: `prevNode`. The `prevNode` field represents what the state of a given node was before resolving the request at hand. The `prevNode` field follows the same format as the `node`, and is omitted in the event that there was no previous state for a given node.
-
-### Deleting a key
-
-You can remove the `/message` key with a `DELETE` request:
-
-```sh
-curl -L http://127.0.0.1:4001/v2/keys/message -XDELETE
-```
-
-```json
-{
-    "action": "delete",
-    "node": {
-        "createdIndex": 3,
-        "key": "/message",
-        "modifiedIndex": 4
-    },
-    "prevNode": {
-    	"key": "/message",
-    	"value": "Hello etcd",
-    	"modifiedIndex": 3,
-    	"createdIndex": 3
-    }
-}
-```
-
-
-### Using key TTL
-
-Keys in etcd can be set to expire after a specified number of seconds.
-You can do this by setting a TTL (time to live) on the key when sending a `PUT` request:
-
-```sh
-curl -L http://127.0.0.1:4001/v2/keys/foo -XPUT -d value=bar -d ttl=5
-```
-
-```json
-{
-    "action": "set",
-    "node": {
-        "createdIndex": 5,
-        "expiration": "2013-12-04T12:01:21.874888581-08:00",
-        "key": "/foo",
-        "modifiedIndex": 5,
-        "ttl": 5,
-        "value": "bar"
-    }
-}
-```
-
-Note the two new fields in response:
-
-1. The `expiration` is the time at which this key will expire and be deleted.
-
-2. The `ttl` is the specified time to live for the key, in seconds.
-
-_NOTE_: Keys can only be expired by a cluster leader, so if a machine gets disconnected from the cluster, its keys will not expire until it rejoins.
-
-Now you can try to get the key by sending a `GET` request:
-
-```sh
-curl -L http://127.0.0.1:4001/v2/keys/foo
-```
-
-If the TTL has expired, the key will have been deleted, and you will be returned a 100.
-
-```json
-{
-    "cause": "/foo",
-    "errorCode": 100,
-    "index": 6,
-    "message": "Key Not Found"
-}
-```
-
-The TTL could be unset to avoid expiration through update operation:
-
-```sh
-curl -L http://127.0.0.1:4001/v2/keys/foo -XPUT -d value=bar -d ttl= -d prevExist=true
-```
-
-```json
-{
-    "action": "update",
-    "node": {
-        "createdIndex": 5,
-        "key": "/foo",
-        "modifiedIndex": 6,
-        "value": "bar"
-    },
-    "prevNode": {
-        "createdIndex": 5,
-        "expiration": "2013-12-04T12:01:21.874888581-08:00",
-        "key": "/foo",
-        "modifiedIndex": 5,
-        "ttl": 3,
-        "value": "bar"
-    }
-}
-```
-
-
-### Waiting for a change
-
-We can watch for a change on a key and receive a notification by using long polling.
-This also works for child keys by passing `recursive=true` in curl.
-
-In one terminal, we send a `GET` with `wait=true` :
-
-```sh
-curl -L http://127.0.0.1:4001/v2/keys/foo?wait=true
-```
-
-Now we are waiting for any changes at path `/foo`.
-
-In another terminal, we set a key `/foo` with value `bar`:
-
-```sh
-curl -L http://127.0.0.1:4001/v2/keys/foo -XPUT -d value=bar
-```
-
-The first terminal should get the notification and return with the same response as the set request:
-
-```json
-{
-    "action": "set",
-    "node": {
-        "createdIndex": 7,
-        "key": "/foo",
-        "modifiedIndex": 7,
-        "value": "bar"
-    },
-    "prevNode": {
-        "createdIndex": 6,
-        "key": "/foo",
-        "modifiedIndex": 6,
-        "value": "bar"
-    }
-}
-```
-
-However, the watch command can do more than this.
-Using the index, we can watch for commands that have happened in the past.
-This is useful for ensuring you don't miss events between watch commands.
-
-Let's try to watch for the set command of index 7 again:
-
-```sh
-curl -L 'http://127.0.0.1:4001/v2/keys/foo?wait=true&waitIndex=7'
-```
-
-The watch command returns immediately with the same response as previously.
-
-
-### Atomically Creating In-Order Keys
-
-Using `POST` on a directory, you can create keys with key names that are created in-order.
-This can be used in a variety of useful patterns, like implementing queues of keys which need to be processed in strict order.
-An example use case is the [locking module][lockmod] which uses it to ensure clients get fair access to a mutex.
-
-Creating an in-order key is easy:
-
-```sh
-curl http://127.0.0.1:4001/v2/keys/queue -XPOST -d value=Job1
-```
-
-```json
-{
-    "action": "create",
-    "node": {
-        "createdIndex": 6,
-        "key": "/queue/6",
-        "modifiedIndex": 6,
-        "value": "Job1"
-    }
-}
-```
-
-If you create another entry some time later, it is guaranteed to have a key name that is greater than the previous key.
-Also note the key names use the global etcd index, so the next key can be more than `previous + 1`.
-
-```sh
-curl http://127.0.0.1:4001/v2/keys/queue -XPOST -d value=Job2
-```
-
-```json
-{
-    "action": "create",
-    "node": {
-        "createdIndex": 29,
-        "key": "/queue/29",
-        "modifiedIndex": 29,
-        "value": "Job2"
-    }
-}
-```
-
-To enumerate the in-order keys as a sorted list, use the "sorted" parameter.
-
-```sh
-curl -s 'http://127.0.0.1:4001/v2/keys/queue?recursive=true&sorted=true'
-```
-
-```json
-{
-    "action": "get",
-    "node": {
-        "createdIndex": 2,
-        "dir": true,
-        "key": "/queue",
-        "modifiedIndex": 2,
-        "nodes": [
-            {
-                "createdIndex": 2,
-                "key": "/queue/2",
-                "modifiedIndex": 2,
-                "value": "Job1"
-            },
-            {
-                "createdIndex": 3,
-                "key": "/queue/3",
-                "modifiedIndex": 3,
-                "value": "Job2"
-            }
-        ]
-    }
-}
-```
-
-[lockmod]: #lock
-
-
-### Using a directory TTL
-
-Like keys, directories in etcd can be set to expire after a specified number of seconds.
-You can do this by setting a TTL (time to live) on a directory when it is created with a `PUT`:
-
-```sh
-curl -L http://127.0.0.1:4001/v2/keys/dir -XPUT -d ttl=30 -d dir=true
-```
-
-```json
-{
-    "action": "set",
-    "node": {
-        "createdIndex": 17,
-        "dir": true,
-        "expiration": "2013-12-11T10:37:33.689275857-08:00",
-        "key": "/dir",
-        "modifiedIndex": 17,
-        "ttl": 30
-    }
-}
-```
-
-The directory's TTL can be refreshed by making an update.
-You can do this by making a PUT with `prevExist=true` and a new TTL.
-
-```sh
-curl -L http://127.0.0.1:4001/v2/keys/dir -XPUT -d ttl=30 -d dir=true -d prevExist=true
-```
-
-Keys that are under this directory work as usual, but when the directory expires, a watcher on a key under the directory will get an expire event:
-
-```sh
-curl 'http://127.0.0.1:4001/v2/keys/dir/asdf?consistent=true&wait=true'
-```
-
-```json
-{
-	"action": "expire",
-	"node": {
-		"createdIndex": 8,
-		"key": "/dir",
-		"modifiedIndex": 15
-	},
-	"prevNode": {
-		"createdIndex": 8,
-		"key": "/dir",
-		"dir":true,
-		"modifiedIndex": 17,
-		"expiration": "2013-12-11T10:39:35.689275857-08:00"
-	}
-}
-```
-
-
-### Atomic Compare-and-Swap
-
-etcd can be used as a centralized coordination service in a cluster, and `CompareAndSwap` (CAS) is the most basic operation used to build a distributed lock service.
-
-This command will set the value of a key only if the client-provided conditions are equal to the current conditions.
-
-The current comparable conditions are:
-
-1. `prevValue` - checks the previous value of the key.
-
-2. `prevIndex` - checks the previous modifiedIndex of the key.
-
-3. `prevExist` - checks existence of the key: if `prevExist` is true, it is an `update` request; if prevExist is `false`, it is a `create` request.
-
-Here is a simple example.
-Let's create a key-value pair first: `foo=one`.
-
-```sh
-curl -L http://127.0.0.1:4001/v2/keys/foo -XPUT -d value=one
-```
-
-Now let's try some invalid `CompareAndSwap` commands.
-
-Trying to set this existing key with `prevExist=false` fails as expected:
-```sh
-curl -L http://127.0.0.1:4001/v2/keys/foo?prevExist=false -XPUT -d value=three
-```
-
-The error code explains the problem:
-
-```json
-{
-    "cause": "/foo",
-    "errorCode": 105,
-    "index": 39776,
-    "message": "Already exists"
-}
-```
-
-Now let's provide a `prevValue` parameter:
-
-```sh
-curl -L http://127.0.0.1:4001/v2/keys/foo?prevValue=two -XPUT -d value=three
-```
-
-This will try to compare the previous value of the key and the previous value we provided. If they are equal, the value of the key will change to three.
-
-```json
-{
-    "cause": "[two != one]",
-    "errorCode": 101,
-    "index": 8,
-    "message": "Test Failed"
-}
-```
-
-which means `CompareAndSwap` failed. `cause` explains why the test failed.
-Note: the condition prevIndex=0 always passes.
-
-Let's try a valid condition:
-
-```sh
-curl -L http://127.0.0.1:4001/v2/keys/foo?prevValue=one -XPUT -d value=two
-```
-
-The response should be:
-
-```json
-{
-    "action": "compareAndSwap",
-    "node": {
-        "createdIndex": 8,
-        "key": "/foo",
-        "modifiedIndex": 9,
-        "value": "two"
-    },
-    "prevNode": {
-    	"createdIndex": 8,
-    	"key": "/foo",
-    	"modifiedIndex": 8,
-    	"value": "one"
-    }
-}
-```
-
-We successfully changed the value from "one" to "two" since we gave the correct previous value.
-
-### Atomic Compare-and-Delete
-
-This command will delete a key only if the client-provided conditions are equal to the current conditions.
-
-The current comparable conditions are:
-
-1. `prevValue` - checks the previous value of the key.
-
-2. `prevIndex` - checks the previous modifiedIndex of the key.
-
-Here is a simple example. Let's first create a key: `foo=one`.
-
-```sh
-curl -L http://127.0.0.1:4001/v2/keys/foo -XPUT -d value=one
-```
-
-Now let's try some `CompareAndDelete` commands.
-
-Trying to delete the key with `prevValue=two` fails as expected:
-```sh
-curl -L http://127.0.0.1:4001/v2/keys/foo?prevValue=two -XDELETE
-```
-
-The error code explains the problem:
-
-```json
-{
-	"errorCode": 101,
-	"message": "Compare failed",
-	"cause": "[two != one]",
-	"index": 8
-}
-```
-
-As does a `CompareAndDelete` with a mismatched `prevIndex`:
-
-```sh
-curl -L http://127.0.0.1:4001/v2/keys/foo?prevIndex=1 -XDELETE
-```
-
-```json
-{
-	"errorCode": 101,
-	"message": "Compare failed",
-	"cause": "[1 != 8]",
-	"index": 8
-}
-```
-
-And now a valid `prevValue` condition:
-
-```sh
-curl -L http://127.0.0.1:4001/v2/keys/foo?prevValue=one -XDELETE
-```
-
-The successful response will look something like:
-
-```json
-{
-	"action": "compareAndDelete",
-	"node": {
-		"key": "/foo",
-		"modifiedIndex": 9,
-		"createdIndex": 8
-	},
-	"prevNode": {
-		"key": "/foo",
-		"value": "one",
-		"modifiedIndex": 8,
-		"createdIndex": 8
-	}
-}
-```
-
-### Creating Directories
-
-In most cases, directories for a key are automatically created.
-But there are cases where you will want to create a directory or remove one.
-
-Creating a directory is just like a key except you cannot provide a value and must add the `dir=true` parameter.
-
-```sh
-curl -L http://127.0.0.1:4001/v2/keys/dir -XPUT -d dir=true
-```
-```json
-{
-    "action": "set",
-    "node": {
-        "createdIndex": 30,
-        "dir": true,
-        "key": "/dir",
-        "modifiedIndex": 30
-    }
-}
-```
-
-
-### Listing a directory
-
-In etcd we can store two types of things: keys and directories.
-Keys store a single string value.
-Directories store a set of keys and/or other directories.
-
-In this example, let's first create some keys:
-
-We already have `/foo=two` so now we'll create another one called `/foo_dir/foo` with the value of `bar`:
-
-```sh
-curl -L http://127.0.0.1:4001/v2/keys/foo_dir/foo -XPUT -d value=bar
-```
-
-```json
-{
-    "action": "set",
-    "node": {
-        "createdIndex": 2,
-        "key": "/foo_dir/foo",
-        "modifiedIndex": 2,
-        "value": "bar"
-    }
-}
-```
-
-Now we can list the keys under root `/`:
-
-```sh
-curl -L http://127.0.0.1:4001/v2/keys/
-```
-
-We should see the response as an array of items:
-
-```json
-{
-    "action": "get",
-    "node": {
-        "dir": true,
-        "key": "/",
-        "nodes": [
-            {
-                "createdIndex": 2,
-                "dir": true,
-                "key": "/foo_dir",
-                "modifiedIndex": 2
-            }
-        ]
-    }
-}
-```
-
-Here we can see `/foo` is a key-value pair under `/` and `/foo_dir` is a directory.
-We can also recursively get all the contents under a directory by adding `recursive=true`.
-
-```sh
-curl -L http://127.0.0.1:4001/v2/keys/?recursive=true
-```
-
-```json
-{
-    "action": "get",
-    "node": {
-        "dir": true,
-        "key": "/",
-        "nodes": [
-            {
-                "createdIndex": 2,
-                "dir": true,
-                "key": "/foo_dir",
-                "modifiedIndex": 2,
-                "nodes": [
-                    {
-                        "createdIndex": 2,
-                        "key": "/foo_dir/foo",
-                        "modifiedIndex": 2,
-                        "value": "bar"
-                    }
-                ]
-            }
-        ]
-    }
-}
-```
-
-
-### Deleting a Directory
-
-Now let's try to delete the directory `/foo_dir`.
-
-You can remove an empty directory using the `DELETE` verb and the `dir=true` parameter.
-
-```sh
-curl -L 'http://127.0.0.1:4001/v2/keys/foo_dir?dir=true' -XDELETE
-```
-```json
-{
-    "action": "delete",
-    "node": {
-        "createdIndex": 30,
-        "dir": true,
-        "key": "/foo_dir",
-        "modifiedIndex": 31
-    },
-    "prevNode": {
-    	"createdIndex": 30,
-    	"key": "/foo_dir",
-    	"dir": true,
-    	"modifiedIndex": 30
-    }
-}
-```
-
-To delete a directory that holds keys, you must add `recursive=true`.
-
-```sh
-curl -L http://127.0.0.1:4001/v2/keys/dir?recursive=true -XDELETE
-```
-
-```json
-{
-    "action": "delete",
-    "node": {
-        "createdIndex": 10,
-        "dir": true,
-        "key": "/dir",
-        "modifiedIndex": 11
-    },
-    "prevNode": {
-    	"createdIndex": 10,
-    	"dir": true,
-    	"key": "/dir",
-    	"modifiedIndex": 10
-    }
-}
-```
-
-
-### Creating a hidden node
-
-We can create a hidden key-value pair or directory by add a `_` prefix.
-The hidden item will not be listed when sending a `GET` request for a directory.
-
-First we'll add a hidden key named `/_message`:
-
-```sh
-curl -L http://127.0.0.1:4001/v2/keys/_message -XPUT -d value="Hello hidden world"
-```
-
-```json
-{
-    "action": "set",
-    "node": {
-        "createdIndex": 3,
-        "key": "/_message",
-        "modifiedIndex": 3,
-        "value": "Hello hidden world"
-    }
-}
-```
-
-Next we'll add a regular key named `/message`:
-
-```sh
-curl -L http://127.0.0.1:4001/v2/keys/message -XPUT -d value="Hello world"
-```
-
-```json
-{
-    "action": "set",
-    "node": {
-        "createdIndex": 4,
-        "key": "/message",
-        "modifiedIndex": 4,
-        "value": "Hello world"
-    }
-}
-```
-
-Now let's try to get a listing of keys under the root directory, `/`:
-
-```sh
-curl -L http://127.0.0.1:4001/v2/keys/
-```
-
-```json
-{
-    "action": "get",
-    "node": {
-        "dir": true,
-        "key": "/",
-        "nodes": [
-            {
-                "createdIndex": 2,
-                "dir": true,
-                "key": "/foo_dir",
-                "modifiedIndex": 2
-            },
-            {
-                "createdIndex": 4,
-                "key": "/message",
-                "modifiedIndex": 4,
-                "value": "Hello world"
-            }
-        ]
-    }
-}
-```
-
-Here we see the `/message` key but our hidden `/_message` key is not returned.
-
-### Setting a key from a file
-
-You can also use etcd to store small configuration files, json documents, XML documents, etc directly.
-For example you can use curl to upload a simple text file and encode it:
-
-```
-echo "Hello\nWorld" > afile.txt
-curl -L http://127.0.0.1:4001/v2/keys/afile -XPUT --data-urlencode value@afile.txt
-```
-
-```json
-{
-    "action": "get",
-    "node": {
-        "createdIndex": 2,
-        "key": "/afile",
-        "modifiedIndex": 2,
-        "value": "Hello\nWorld\n"
-    }
-}
-```
-
-### Read Consistency
-
-#### Read from the Master
-
-Followers in a cluster can be behind the leader in their copy of the keyspace.
-If your application wants or needs the most up-to-date version of a key then it should ensure it reads from the current leader.
-By using the `consistent=true` flag in your GET requests, etcd will make sure you are talking to the current master.
-
-As an example of how a machine can be behind the leader let's start with a three machine cluster: L, F1, and F2.
-A client makes a write to L and F1 acknowledges the request.
-The client is told the write was successful and the keyspace is updated.
-Meanwhile F2 has partitioned from the network and will have an out-of-date version of the keyspace until the partition resolves.
-Since F2 missed the most recent write, a client reading from F2 will have an out-of-date version of the keyspace.
-
-Implementation notes on `consistent=true`: If the leader you are talking to is
-partitioned it will be unable to determine if it is not currently the master.
-In a later version we will provide a mechanism to set an upperbound of time
-that the current master can be unable to contact the quorom and still serve
-reads.
-
-### Read Linearization
-
-If you want a read that is fully linearized you can use a `quorum=true` GET.
-The read will take a very similar path to a write and will have a similar
-speed. If you are unsure if you need this feature feel free to email etcd-dev
-for advice.
-
-## Lock Module (*Deprecated and Removed*)
-
-The lock module is used to serialize access to resources used by clients.
-Multiple clients can attempt to acquire a lock but only one can have it at a time.
-Once the lock is released, the next client waiting for the lock will receive it.
-
-**Warning:** This module is deprecated and removed at v0.4. See [Modules][modules] for more details.
-
-
-### Acquiring a Lock
-
-To acquire a lock, simply send a `POST` request to the lock module with the lock name and TTL:
-
-```sh
-curl -L http://127.0.0.1:4001/mod/v2/lock/mylock -XPOST -d ttl=20
-```
-
-You will receive the lock index when you acquire the lock:
-
-```
-2
-```
-
-If the TTL is not specified or is not a number then you'll receive the following error:
-
-```json
-{
-    "errorCode": 202,
-    "message": "The given TTL in POST form is not a number",
-    "cause": "Acquire",
-}
-```
-
-If you specify a timeout that is not a number then you'll receive the following error:
-
-```json
-{
-    "errorCode": 205,
-    "message": "The given timeout in POST form is not a number",
-    "cause": "Acquire",
-}
-```
-
-
-### Renewing a Lock
-
-To extend the TTL of an already acquired lock, simply repeat your original request but with a `PUT` and the lock index instead:
-
-```sh
-curl -L http://127.0.0.1:4001/mod/v2/lock/mylock -XPUT -d index=5 -d ttl=20
-```
-
-If the index or value is not specified then you'll receive the following error:
-
-```json
-{
-    "errorCode": 207,
-    "message": "Index or value is required",
-    "cause": "Renew",
-}
-```
-
-If the index or value does not exist then you'll receive the following error with a `404 Not Found` HTTP code:
-
-```json
-{
-    "errorCode": 100,
-    "message": "Key not found",
-    "index": 1
-}
-```
-
-If the TTL is not specified or is not a number then you'll receive the following error:
-
-```json
-{
-    "errorCode": 202,
-    "message": "The given TTL in POST form is not a number",
-    "cause": "Renew",
-}
-```
-
-
-### Releasing a Lock
-
-When the client is finished with the lock, simply send a `DELETE` request to release the lock:
-
-```sh
-curl -L http://127.0.0.1:4001/mod/v2/lock/mylock?index=5 -XDELETE
-```
-
-If the index or value is not specified then you'll receive the following error:
-
-```json
-{
-    "errorCode": 207,
-    "message": "Index or value is required",
-    "cause": "Release",
-}
-```
-
-If the index and value are both specified then you'll receive the following error:
-
-```json
-{
-    "errorCode": 208,
-    "message": "Index and value cannot both be specified",
-    "cause": "Release",
-}
-```
-
-If the index or value does not exist then you'll receive the following error with a `404 Not Found` HTTP code:
-
-```json
-{
-    "errorCode": 100,
-    "message": "Key not found",
-    "index": 1
-}
-```
-
-
-### Retrieving a Lock
-
-To determine the current value or index of a lock, send a `GET` request to the lock.
-You can specify a `field` of `index` or `value`.
-The default is `value`.
-
-```sh
-curl -L http://127.0.0.1:4001/mod/v2/lock/mylock?field=index
-```
-
-Will return the current index:
-
-```sh
-2
-```
-
-If you specify a field other than `index` or `value` then you'll receive the following error:
-
-```json
-{
-    "errorCode": 209,
-    "message": "Invalid field",
-    "cause": "Get",
-}
-```
-
-
-## Leader Module (*Deprecated*)
-
-The leader module wraps the lock module to provide a simple interface for electing a single leader in a cluster.
-
-**Warning:** This module is deprecated at v0.4. See [Modules][modules] for more details.
-[modules]: https://github.com/coreos/etcd/blob/master/Documentation/modules.md
-
-
-### Setting the Leader
-
-A client can attempt to become leader by sending a `PUT` request to the leader module with the name of the leader to elect:
-
-```sh
-curl -L http://127.0.0.1:4001/mod/v2/leader/myclustername -XPUT -d ttl=300 -d name=foo.mydomain.com
-```
-
-You will receive a successful `200` HTTP response code when the leader is elected.
-
-If the name is not specified then you'll receive the following error:
-
-```json
-{
-    "errorCode": 206,
-    "message": "Name is required in POST form",
-    "cause": "Set",
-}
-```
-
-You can also receive any errors specified by the Lock module.
-
-
-### Retrieving the Current Leader
-
-A client can check to determine if there is a current leader by sending a `GET` request to the leader module:
-
-```sh
-curl -L http://127.0.0.1:4001/mod/v2/leader/myclustername
-```
-
-You will receive the name of the current leader:
-
-```sh
-foo.mydomain.com
-```
-
-
-### Relinquishing Leadership
-
-A client can give up leadership by sending a `DELETE` request with the leader name:
-
-```sh
-curl -L http://127.0.0.1:4001/mod/v2/leader/myclustername?name=foo.mydomain.com -XDELETE
-```
-
-If the name is not specified then you'll receive the following error:
-
-```json
-{
-    "errorCode": 206,
-    "message": "Name is required in POST form",
-    "cause": "Set",
-}
-```
-
-
-## Statistics
-
-An etcd cluster keeps track of a number of statistics including latency, bandwidth and uptime.
-These statistics are used in the `/mod/dashboard` endpoint to generate tables and graphs about the cluster state.
-
-
-### Leader Statistics
-
-The leader has a view of the entire cluster and keeps track of two interesting statistics: latency to each peer in the cluster, and the number of failed and successful Raft RPC requests.
-You can grab these statistics from the `/v2/stats/leader` endpoint:
-
-```sh
-curl -L http://127.0.0.1:4001/v2/stats/leader
-```
-
-```json
-{
-    "followers": {
-        "etcd-node1": {
-            "counts": {
-                "fail": 1212,
-                "success": 4163176
-            },
-            "latency": {
-                "average": 2.7206299430775007,
-                "current": 1.486487,
-                "maximum": 2018.410279,
-                "minimum": 1.011763,
-                "standardDeviation": 6.246990702203536
-            }
-        },
-        "etcd-node3": {
-            "counts": {
-                "fail": 1378,
-                "success": 4164598
-            },
-            "latency": {
-                "average": 2.707100125761001,
-                "current": 1.666258,
-                "maximum": 1409.054765,
-                "minimum": 0.998415,
-                "standardDeviation": 5.910089773061448
-            }
-        }
-    },
-    "leader": "etcd-node2"
-}
-```
-
-
-### Self Statistics
-
-Each node keeps a number of internal statistics:
-
-- `leaderInfo.leader`: name of the current leader machine
-- `leaderInfo.uptime`: amount of time the leader has been leader
-- `name`: this machine's name
-- `recvAppendRequestCnt`: number of append requests this node has processed
-- `recvBandwidthRate`: number of bytes per second this node is receiving (follower only)
-- `recvPkgRate`: number of requests per second this node is receiving (follower only)
-- `sendAppendRequestCnt`: number of requests that this node has sent
-- `sendBandwidthRate`: number of bytes per second this node is receiving (leader only). This value is undefined on single machine clusters.
-- `sendPkgRate`: number of requests per second this node is receiving (leader only). This value is undefined on single machine clusters.
-- `state`: either leader or follower
-- `startTime`: the time when this node was started
-
-This is an example response from a follower machine:
-
-```sh
-curl -L http://127.0.0.1:4001/v2/stats/self
-```
-
-```json
-{
-    "leaderInfo": {
-        "leader": "machine1",
-        "uptime": "1m18.544996775s"
-    },
-    "name": "machine0",
-    "recvAppendRequestCnt": 5871307,
-    "recvBandwidthRate": 630.3121596542599,
-    "recvPkgRate": 19.272654323628185,
-    "sendAppendRequestCnt": 3175763,
-    "startTime": "2014-01-01T15:26:24.96569404Z",
-    "state": "follower"
-}
-```
-
-And this is an example response from a leader machine:
-
-```sh
-curl -L http://127.0.0.1:4001/v2/stats/self
-```
-
-```json
-{
-    "leaderInfo": {
-        "leader": "machine0",
-        "uptime": "24.648619798s"
-    },
-    "name": "machine0",
-    "recvAppendRequestCnt": 5901116,
-    "sendAppendRequestCnt": 3212344,
-    "sendBandwidthRate": 1254.3151237301615,
-    "sendPkgRate": 38.71342974475808,
-    "startTime": "2014-01-01T15:26:24.96569404Z",
-    "state": "leader"
-}
-```
-
-
-### Store Statistics
-
-The store statistics include information about the operations that this node has handled.
-
-Operations that modify the store's state like create, delete, set and update are seen by the entire cluster and the number will increase on all nodes.
-Operations like get and watch are node local and will only be seen on this node.
-
-```sh
-curl -L http://127.0.0.1:4001/v2/stats/store
-```
-
-```json
-{
-    "compareAndSwapFail": 0,
-    "compareAndSwapSuccess": 0,
-    "createFail": 0,
-    "createSuccess": 2,
-    "deleteFail": 0,
-    "deleteSuccess": 0,
-    "expireCount": 0,
-    "getsFail": 4,
-    "getsSuccess": 75,
-    "setsFail": 2,
-    "setsSuccess": 4,
-    "updateFail": 0,
-    "updateSuccess": 0,
-    "watchers": 0
-}
-```
-
-## Cluster Config
-
-The configuration endpoint manages shared cluster wide properties.
-
-### Set Cluster Config
-
-```sh
-curl -L http://127.0.0.1:7001/v2/admin/config -XPUT -d '{"activeSize":3, "removeDelay":1800,"syncInterval":5}'
-```
-
-```json
-{
-    "activeSize": 3,
-    "removeDelay": 1800,
-    "syncInterval":5
-}
-```
-
-`activeSize` is the maximum number of peers that can join the cluster and participate in the consensus protocol.
-
-The size of cluster is controlled to be around a certain number. If it is not, standby-mode instances will join or peer-mode instances will be removed to make it happen.
-
-`removeDelay` indicates the minimum time that a machine has been observed to be unresponsive before it is removed from the cluster.
-
-### Get Cluster Config
-
-```sh
-curl -L http://127.0.0.1:7001/v2/admin/config
-```
-
-```json
-{
-    "activeSize": 3,
-    "removeDelay": 1800,
-    "syncInterval":5
-}
-```
-
-## Remove Machines
-
-At times you may want to manually remove a machine. Using the machines endpoint
-you can find and remove machines.
-
-First, list all the machines in the cluster.
-
-```sh
-curl -L http://127.0.0.1:7001/v2/admin/machines
-```
-```json
-[
-    {
-        "clientURL": "http://127.0.0.1:4001",
-        "name": "peer1",
-        "peerURL": "http://127.0.0.1:7001",
-        "state": "leader"
-    },
-    {
-        "clientURL": "http://127.0.0.1:4002",
-        "name": "peer2",
-        "peerURL": "http://127.0.0.1:7002",
-        "state": "follower"
-    },
-    {
-        "clientURL": "http://127.0.0.1:4003",
-        "name": "peer3",
-        "peerURL": "http://127.0.0.1:7003",
-        "state": "follower"
-    }
-]
-```
-
-Then take a closer look at the machine you want to remove.
-
-```sh
-curl -L http://127.0.0.1:7001/v2/admin/machines/peer2
-```
-
-```json
-{
-    "clientURL": "http://127.0.0.1:4002",
-    "name": "peer2",
-    "peerURL": "http://127.0.0.1:7002",
-    "state": "follower"
-}
-```
-
-And finally remove it.
-
-```sh
-curl -L -XDELETE http://127.0.0.1:7001/v2/admin/machines/peer2
-```
diff --git a/doc/clients-matrix.md b/doc/clients-matrix.md
deleted file mode 100644
index fa6659a..0000000
--- a/doc/clients-matrix.md
+++ /dev/null
@@ -1,42 +0,0 @@
-# Client libraries support matrix for etcd
-
-As etcd features support is really uneven between client libraries, a compatibility matrix can be important.
-
-## v2 clients
-
-The v2 API has a lot of features, we will categorize them in a few categories:
-- **Language**: The language in which the client library was written.
-- **HTTPS Auth**: Support for SSL-certificate based authentication
-- **Reconnect**: If the client is able to reconnect automatically to another server if one fails.
-- **Mod/Lock**: Support for the locking module
-- **Mod/Leader**: Support for the leader election module
-- **GET,PUT,POST,DEL Features**: Support for all the modifiers when calling the etcd server with said HTTP method.
-
-### Supported features matrix
-
-**Legend**
-**F**: Full support **G**: Good support **B**: Basic support
-**Y**: Feature supported  **-**: Feature not supported
-
-Sorted alphabetically on language/name
-
-|Client |**Language**|**HTTPS Auth**|**Re-connect**|**GET**|**PUT**|**POST**|**DEL**|**Mod Lock**|**Mod Leader**|
-| --- | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | 
-|[etcd-api](https://github.com/jdarcy/etcd-api)                   |C      |-|Y|B|G|-|B|-|-|
-|[etcdcpp](https://github.com/edwardcapriolo/etcdcpp)             |C++    |-|-|F|F|G|-|-|-|
-|[cetcd](https://github.com/dwwoelfel/cetcd)                      |Clojure|-|-|F|F|-|G|-|-|
-|[clj-etcd](https://github.com/rthomas/clj-etcd)                  |Clojure|-|-|G|G|-|B|-|-|
-|[etcd-clojure](https://github.com/aterreno/etcd-clojure)         |Clojure|-|-|F|F|F|F|-|-|
-|[go-etcd](https://github.com/coreos/go-etcd)                     |go     |Y|Y|F|F|F|F|-|-|
-|[etcd4j](https://github.com/jurmous/etcd4j)                      |java   |Y|Y|F|F|F|F|-|-|
-|[jetcd](https://github.com/diwakergupta/jetcd)                   |java   |Y|-|B|B|-|B|-|-|
-|[jetcd](https://github.com/justinsb/jetcd)                       |java   |-|-|B|B|-|B|-|-|
-|[Etcd.jl](https://github.com/forio/Etcd.jl)                      |Julia  |-|-|F|F|F|F|Y|Y|
-|[etcetera](https://github.com/drusellers/etcetera)               |.net   |-|-|F|F|F|F|-|-|
-|[node-etcd](https://github.com/stianeikeland/node-etcd)          |nodejs |Y|-|F|F|-|F|-|-|
-|[nodejs-etcd](https://github.com/lavagetto/nodejs-etcd)          |nodejs |Y|-|F|F|F|F|-|-|
-|[p5-etcd](https://metacpan.org/release/Etcd)                     |perl   |-|-|F|F|F|F|-|-|
-|[python-etcd](https://github.com/jplana/python-etcd)             |python |Y|Y|F|F|F|F|Y|-|
-|[python-etcd-client](https://github.com/dsoprea/PythonEtcdClient)|python |Y|Y|F|F|F|F|Y|Y|
-|[txetcd](https://github.com/russellhaering/txetcd)               |python |-|-|G|G|F|G|-|-|
-|[etcd-ruby](https://github.com/ranjib/etcd-ruby)                 |ruby   |-|-|F|F|F|F|-|-|
diff --git a/doc/cluster-discovery.md b/doc/cluster-discovery.md
deleted file mode 100644
index 8f53c9d..0000000
--- a/doc/cluster-discovery.md
+++ /dev/null
@@ -1,60 +0,0 @@
-# Cluster Discovery
-
-## Overview
-
-Starting an etcd cluster requires that each node knows another in the cluster. If you are trying to bring up a cluster all at once, say using a cloud formation, you also need to coordinate who will be the initial cluster leader. The discovery protocol helps you by providing an automated way to discover other existing peers in a cluster.
-
-For more information on how etcd can locate the cluster, see the [finding the cluster][cluster-finding] documentation.
-
-Please note - at least 3 nodes are required for [cluster availability][optimal-cluster-size].
-
-[cluster-finding]: https://github.com/coreos/etcd/blob/master/Documentation/design/cluster-finding.md
-[optimal-cluster-size]: https://github.com/coreos/etcd/blob/master/Documentation/optimal-cluster-size.md
-
-## Using discovery.etcd.io
-
-### Create a Discovery URL
-
-To use the discovery API, you must first create a unique discovery URL for your etcd cluster. Visit [https://discovery.etcd.io/new](https://discovery.etcd.io/new) to create a new discovery URL.
-
-You can inspect the list of peers by viewing `https://discovery.etcd.io/<cluster id>`.
-
-### Start etcd With the Discovery Flag
-
-Specify the `-discovery` flag when you start each etcd instance. The list of existing peers in the cluster will be downloaded and configured. If the instance is the first peer, it will start as the leader of the cluster.
-
-Here's a full example:
-
-```
-URL=$(curl https://discovery.etcd.io/new)
-./etcd -name instance1 -peer-addr 10.1.2.3:7001 -addr 10.1.2.3:4001 -discovery $URL
-./etcd -name instance2 -peer-addr 10.1.2.4:7001 -addr 10.1.2.4:4001 -discovery $URL
-./etcd -name instance3 -peer-addr 10.1.2.5:7001 -addr 10.1.2.5:4001 -discovery $URL
-```
-
-## Running Your Own Discovery Endpoint
-
-The discovery API communicates with a separate etcd cluster to store and retrieve the list of peers. CoreOS provides [https://discovery.etcd.io](https://discovery.etcd.io) as a free service, but you can easily run your own etcd cluster for this purpose. Here's an example using an etcd cluster located at `10.10.10.10:4001`:
-
-```
-URL="http://10.10.10.10:4001/v2/keys/testcluster"
-./etcd -name instance1 -peer-addr 10.1.2.3:7001 -addr 10.1.2.3:4001 -discovery $URL
-./etcd -name instance2 -peer-addr 10.1.2.4:7001 -addr 10.1.2.4:4001 -discovery $URL
-./etcd -name instance3 -peer-addr 10.1.2.5:7001 -addr 10.1.2.5:4001 -discovery $URL
-```
-
-If you're interested in how to discovery API works behind the scenes, read about the [Discovery Protocol](https://github.com/coreos/etcd/blob/master/Documentation/discovery-protocol.md).
-
-## Setting Peer Addresses Correctly
-
-The Discovery API submits the `-peer-addr` of each etcd instance to the configured Discovery endpoint. It's important to select an address that *all* peers in the cluster can communicate with. For example, if you're located in two regions of a cloud provider, configuring a private `10.x` address will not work between the two regions, and communication will not be possible between all peers.
-
-## Stale Peers
-
-The discovery API will automatically clean up the address of a stale peer that is no longer part of the cluster. The TTL for this process is a week, which should be long enough to handle any extremely long outage you may encounter. There is no harm in having stale peers in the list until they are cleaned up, since an etcd instance only needs to connect to one valid peer in the cluster to join.
-
-## Lifetime of a Discovery URL
-
-A discovery URL identifies a single etcd cluster. Do not re-use discovery URLs for new clusters.
-
-When a machine starts with a new discovery URL the discovery URL will be activated and record the machine's metadata. If you destroy the whole cluster and attempt to bring the cluster back up with the same discovery URL it will fail. This is intentional because all of the registered machines are gone including their logs so there is nothing to recover the killed cluster.
diff --git a/doc/clustering.md b/doc/clustering.md
deleted file mode 100644
index 5487bc2..0000000
--- a/doc/clustering.md
+++ /dev/null
@@ -1,169 +0,0 @@
-## Clustering
-
-### Example cluster of three machines
-
-Let's explore the use of etcd clustering.
-We use Raft as the underlying distributed protocol which provides consistency and persistence of the data across all of the etcd instances.
-
-Let start by creating 3 new etcd instances.
-
-We use `-peer-addr` to specify server port and `-addr` to specify client port and `-data-dir` to specify the directory to store the log and info of the machine in the cluster:
-
-```sh
-./etcd -peer-addr 127.0.0.1:7001 -addr 127.0.0.1:4001 -data-dir machines/machine1 -name machine1
-```
-
-**Note:** If you want to run etcd on an external IP address and still have access locally, you'll need to add `-bind-addr 0.0.0.0` so that it will listen on both external and localhost addresses.
-A similar argument `-peer-bind-addr` is used to setup the listening address for the server port.
-
-Let's join two more machines to this cluster using the `-peers` argument. A single connection to any peer will allow a new machine to join, but multiple can be specified for greater resiliency.
-
-```sh
-./etcd -peer-addr 127.0.0.1:7002 -addr 127.0.0.1:4002 -peers 127.0.0.1:7001,127.0.0.1:7003 -data-dir machines/machine2 -name machine2
-./etcd -peer-addr 127.0.0.1:7003 -addr 127.0.0.1:4003 -peers 127.0.0.1:7001,127.0.0.1:7002 -data-dir machines/machine3 -name machine3
-```
-
-We can retrieve a list of machines in the cluster using the HTTP API:
-
-```sh
-curl -L http://127.0.0.1:4001/v2/machines
-```
-
-We should see there are three machines in the cluster
-
-```
-http://127.0.0.1:4001, http://127.0.0.1:4002, http://127.0.0.1:4003
-```
-
-The machine list is also available via the main key API:
-
-```sh
-curl -L http://127.0.0.1:4001/v2/keys/_etcd/machines
-```
-
-```json
-{
-    "action": "get",
-    "node": {
-        "createdIndex": 1,
-        "dir": true,
-        "key": "/_etcd/machines",
-        "modifiedIndex": 1,
-        "nodes": [
-            {
-                "createdIndex": 1,
-                "key": "/_etcd/machines/machine1",
-                "modifiedIndex": 1,
-                "value": "raft=http://127.0.0.1:7001&etcd=http://127.0.0.1:4001"
-            },
-            {
-                "createdIndex": 2,
-                "key": "/_etcd/machines/machine2",
-                "modifiedIndex": 2,
-                "value": "raft=http://127.0.0.1:7002&etcd=http://127.0.0.1:4002"
-            },
-            {
-                "createdIndex": 3,
-                "key": "/_etcd/machines/machine3",
-                "modifiedIndex": 3,
-                "value": "raft=http://127.0.0.1:7003&etcd=http://127.0.0.1:4003"
-            }
-        ]
-    }
-}
-```
-
-We can also get the current leader in the cluster:
-
-```
-curl -L http://127.0.0.1:4001/v2/leader
-```
-
-The first server we set up should still be the leader unless it has died during these commands.
-
-```
-http://127.0.0.1:7001
-```
-
-Now we can do normal SET and GET operations on keys as we explored earlier.
-
-```sh
-curl -L http://127.0.0.1:4001/v2/keys/foo -XPUT -d value=bar
-```
-
-```json
-{
-    "action": "set",
-    "node": {
-        "createdIndex": 4,
-        "key": "/foo",
-        "modifiedIndex": 4,
-        "value": "bar"
-    }
-}
-```
-
-### Rejoining to the Cluster
-
-If one machine disconnects from the cluster, it could rejoin the cluster automatically when the communication is recovered.
-
-If one machine is killed, it could rejoin the cluster when started with old name. If the peer address is changed, etcd will treat the new peer address as the refreshed one, which benefits instance migration, or virtual machine boot with different IP. The peer-address-changing functionality is only supported when the majority of the cluster is alive, because this behavior needs the consensus of the etcd cluster.
-
-**Note:** For now, it is user responsibility to ensure that the machine doesn't join the cluster that has the member with the same name. Or unexpected error will happen. It would be improved sooner or later.
-
-### Killing Nodes in the Cluster
-
-Now if we kill the leader of the cluster, we can get the value from one of the other two machines:
-
-```sh
-curl -L http://127.0.0.1:4002/v2/keys/foo
-```
-
-We can also see that a new leader has been elected:
-
-```
-curl -L http://127.0.0.1:4002/v2/leader
-```
-
-```
-http://127.0.0.1:7002
-```
-
-or
-
-```
-http://127.0.0.1:7003
-```
-
-
-### Testing Persistence
-
-Next we'll kill all the machines to test persistence.
-Type `CTRL-C` on each terminal and then rerun the same command you used to start each machine.
-
-Your request for the `foo` key will return the correct value:
-
-```sh
-curl -L http://127.0.0.1:4002/v2/keys/foo
-```
-
-```json
-{
-    "action": "get",
-    "node": {
-        "createdIndex": 4,
-        "key": "/foo",
-        "modifiedIndex": 4,
-        "value": "bar"
-    }
-}
-```
-
-
-### Using HTTPS between servers
-
-In the previous example we showed how to use SSL client certs for client-to-server communication.
-Etcd can also do internal server-to-server communication using SSL client certs.
-To do this just change the `-*-file` flags to `-peer-*-file`.
-
-If you are using SSL for server-to-server communication, you must use it on all instances of etcd.
diff --git a/doc/configuration.md b/doc/configuration.md
deleted file mode 100644
index e90f6e9..0000000
--- a/doc/configuration.md
+++ /dev/null
@@ -1,135 +0,0 @@
-# Etcd Configuration
-
-## Node Configuration
-
-Individual node configuration options can be set in three places:
-
- 1. Command line flags
- 2. Environment variables
- 3. Configuration file
-
-Options set on the command line take precedence over all other sources.
-Options set in environment variables take precedence over options set in
-configuration files.
-
-## Cluster Configuration
-
-Cluster-wide settings are configured via the `/config` admin endpoint and additionally in the configuration file. Values contained in the configuration file will seed the cluster setting with the provided value. After the cluster is running, only the admin endpoint is used.
-
-The full documentation is contained in the [API docs](https://github.com/coreos/etcd/blob/master/Documentation/api.md#cluster-config).
-
-* `activeSize` - the maximum number of peers that can participate in the consensus protocol. Other peers will join as standbys.
-* `removeDelay` - the minimum time in seconds that a machine has been observed to be unresponsive before it is removed from the cluster.
-* `syncInterval` - the amount of time in seconds between cluster sync when it runs in standby mode.
-
-## Command Line Flags
-
-### Required
-
-* `-name` - The node name. Defaults to a UUID.
-
-### Optional
-
-* `-addr` - The advertised public hostname:port for client communication. Defaults to `127.0.0.1:4001`.
-* `-discovery` - A URL to use for discovering the peer list. (i.e `"https://discovery.etcd.io/your-unique-key"`).
-* `-http-read-timeout` - The number of seconds before an HTTP read operation is timed out.
-* `-http-write-timeout` - The number of seconds before an HTTP write operation is timed out.
-* `-bind-addr` - The listening hostname for client communication. Defaults to advertised IP.
-* `-peers` - A comma separated list of peers in the cluster (i.e `"203.0.113.101:7001,203.0.113.102:7001"`).
-* `-peers-file` - The file path containing a comma separated list of peers in the cluster.
-* `-ca-file` - The path of the client CAFile. Enables client cert authentication when present.
-* `-cert-file` - The cert file of the client.
-* `-key-file` - The key file of the client.
-* `-config` - The path of the etcd configuration file. Defaults to `/etc/etcd/etcd.conf`.
-* `-cors` - A comma separated white list of origins for cross-origin resource sharing.
-* `-cpuprofile` - The path to a file to output CPU profile data. Enables CPU profiling when present.
-* `-data-dir` - The directory to store log and snapshot. Defaults to the current working directory.
-* `-max-result-buffer` - The max size of result buffer. Defaults to `1024`.
-* `-max-retry-attempts` - The max retry attempts when trying to join a cluster. Defaults to `3`.
-* `-peer-addr` - The advertised public hostname:port for server communication. Defaults to `127.0.0.1:7001`.
-* `-peer-bind-addr` - The listening hostname for server communication. Defaults to advertised IP.
-* `-peer-ca-file` - The path of the CAFile. Enables client/peer cert authentication when present.
-* `-peer-cert-file` - The cert file of the server.
-* `-peer-key-file` - The key file of the server.
-* `-peer-election-timeout` - The number of milliseconds to wait before the leader is declared unhealthy.
-* `-peer-heartbeat-interval` - The number of milliseconds in between heartbeat requests
-* `-snapshot=false` - Disable log snapshots. Defaults to `true`.
-* `-cluster-active-size` - The expected number of instances participating in the consensus protocol. Only applied if the etcd instance is the first peer in the cluster.
-* `-cluster-remove-delay` - The number of seconds before one node is removed from the cluster since it cannot be connected at all. Only applied if the etcd instance is the first peer in the cluster.
-* `-cluster-sync-interval` - The number of seconds between synchronization for standby-mode instance with the cluster. Only applied if the etcd instance is the first peer in the cluster.
-* `-v` - Enable verbose logging. Defaults to `false`.
-* `-vv` - Enable very verbose logging. Defaults to `false`.
-* `-version` - Print the version and exit.
-
-## Configuration File
-
-The etcd configuration file is written in [TOML](https://github.com/mojombo/toml)
-and read from `/etc/etcd/etcd.conf` by default.
-
-```TOML
-addr = "127.0.0.1:4001"
-bind_addr = "127.0.0.1:4001"
-ca_file = ""
-cert_file = ""
-cors = []
-cpu_profile_file = ""
-data_dir = "."
-discovery = "http://etcd.local:4001/v2/keys/_etcd/registry/examplecluster"
-http_read_timeout = 10
-http_write_timeout = 10
-key_file = ""
-peers = []
-peers_file = ""
-max_cluster_size = 9
-max_result_buffer = 1024
-max_retry_attempts = 3
-name = "default-name"
-snapshot = true
-verbose = false
-very_verbose = false
-
-[peer]
-addr = "127.0.0.1:7001"
-bind_addr = "127.0.0.1:7001"
-ca_file = ""
-cert_file = ""
-key_file = ""
-
-[cluster]
-active_size = 9
-remove_delay = 1800.0
-sync_interval = 5.0
-```
-
-## Environment Variables
-
- * `ETCD_ADDR`
- * `ETCD_BIND_ADDR`
- * `ETCD_CA_FILE`
- * `ETCD_CERT_FILE`
- * `ETCD_CORS_ORIGINS`
- * `ETCD_CONFIG`
- * `ETCD_CPU_PROFILE_FILE`
- * `ETCD_DATA_DIR`
- * `ETCD_DISCOVERY`
- * `ETCD_CLUSTER_HTTP_READ_TIMEOUT`
- * `ETCD_CLUSTER_HTTP_WRITE_TIMEOUT`
- * `ETCD_KEY_FILE`
- * `ETCD_PEERS`
- * `ETCD_PEERS_FILE`
- * `ETCD_MAX_CLUSTER_SIZE`
- * `ETCD_MAX_RESULT_BUFFER`
- * `ETCD_MAX_RETRY_ATTEMPTS`
- * `ETCD_NAME`
- * `ETCD_SNAPSHOT`
- * `ETCD_VERBOSE`
- * `ETCD_VERY_VERBOSE`
- * `ETCD_PEER_ADDR`
- * `ETCD_PEER_BIND_ADDR`
- * `ETCD_PEER_CA_FILE`
- * `ETCD_PEER_CERT_FILE`
- * `ETCD_PEER_KEY_FILE`
- * `ETCD_PEER_ELECTION_TIMEOUT`
- * `ETCD_CLUSTER_ACTIVE_SIZE`
- * `ETCD_CLUSTER_REMOVE_DELAY`
- * `ETCD_CLUSTER_SYNC_INTERVAL`
diff --git a/doc/debugging.md b/doc/debugging.md
deleted file mode 100644
index 1e266b1..0000000
--- a/doc/debugging.md
+++ /dev/null
@@ -1,69 +0,0 @@
-# Debugging etcd
-
-Diagnosing issues in a distributed application is hard.
-etcd will help as much as it can - just enable these debug features using the CLI flag `-trace=*` or the config option `trace=*`.
-
-## Logging
-
-Log verbosity can be increased to the max using either the `-vvv` CLI flag or the `very_very_verbose=true` config option.
-
-The only supported logging mode is to stdout.
-
-## Metrics
-
-etcd itself can generate a set of metrics.
-These metrics represent many different internal data points that can be helpful when debugging etcd servers.
-
-#### Metrics reference
-
-Each individual metric name is prefixed with `etcd.<NAME>`, where \<NAME\> is the configured name of the etcd server.
-
-* `timer.appendentries.handle`: amount of time a peer takes to process an AppendEntriesRequest from the POV of the peer itself
-* `timer.peer.<PEER>.heartbeat`: amount of time a peer heartbeat operation takes from the POV of the leader that initiated that operation for peer \<PEER\>
-* `timer.command.<COMMAND>`: amount of time a given command took to be processed through the local server's raft state machine. This does not include time waiting on locks.
-
-#### Fetching metrics over HTTP
-
-Once tracing has been enabled on a given etcd server, all metric data is available at the server's `/debug/metrics` HTTP endpoint (i.e. `http://127.0.0.1:4001/debug/metrics`).
-Executing a GET HTTP command against the metrics endpoint will yield the current state of all metrics in the etcd server.
-
-#### Sending metrics to Graphite
-
-etcd supports [Graphite's Carbon plaintext protocol](https://graphite.readthedocs.org/en/latest/feeding-carbon.html#the-plaintext-protocol) - a TCP wire protocol designed for shipping metric data to an aggregator.
-To send metrics to a Graphite endpoint using this protocol, use of the `-graphite-host` CLI flag or the `graphite_host` config option (i.e. `graphite_host=172.17.0.19:2003`).
-
-See an [example graphite deploy script](https://github.com/coreos/etcd/contrib/graphite).
-
-#### Generating additional metrics with Collectd
-
-[Collectd](http://collectd.org/documentation.shtml) gathers metrics from the host running etcd.
-While these aren't metrics generated by etcd itself, it can be invaluable to compare etcd's view of the world to that of a separate process running next to etcd.
-
-See an [example collectd deploy script](https://github.com/coreos/etcd/contrib/collectd).
-
-## Profiling
-
-etcd exposes profiling information from the Go pprof package over HTTP.
-The basic browsable interface is served by etcd at the `/debug/pprof` HTTP endpoint (i.e. `http://127.0.0.1:4001/debug/pprof`).
-For more information on using profiling tools, see http://blog.golang.org/profiling-go-programs.
-
-**NOTE**: In the following examples you need to ensure that the `./bin/etcd` is identical to the `./bin/etcd` that you are targeting (same git hash, arch, platform, etc).
-
-#### Heap memory profile
-
-```
-go tool pprof ./bin/etcd http://127.0.0.1:4001/debug/pprof/heap
-```
-
-#### CPU profile
-
-```
-go tool pprof ./bin/etcd http://127.0.0.1:4001/debug/pprof/profile
-```
-
-#### Blocked goroutine profile
-
-```
-go tool pprof ./bin/etcd http://127.0.0.1:4001/debug/pprof/block
-```
-
diff --git a/doc/design/cluster-finding.md b/doc/design/cluster-finding.md
deleted file mode 100644
index 9cc3baf..0000000
--- a/doc/design/cluster-finding.md
+++ /dev/null
@@ -1,34 +0,0 @@
-## Cluster Finding Process
-
-Peer discovery uses the following sources in this order: log data in `-data-dir`, `-discovery` and `-peers`.
-
-If log data is provided, etcd will concatenate possible peers from three sources: the log data, the `-discovery` option, and `-peers`. Then it tries to join cluster through them one by one. If all connection attempts fail (which indicates that the majority of the cluster is currently down), it will restart itself based on the log data, which helps the cluster to recover from a full outage.
-
-Without log data, the instance is assumed to be a brand new one. If possible targets are provided by `-discovery` and `-peers`, etcd will make a best effort attempt to join them, and if none is reachable it will exit. Otherwise, if no `-discovery` or `-peers` option is provided, a new cluster will always be started.
-
-This ensures that users can always restart the node safely with the same command (without --force), and etcd will either reconnect to the old cluster if it is still running or recover its cluster from a outage.
-
-## Logical Workflow
-
-Start an etcd machine:
-
-```
-If log data is given:
-	Try to join via peers in previous cluster
-	Try to join via peers found in discover URL
-	Try to join via peers in peer list
-	Restart the previous cluster which is down
-	return
-
-If discover URL is given:
-	Fetch peers through discover URL
-	If Success:
-		Join via peers found
-		return
-
-If peer list is given:
-	Join as follower via peers in peer list
-	return
-
-Start as the leader of a new cluster
-```
diff --git a/doc/design/standbys.md b/doc/design/standbys.md
deleted file mode 100644
index 1a37c2f..0000000
--- a/doc/design/standbys.md
+++ /dev/null
@@ -1,232 +0,0 @@
-## Standbys
-
-Adding peers in an etcd cluster adds network, CPU, and disk overhead to the leader since each one requires replication.
-Peers primarily provide resiliency in the event of a leader failure but the benefit of more failover nodes decreases as the cluster size increases.
-A lightweight alternative is the standby.
-
-Standbys are a way for an etcd node to forward requests along to the cluster but the standbys are not part of the Raft cluster themselves.
-This provides an easier API for local applications while reducing the overhead required by a regular peer node.
-Standbys also act as standby nodes in the event that a peer node in the cluster has not recovered after a long duration.
-
-
-## Configuration Parameters
-
-There are three configuration parameters used by standbys: active size, remove delay and standby sync interval.
-
-The active size specifies a target size for the number of peers in the cluster.
-If there are not enough peers to meet the active size, standbys will send join requests until the peer count is equal to the active size.
-If there are more peers than the target active size then peers are removed by the leader and will become standbys.
-
-The remove delay specifies how long the cluster should wait before removing a dead peer.
-By default this is 30 minutes.
-If a peer is inactive for 30 minutes then the peer is removed.
-
-The standby sync interval specifies the synchronization interval of standbys with the cluster.
-By default this is 5 seconds.
-After each interval, standbys synchronize information with cluster.
-
-
-## Logical Workflow
-
-### Start a etcd machine
-
-#### Main logic
-
-```
-If find existing standby cluster info:
-  Goto standby loop
-
-Find cluster as required
-If determine to start peer server:
-  Goto peer loop
-Else:
-  Goto standby loop
-
-Peer loop:
-  Start peer mode
-  If running:
-    Wait for stop
-  Goto standby loop
-
-Standby loop:
-  Start standby mode
-  If running:
-    Wait for stop
-  Goto peer loop
-```
-
-
-#### [Cluster finding logic][cluster-finding.md]
-
-
-#### Join request logic:
-
-```
-Fetch machine info
-If cannot match version:
-  return false
-If active size <= peer count:
-  return false
-If it has existed in the cluster:
-  return true
-If join request fails:
-  return false
-return true
-```
-
-**Note**
-1. [TODO] The running mode cannot be determined by log, because the log may be outdated. But the log could be used to estimate its state.
-2. Even if sync cluster fails, it will restart still for recovery from full outage.
-
-
-#### Peer mode start logic
-
-```
-Start raft server
-Start other helper routines
-```
-
-
-#### Peer mode auto stop logic
-
-```
-When removed from the cluster:
-  Stop raft server
-  Stop other helper routines
-```
-
-
-#### Standby mode run logic
-
-```
-Loop:
-  Sleep for some time
-
-  Sync cluster, and write cluster info into disk
-
-  Check active size and send join request if needed
-  If succeed:
-    Clear cluster info from disk
-    Return
-```
-
-
-#### Serve Requests as Standby
-
-Return '404 Page Not Found' always on peer address. This is because peer address is used for raft communication and cluster management, which should not be used in standby mode.
-
-
-Serve requests from client:
-
-```
-Redirect all requests to client URL of leader
-```
-
-**Note**
-1. The leader here implies the one in raft cluster when doing the latest successful synchronization.
-2. [IDEA] We could extend HTTP Redirect to multiple possible targets.
-
-
-### Join Request Handling
-
-```
-If machine has existed in the cluster:
-  Return
-If peer count < active size:
-  Add peer
-  Increase peer count
-```
-
-
-### Remove Request Handling
-
-```
-If machine exists in the cluster:
-  Remove peer
-  Decrease peer count
-```
-
-
-## Cluster Monitor Logic
-
-### Active Size Monitor:
-
-This is only run by current cluster leader.
-
-```
-Loop:
-  Sleep for some time
-
-  If peer count > active size:
-    Remove randomly selected peer
-```
-
-
-### Peer Activity Monitor
-
-This is only run by current cluster leader.
-
-```
-Loop:
-  Sleep for some time
-
-  For each peer:
-    If peer last activity time > remove delay:
-      Remove the peer
-      Goto Loop
-```
-
-
-## Cluster Cases
-
-### Create Cluster with Thousands of Instances
-
-First few machines run in peer mode.
-
-All the others check the status of the cluster and run in standby mode.
-
-
-### Recover from full outage
-
-Machines with log data restart with join failure.
-
-Machines in peer mode recover heartbeat between each other.
-
-Machines in standby mode always sync the cluster. If sync fails, it uses the first address from data log as redirect target.
-
-
-### Kill one peer machine
-
-Leader of the cluster lose the connection with the peer.
-
-When the time exceeds remove delay, it removes the peer from the cluster.
-
-Machine in standby mode finds one available place of the cluster. It sends join request and joins the cluster.
-
-**Note**
-1. [TODO] Machine which was divided from majority and was removed from the cluster will distribute running of the cluster if the new node uses the same name.
-
-
-### Kill one standby machine
-
-No change for the cluster.
-
-
-## Cons
-
-1. New instance cannot join immediately after one peer is kicked out of the cluster, because the leader doesn't know the info about the standby instances.
-
-2. It may introduce join collision
-
-3. Cluster needs a good interval setting to balance the join delay and join collision.
-
-
-## Future Attack Plans
-
-1. Based on heartbeat miss and remove delay, standby could adjust its next check time.
-
-2. Preregister the promotion target when heartbeat miss happens.
-
-3. Get the estimated cluster size from the check happened in the sync interval, and adjust sync interval dynamically.
-
-4. Accept join requests based on active size and alive peers.
diff --git a/doc/development-tools.md b/doc/development-tools.md
deleted file mode 100644
index 0847398..0000000
--- a/doc/development-tools.md
+++ /dev/null
@@ -1,12 +0,0 @@
-# Development tools
-
-## Vagrant
-
-For fast start you can use Vagrant. `vagrant up` will make etcd build and running on virtual machine. Required Vagrant version is 1.5.0.
-
-Next lets set a single key and then retrieve it:
-
-```
-curl -L http://127.0.0.1:4001/v2/keys/mykey -XPUT -d value="this is awesome"
-curl -L http://127.0.0.1:4001/v2/keys/mykey
-```
diff --git a/doc/discovery-protocol.md b/doc/discovery-protocol.md
deleted file mode 100644
index 03be668..0000000
--- a/doc/discovery-protocol.md
+++ /dev/null
@@ -1,87 +0,0 @@
-# Discovery Protocol
-
-Starting a new etcd cluster can be painful since each machine needs to know of at least one live machine in the cluster. If you are trying to bring up a new cluster all at once, say using an AWS cloud formation, you also need to coordinate who will be the initial cluster leader. The discovery protocol uses an existing running etcd cluster to start a second etcd cluster.
-
-To use this feature you add the command line flag `-discovery` to your etcd args. In this example we will use `http://example.com/v2/keys/_etcd/registry` as the URL prefix.
-
-## The Protocol
-
-By convention the etcd discovery protocol uses the key prefix `_etcd/registry`. A full URL to the keyspace will be `http://example.com/v2/keys/_etcd/registry`.
-
-### Creating a New Cluster
-
-Generate a unique token that will identify the new cluster. This will be used as a key prefix in the following steps. An easy way to do this is to use uuidgen:
-
-```
-UUID=$(uuidgen)
-```
-
-### Bringing up Machines
-
-Now that you have your cluster ID you can start bringing up machines. Every machine will follow this protocol internally in etcd if given a `-discovery`.
-
-### Registering your Machine
-
-The first thing etcd must do is register your machine. This is done by using the machine name (from the `-name` arg) and posting it with a long TTL to the given key.
-
-```
-curl -X PUT "http://example.com/v2/keys/_etcd/registry/${UUID}/${etcd_machine_name}?ttl=604800" -d value=${peer_addr}
-```
-
-### Discovering Peers
-
-Now that this etcd machine is registered it must discover its peers.
-
-But, the tricky bit of starting a new cluster is that one machine needs to assume the initial role of leader and will have no peers. To figure out if another machine has already started the cluster etcd needs to create the `_state` key and set its value to "started":
-
-```
-curl -X PUT "http://example.com/v2/keys/_etcd/registry/${UUID}/_state?prevExist=false" -d value=started
-```
-
-If this returns a `200 OK` response then this machine is the initial leader and should start with no peers configured. If, however, this returns a `412 Precondition Failed` then you need to find all of the registered peers:
-
-```
-curl -X GET "http://example.com/v2/keys/_etcd/registry/${UUID}?recursive=true"
-```
-
-```
-{
-    "action": "get",
-    "node": {
-        "createdIndex": 11,
-        "dir": true,
-        "key": "/_etcd/registry/9D4258A5-A1D3-4074-8837-31C1E091131D",
-        "modifiedIndex": 11,
-        "nodes": [
-            {
-                "createdIndex": 16,
-                "expiration": "2014-02-03T13:19:57.631253589-08:00",
-                "key": "/_etcd/registry/9D4258A5-A1D3-4074-8837-31C1E091131D/peer1",
-                "modifiedIndex": 16,
-                "ttl": 604765,
-                "value": "127.0.0.1:7001"
-            },
-            {
-                "createdIndex": 17,
-                "expiration": "2014-02-03T13:19:57.631253589-08:00",
-                "key": "/_etcd/registry/9D4258A5-A1D3-4074-8837-31C1E091131D/peer2",
-                "modifiedIndex": 17,
-                "ttl": 604765,
-                "value": "127.0.0.1:7002"
-            }
-        ]
-    }
-}
-```
-
-Using this information you can connect to the rest of the peers in the cluster.
-
-### Heartbeating
-
-At this point etcd will start heart beating to your registration URL. The
-protocol uses a heartbeat so permanently deleted nodes get slowly removed from
-the discovery information cluster.
-
-The heartbeat interval is about once per day and the TTL is one week. This
-should give a sufficiently wide window to protect against a discovery service
-taking a temporary outage yet provide adequate cleanup.
diff --git a/doc/errorcode.md b/doc/errorcode.md
deleted file mode 100644
index b483973..0000000
--- a/doc/errorcode.md
+++ /dev/null
@@ -1,60 +0,0 @@
-Error Code
-======
-
-This document describes the error code in **Etcd** project.
-
-It's categorized into four groups:
-
-- Command Related Error
-- Post Form Related Error
-- Raft Related Error
-- Etcd Related Error
-
-Error code corresponding strerror
-------
-
-    const (
-        EcodeKeyNotFound    = 100
-        EcodeTestFailed     = 101
-        EcodeNotFile        = 102
-        EcodeNoMorePeer     = 103
-        EcodeNotDir         = 104
-        EcodeNodeExist      = 105
-        EcodeKeyIsPreserved = 106
-        EcodeRootROnly      = 107
-
-        EcodeValueRequired     = 200
-        EcodePrevValueRequired = 201
-        EcodeTTLNaN            = 202
-        EcodeIndexNaN          = 203
-
-        EcodeRaftInternal = 300
-        EcodeLeaderElect  = 301
-
-        EcodeWatcherCleared = 400
-        EcodeEventIndexCleared = 401
-    )
-
-    // command related errors
-    errors[100] = "Key Not Found"
-    errors[101] = "Test Failed" //test and set
-    errors[102] = "Not A File"
-    errors[103] = "Reached the max number of peers in the cluster"
-    errors[104] = "Not A Directory"
-    errors[105] = "Already exists" // create
-    errors[106] = "The prefix of given key is a keyword in etcd"
-    errors[107] = "Root is read only"
-
-    // Post form related errors
-    errors[200] = "Value is Required in POST form"
-    errors[201] = "PrevValue is Required in POST form"
-    errors[202] = "The given TTL in POST form is not a number"
-    errors[203] = "The given index in POST form is not a number"
-
-    // raft related errors
-    errors[300] = "Raft Internal Error"
-    errors[301] = "During Leader Election"
-
-    // etcd related errors
-    errors[400] = "watcher is cleared due to etcd recovery"
-    errors[401] = "The event in requested index is outdated and cleared"
diff --git a/doc/etcd-file-system.md b/doc/etcd-file-system.md
deleted file mode 100644
index f4ae09f..0000000
--- a/doc/etcd-file-system.md
+++ /dev/null
@@ -1,101 +0,0 @@
-#Etcd File System
-
-## Structure
-[TODO]
-![alt text](./img/etcd_fs_structure.jpg "etcd file system structure")
-
-## Node
-In **etcd**, the **node** is the base from which the filesystem is constructed.
-**etcd**'s file system is Unix-like with two kinds of nodes: file and directories.
-
-- A **file node** has data associated with it.
-- A **directory node** has child nodes associated with it.
-
-All nodes, regardless of type, have the following attributes and operations:
-
-### Attributes:
-- **Expiration Time** [optional]
-
-  The node will be deleted when it expires.
-
-- **ACL**
-
-  The path to the node's access control list.
-
-### Operation:
-- **Get** (path, recursive, sorted)
-
-  Get the content of the node
-    - If the node is a file, the data of the file will be returned.
-    - If the node is a directory, the child nodes of the directory will be returned.
-    - If recursive is true, it will recursively get the nodes of the directory.
-    - If sorted is true, the result will be sorted based on the path.
-
-- **Create** (path, value[optional], ttl [optional])
-
-  Create a file. Create operation will help to create intermediate directories with no expiration time.
-    - If the file already exists, create will fail.
-    - If the value is given, set will create a file.
-    - If the value is not given, set will crate a directory.
-    - If ttl is given, the node will be deleted when it expires.
-
-- **Update** (path, value[optional], ttl [optional])
-
-  Update the content of the node.
-    - If the value is given, the value of the key will be updated.
-    - If ttl is given, the expiration time of the node will be updated.
-
-- **Delete** (path, recursive)
-
-  Delete the node of given path.
-    - If the node is a directory:
-    - If recursive is true, the operation will delete all nodes under the directory.
-    - If recursive is false, error will be returned.
-
-- **TestAndSet** (path, prevValue [prevIndex], value, ttl)
-
-  Atomic *test and set* value to a file. If test succeeds, this operation will change the previous value of the file to the given value.
-    - If the prevValue is given, it will test against previous value of
-    the node.
-    - If the prevValue is empty, it will test if the node is not existing.
-    - If the prevValue is not empty, it will test if the prevValue is equal to the current value of the file.
-    - If the prevIndex is given, it will test if the create/last modified index of the node is equal to prevIndex.
-
-- **Renew** (path, ttl)
-
-  Set the node's expiration time to (current time + ttl)
-
-## ACL
-
-### Theory
-Etcd exports a Unix-like file system interface consisting of files and directories, collectively called nodes.
-Each node has various meta-data, including three names of the access control lists used to control reading, writing and changing (change ACL names for the node).
-
-We are storing the ACL names for nodes under a special *ACL* directory.
-Each node has ACL name corresponding to one file within *ACL* dir.
-Unless overridden, a node naturally inherits the ACL names of its parent directory on creation.
-
-For each ACL name, it has three children: *R (Reading)*, *W (Writing)*, *C (Changing)*
-
-Each permission is also a node. Under the node it contains the users who have this permission for the file referring to this ACL name.
-
-### Example
-[TODO]
-### Diagram
-[TODO]
-
-### Interface
-
-Testing permissions:
-
-- (node *Node) get_perm()
-- (node *Node) has_perm(perm string, user string)
-
-Setting/Changing permissions:
-
-- (node *Node) set_perm(perm string)
-- (node *Node) change_ACLname(aclname string)
-
-
-## User Group
-[TODO]
diff --git a/doc/img/etcd_fs_structure.jpg b/doc/img/etcd_fs_structure.jpg
deleted file mode 100644
index 51140ca..0000000
Binary files a/doc/img/etcd_fs_structure.jpg and /dev/null differ
diff --git a/doc/internal-protocol-versioning.md b/doc/internal-protocol-versioning.md
deleted file mode 100644
index 6df1fd4..0000000
--- a/doc/internal-protocol-versioning.md
+++ /dev/null
@@ -1,61 +0,0 @@
-# Versioning
-
-Goal: We want to be able to upgrade an individual peer in an etcd cluster to a newer version of etcd.
-The process will take the form of individual followers upgrading to the latest version until the entire cluster is on the new version.
-
-Immediate need: etcd is moving too fast to version the internal API right now.
-But, we need to keep mixed version clusters from being started by a rolling upgrade process (e.g. the CoreOS developer alpha).
-
-Longer term need: Having a mixed version cluster where all peers are not running the exact same version of etcd itself but are able to speak one version of the internal protocol.
-
-Solution: The internal protocol needs to be versioned just as the client protocol is.
-Initially during the 0.\*.\* series of etcd releases we won't allow mixed versions at all.
-
-## Join Control
-
-We will add a version field to the join command.
-But, who decides whether a newly upgraded follower should be able to join a cluster?
-
-### Leader Controlled
-
-If the leader controls the version of followers joining the cluster then it compares its version to the version number presented by the follower in the JoinCommand and rejects the join if the number is less than the leader's version number.
-
-Advantages
-
-- Leader controls all cluster decisions still
-
-Disadvantages
-
-- Follower knows better what versions of the internal protocol it can talk than the leader
-
-
-### Follower Controlled
-
-A newly upgraded follower should be able to figure out the leaders internal version from a defined internal backwards compatible API endpoint and figure out if it can join the cluster.
-If it cannot join the cluster then it simply exits.
-
-Advantages
-
-- The follower is running newer code and knows better if it can talk older protocols
-
-Disadvantages
-
-- This cluster decision isn't made by the leader
-
-## Recommendation
-
-To solve the immediate need and to plan for the future lets do the following:
-
-- Add Version field to JoinCommand
-- Have a joining follower read the Version field of the leader and if its own version doesn't match the leader then sleep for some random interval and retry later to see if the leader has upgraded.
-
-# Research
-
-## Zookeeper versioning
-
-Zookeeper very recently added versioning into the protocol and it doesn't seem to have seen any use yet.
-https://issues.apache.org/jira/browse/ZOOKEEPER-1633
-
-## doozerd
-
-doozerd stores the version number of the peers in the datastore for other clients to check, no decisions are made off of this number currently.
diff --git a/doc/libraries-and-tools.md b/doc/libraries-and-tools.md
deleted file mode 100644
index 23dd861..0000000
--- a/doc/libraries-and-tools.md
+++ /dev/null
@@ -1,99 +0,0 @@
-## Libraries and Tools
-
-**Tools**
-
-- [etcdctl](https://github.com/coreos/etcdctl) - A command line client for etcd
-- [etcd-dump](https://npmjs.org/package/etcd-dump) - Command line utility for dumping/restoring etcd.
-- [etcd-fs](https://github.com/xetorthio/etcd-fs) - FUSE filesystem for etcd
-- [etcd-browser](https://github.com/henszey/etcd-browser) - A web-based key/value editor for etcd using AngularJS
-
-**Go libraries**
-
-- [go-etcd](https://github.com/coreos/go-etcd) - Supports v2
-
-**Java libraries**
-
-- [justinsb/jetcd](https://github.com/justinsb/jetcd)
-- [diwakergupta/jetcd](https://github.com/diwakergupta/jetcd) - Supports v2
-- [jurmous/etcd4j](https://github.com/jurmous/etcd4j) - Supports v2
-
-**Python libraries**
-
-- [jplana/python-etcd](https://github.com/jplana/python-etcd) - Supports v2
-- [russellhaering/txetcd](https://github.com/russellhaering/txetcd) - a Twisted Python library
-- [cholcombe973/autodock](https://github.com/cholcombe973/autodock) - A docker deployment automation tool
-
-**Node libraries**
-
-- [stianeikeland/node-etcd](https://github.com/stianeikeland/node-etcd) - Supports v2 (w Coffeescript)
-- [lavagetto/nodejs-etcd](https://github.com/lavagetto/nodejs-etcd) - Supports v2
-
-**Ruby libraries**
-
-- [iconara/etcd-rb](https://github.com/iconara/etcd-rb)
-- [jpfuentes2/etcd-ruby](https://github.com/jpfuentes2/etcd-ruby)
-- [ranjib/etcd-ruby](https://github.com/ranjib/etcd-ruby) - Supports v2
-
-**C libraries**
-
-- [jdarcy/etcd-api](https://github.com/jdarcy/etcd-api) - Supports v2
-
-**C++ libraries**
-- [edwardcapriolo/etcdcpp](https://github.com/edwardcapriolo/etcdcpp) - Supports v2
-
-**Clojure libraries**
-
-- [aterreno/etcd-clojure](https://github.com/aterreno/etcd-clojure)
-- [dwwoelfel/cetcd](https://github.com/dwwoelfel/cetcd) - Supports v2
-- [rthomas/clj-etcd](https://github.com/rthomas/clj-etcd) - Supports v2
-
-**Erlang libraries**
-
-- [marshall-lee/etcd.erl](https://github.com/marshall-lee/etcd.erl)
-
-**.Net Libraries**
-
-- [drusellers/etcetera](https://github.com/drusellers/etcetera)
-
-**PHP Libraries**
-
-- [linkorb/etcd-php](https://github.com/linkorb/etcd-php)
-
-**Haskell libraries**
-
-- [wereHamster/etcd-hs](https://github.com/wereHamster/etcd-hs)
-
-A detailed recap of client functionalities can be found in the [clients compatibility matrix][clients-matrix.md].
-
-[clients-matrix.md]: https://github.com/coreos/etcd/blob/master/Documentation/clients-matrix.md
-
-**Chef Integration**
-
-- [coderanger/etcd-chef](https://github.com/coderanger/etcd-chef)
-
-**Chef Cookbook**
-
-- [spheromak/etcd-cookbook](https://github.com/spheromak/etcd-cookbook)
-
-**BOSH Releases**
-
-- [cloudfoundry-community/etcd-boshrelease](https://github.com/cloudfoundry-community/etcd-boshrelease)
-- [cloudfoundry/cf-release](https://github.com/cloudfoundry/cf-release/tree/master/jobs/etcd)
-
-**Projects using etcd**
-
-- [binocarlos/yoda](https://github.com/binocarlos/yoda) - etcd + ZeroMQ
-- [calavera/active-proxy](https://github.com/calavera/active-proxy) - HTTP Proxy configured with etcd
-- [derekchiang/etcdplus](https://github.com/derekchiang/etcdplus) - A set of distributed synchronization primitives built upon etcd
-- [go-discover](https://github.com/flynn/go-discover) - service discovery in Go
-- [gleicon/goreman](https://github.com/gleicon/goreman/tree/etcd) - Branch of the Go Foreman clone with etcd support
-- [garethr/hiera-etcd](https://github.com/garethr/hiera-etcd) - Puppet hiera backend using etcd
-- [mattn/etcd-vim](https://github.com/mattn/etcd-vim) - SET and GET keys from inside vim
-- [mattn/etcdenv](https://github.com/mattn/etcdenv) - "env" shebang with etcd integration
-- [kelseyhightower/confd](https://github.com/kelseyhightower/confd) - Manage local app config files using templates and data from etcd
-- [configdb](https://git.autistici.org/ai/configdb/tree/master) - A REST relational abstraction on top of arbitrary database backends, aimed at storing configs and inventories.
-- [scrz](https://github.com/scrz/scrz) - Container manager, stores configuration in etcd.
-- [fleet](https://github.com/coreos/fleet) - Distributed init system
-- [GoogleCloudPlatform/kubernetes](https://github.com/GoogleCloudPlatform/kubernetes) - Container cluster manager.
-- [mailgun/vulcand](https://github.com/mailgun/vulcand) - HTTP proxy that uses etcd as a configuration backend.
-- [duedil-ltd/discodns](https://github.com/duedil-ltd/discodns) - Simple DNS nameserver using etcd as a database for names and records.
diff --git a/doc/modules.md b/doc/modules.md
deleted file mode 100644
index 08944ce..0000000
--- a/doc/modules.md
+++ /dev/null
@@ -1,118 +0,0 @@
-## Modules
-
-etcd has a number of modules that are built on top of the core etcd API.
-These modules provide things like dashboards, locks and leader election (removed).
-
-**Warning**: Modules are deprecated from v0.4 until we have a solid base we can apply them back onto.
-For now, we are choosing to focus on raft algorithm and core etcd to make sure that it works correctly and fast.
-And it is time consuming to maintain these modules in this period, given that etcd's API changes from time to time.
-Moreover, the lock module has some unfixed bugs, which may mislead users.
-But we also notice that these modules are popular and useful, and plan to add them back with full functionality as soon as possible.
-
-### Dashboard
-
-An HTML dashboard can be found at `http://127.0.0.1:4001/mod/dashboard/`.
-This dashboard is compiled into the etcd binary and uses the same API as regular etcd clients.
-
-Use the `-cors='*'` flag to allow your browser to request information from the current master as it changes.
-
-### Lock
-
-The Lock module implements a fair lock that can be used when lots of clients want access to a single resource.
-A lock can be associated with a value.
-The value is unique so if a lock tries to request a value that is already queued for a lock then it will find it and watch until that value obtains the lock.
-You may supply a `timeout` which will cancel the lock request if it is not obtained within `timeout` seconds.  If `timeout` is not supplied, it is presumed to be infinite.  If `timeout` is `0`, the lock request will fail if it is not immediately acquired.
-If you lock the same value on a key from two separate curl sessions they'll both return at the same time.
-
-Here's the API:
-
-**Acquire a lock (with no value) for "customer1"**
-
-```sh
-curl -X POST http://127.0.0.1:4001/mod/v2/lock/customer1?ttl=60
-```
-
-**Acquire a lock for "customer1" that is associated with the value "bar"**
-
-```sh
-curl -X POST http://127.0.0.1:4001/mod/v2/lock/customer1?ttl=60 -d value=bar
-```
-
-**Acquire a lock for "customer1" that is associated with the value "bar" only if it is done within 2 seconds**
-
-```sh
-curl -X POST http://127.0.0.1:4001/mod/v2/lock/customer1?ttl=60 -d value=bar -d timeout=2
-```
-
-**Renew the TTL on the "customer1" lock for index 2**
-
-```sh
-curl -X PUT http://127.0.0.1:4001/mod/v2/lock/customer1?ttl=60 -d index=2
-```
-
-**Renew the TTL on the "customer1" lock for value "bar"**
-
-```sh
-curl -X PUT http://127.0.0.1:4001/mod/v2/lock/customer1?ttl=60 -d value=bar
-```
-
-**Retrieve the current value for the "customer1" lock.**
-
-```sh
-curl http://127.0.0.1:4001/mod/v2/lock/customer1
-```
-
-**Retrieve the current index for the "customer1" lock**
-
-```sh
-curl http://127.0.0.1:4001/mod/v2/lock/customer1?field=index
-```
-
-**Delete the "customer1" lock with the index 2**
-
-```sh
-curl -X DELETE http://127.0.0.1:4001/mod/v2/lock/customer1?index=2
-```
-
-**Delete the "customer1" lock with the value "bar"**
-
-```sh
-curl -X DELETE http://127.0.0.1:4001/mod/v2/lock/customer1?value=bar
-```
-
-
-### Leader Election (Deprecated and Removed in 0.4)
-
-The Leader Election module wraps the Lock module to allow clients to come to consensus on a single value.
-This is useful when you want one server to process at a time but allow other servers to fail over.
-The API is similar to the Lock module but is limited to simple strings values.
-
-Here's the API:
-
-**Attempt to set a value for the "order_processing" leader key:**
-
-```sh
-curl -X PUT http://127.0.0.1:4001/mod/v2/leader/order_processing?ttl=60 -d name=myserver1.foo.com
-```
-
-**Retrieve the current value for the "order_processing" leader key:**
-
-```sh
-curl http://127.0.0.1:4001/mod/v2/leader/order_processing
-myserver1.foo.com
-```
-
-**Remove a value from the "order_processing" leader key:**
-
-```sh
-curl -X DELETE http://127.0.0.1:4001/mod/v2/leader/order_processing?name=myserver1.foo.com
-```
-
-If multiple clients attempt to set the value for a key then only one will succeed.
-The other clients will hang until the current value is removed because of TTL or because of a `DELETE` operation.
-Multiple clients can submit the same value and will all be notified when that value succeeds.
-
-To update the TTL of a value simply reissue the same `PUT` command that you used to set the value.
-
-
-
diff --git a/doc/optimal-cluster-size.md b/doc/optimal-cluster-size.md
deleted file mode 100644
index 2aa7c95..0000000
--- a/doc/optimal-cluster-size.md
+++ /dev/null
@@ -1,38 +0,0 @@
-# Optimal etcd Cluster Size
-
-etcd's Raft consensus algorithm is most efficient in small clusters between 3 and 9 peers. For clusters larger than 9, etcd will select a subset of instances to participate in the algorithm in order to keep it efficient. The end of this document briefly explores how etcd works internally and why these choices have been made.
-
-## Cluster Management
-
-You can manage the active cluster size through the [cluster config API](https://github.com/coreos/etcd/blob/master/Documentation/api.md#cluster-config). `activeSize` represents the etcd peers allowed to actively participate in the consensus algorithm.
-
-If the total number of etcd instances exceeds this number, additional peers are started as [standbys](https://github.com/coreos/etcd/blob/master/Documentation/design/standbys.md), which can be promoted to active participation if one of the existing active instances has failed or been removed.
-
-## Internals of etcd
-
-### Writing to etcd
-
-Writes to an etcd peer are always redirected to the leader of the cluster and distributed to all of the peers immediately. A write is only considered successful when a majority of the peers acknowledge the write.
-
-For example, in a cluster with 5 peers, a write operation is only as fast as the 3rd fastest machine. This is the main reason for keeping the number of active peers below 9. In practice, you only need to worry about write performance in high latency environments such as a cluster spanning multiple data centers.
-
-### Leader Election
-
-The leader election process is similar to writing a key &mdash; a majority of the active peers must acknowledge the new leader before cluster operations can continue. The longer each peer takes to elect a new leader means you have to wait longer before you can write to the cluster again. In low latency environments this process takes milliseconds.
-
-### Odd Active Cluster Size
-
-The other important cluster optimization is to always have an odd active cluster size (i.e. `activeSize`). Adding an odd node to the number of peers doesn't change the size of the majority and therefore doesn't increase the total latency of the majority as described above. But, you gain a higher tolerance for peer failure by adding the extra machine. You can see this in practice when comparing two even and odd sized clusters:
-
-| Active Peers | Majority   | Failure Tolerance |
-|--------------|------------|-------------------|
-| 1 peers      | 1 peers    | None              |
-| 3 peers      | 2 peers    | 1 peer            |
-| 4 peers      | 3 peers    | 1 peer           |
-| 5 peers      | 3 peers    | **2 peers**       |
-| 6 peers      | 4 peers    | 2 peers           |
-| 7 peers      | 4 peers    | **3 peers**       |
-| 8 peers      | 5 peers    | 3 peers           |
-| 9 peers      | 5 peers    | **4 peers**       |
-
-As you can see, adding another peer to bring the number of active peers up to an odd size is always worth it. During a network partition, an odd number of active peers also guarantees that there will almost always be a majority of the cluster that can continue to operate and be the source of truth when the partition ends.
diff --git a/doc/platforms/freebsd.md b/doc/platforms/freebsd.md
deleted file mode 100644
index 13384db..0000000
--- a/doc/platforms/freebsd.md
+++ /dev/null
@@ -1,62 +0,0 @@
-# FreeBSD
-
-Starting with version 0.1.2 both etcd and etcdctl have been ported to FreeBSD and can
-be installed either via packages or ports system. Their versions have been recently
-updated to 0.2.0 so now you can enjoy using etcd and etcdctl on FreeBSD 10.0 (RC4 as
-of now) and 9.x where they have been tested. They might also work when installed from
-ports on earlier versions of FreeBSD, but your mileage may vary.
-
-## Installation
-
-### Using pkgng package system
-
-1. If you do not have pkg­ng installed, install it with command `pkg` and answering 'Y'
-when asked
-
-2. Update your repository data with `pkg update`
-
-3. Install etcd with `pkg install coreos­etcd coreos­etcdctl`
-
-4. Verify successful installation with `pkg info | grep etcd` and you should get:
-
-```
-r@fbsd­10:/ # pkg info | grep etcd
-coreos­etcd­0.2.0              Highly­available key value store and service discovery
-coreos­etcdctl­0.2.0           Simple commandline client for etcd
-r@fbsd­10:/ #
-```
-
-5. You’re ready to use etcd and etcdctl! For more information about using pkgng, please
-see: http://www.freebsd.org/doc/handbook/pkgng­intro.html
- 
-### Using ports system
-
-1. If you do not have ports installed, install with with `portsnap fetch extract` (it
-may take some time depending on your hardware and network connection)
-
-2. Build etcd with `cd /usr/ports/devel/etcd && make install clean`, you
-will get an option to build and install documentation and etcdctl with it.
-
-3. If you haven't installed it with etcdctl, and you would like to install it later, you can build it
-with `cd /usr/ports/devel/etcdctl && make install clean`
-
-4. Verify successful installation with `pkg info | grep etcd` and you should get:
- 
-
-```
-r@fbsd­10:/ # pkg info | grep etcd
-coreos­etcd­0.2.0              Highly­available key value store and service discovery
-coreos­etcdctl­0.2.0           Simple commandline client for etcd
-r@fbsd­10:/ #
-```
-
-5. You’re ready to use etcd and etcdctl! For more information about using ports system,
-please see: https://www.freebsd.org/doc/handbook/ports­using.html
-
-## Issues
-
-If you find any issues with the build/install procedure or you've found a problem that
-you've verified is local to FreeBSD version only (for example, by not being able to
-reproduce it on any other platform, like OSX or Linux), please sent a
-problem report using this page for more
-information: http://www.freebsd.org/send­pr.html
diff --git a/doc/production-ready.md b/doc/production-ready.md
deleted file mode 100644
index a76e08f..0000000
--- a/doc/production-ready.md
+++ /dev/null
@@ -1,7 +0,0 @@
-etcd is being used successfully by many companies in production. It is,
-however, under active development and systems like etcd are difficult to get
-correct. If you are comfortable with bleeding-edge software please use etcd and
-provide us with the feedback and testing young software needs.
-
-When the etcd team feels confident removing this warning we will release etcd
-1.0.
diff --git a/doc/security.md b/doc/security.md
deleted file mode 100644
index 30a881d..0000000
--- a/doc/security.md
+++ /dev/null
@@ -1,131 +0,0 @@
-# Reading and Writing over HTTPS
-
-## Transport Security with HTTPS
-
-Etcd supports SSL/TLS and client cert authentication for clients to server, as well as server to server communication.
-
-First, you need to have a CA cert `clientCA.crt` and signed key pair `client.crt`, `client.key`.
-This site has a good reference for how to generate self-signed key pairs:
-http://www.g-loaded.eu/2005/11/10/be-your-own-ca/
-Or you could use [etcd-ca](https://github.com/coreos/etcd-ca) to generate certs and keys.
-
-For testing you can use the certificates in the `fixtures/ca` directory.
-
-Let's configure etcd to use this keypair:
-
-```sh
-./etcd -f -name machine0 -data-dir machine0 -cert-file=./fixtures/ca/server.crt -key-file=./fixtures/ca/server.key.insecure
-```
-
-There are a few new options we're using:
-
-* `-f` - forces a new machine configuration, even if an existing configuration is found. (WARNING: data loss!)
-* `-cert-file` and `-key-file` specify the location of the cert and key files to be used for for transport layer security between the client and server.
-
-You can now test the configuration using HTTPS:
-
-```sh
-curl --cacert ./fixtures/ca/server-chain.pem https://127.0.0.1:4001/v2/keys/foo -XPUT -d value=bar -v
-```
-
-You should be able to see the handshake succeed.
-
-**OSX 10.9+ Users**: curl 7.30.0 on OSX 10.9+ doesn't understand certificates passed in on the command line.
-Instead you must import the dummy ca.crt directly into the keychain or add the `-k` flag to curl to ignore errors.
-If you want to test without the `-k` flag run `open ./fixtures/ca/ca.crt` and follow the prompts.
-Please remove this certificate after you are done testing!
-If you know of a workaround let us know.
-
-```
-...
-SSLv3, TLS handshake, Finished (20):
-...
-```
-
-And also the response from the etcd server:
-
-```json
-{
-    "action": "set",
-    "key": "/foo",
-    "modifiedIndex": 3,
-    "value": "bar"
-}
-```
-
-
-## Authentication with HTTPS Client Certificates
-
-We can also do authentication using CA certs.
-The clients will provide their cert to the server and the server will check whether the cert is signed by the CA and decide whether to serve the request.
-
-```sh
-./etcd -f -name machine0 -data-dir machine0 -ca-file=./fixtures/ca/ca.crt -cert-file=./fixtures/ca/server.crt -key-file=./fixtures/ca/server.key.insecure
-```
-
-```-ca-file``` is the path to the CA cert.
-
-Try the same request to this server:
-
-```sh
-curl --cacert ./fixtures/ca/server-chain.pem https://127.0.0.1:4001/v2/keys/foo -XPUT -d value=bar -v
-```
-
-The request should be rejected by the server.
-
-```
-...
-routines:SSL3_READ_BYTES:sslv3 alert bad certificate
-...
-```
-
-We need to give the CA signed cert to the server.
-
-```sh
-curl --key ./fixtures/ca/server2.key.insecure --cert ./fixtures/ca/server2.crt --cacert ./fixtures/ca/server-chain.pem -L https://127.0.0.1:4001/v2/keys/foo -XPUT -d value=bar -v
-```
-
-You should able to see:
-
-```
-...
-SSLv3, TLS handshake, CERT verify (15):
-...
-TLS handshake, Finished (20)
-```
-
-And also the response from the server:
-
-```json
-{
-    "action": "set",
-    "node": {
-        "createdIndex": 12,
-        "key": "/foo",
-        "modifiedIndex": 12,
-        "value": "bar"
-    }
-}
-```
-
-### Why SSLv3 alert handshake failure when using SSL client auth?
-
-The `crypto/tls` package of `golang` checks the key usage of the certificate public key before using it.
-To use the certificate public key to do client auth, we need to add `clientAuth` to `Extended Key Usage` when creating the certificate public key.
-
-Here is how to do it:
-
-Add the following section to your openssl.cnf:
-
-```
-[ ssl_client ]
-...
-  extendedKeyUsage = clientAuth
-...
-```
-
-When creating the cert be sure to reference it in the `-extensions` flag:
-
-```
-openssl ca -config openssl.cnf -policy policy_anything -extensions ssl_client -out certs/machine.crt -infiles machine.csr
-```
diff --git a/doc/tuning.md b/doc/tuning.md
deleted file mode 100644
index c9899a5..0000000
--- a/doc/tuning.md
+++ /dev/null
@@ -1,95 +0,0 @@
-## Tuning
-
-The default settings in etcd should work well for installations on a local network where the average network latency is low.
-However, when using etcd across multiple data centers or over networks with high latency you may need to tweak the heartbeat interval and election timeout settings.
-
-The network isn't the only source of latency. Each request and response may be impacted by slow disks on both the leader and follower. Each of these timeouts represents the total time from request to successful response from the other machine.
-
-### Time Parameters
-
-The underlying distributed consensus protocol relies on two separate time parameters to ensure that nodes can handoff leadership if one stalls or goes offline.
-The first parameter is called the *Heartbeat Interval*.
-This is the frequency with which the leader will notify followers that it is still the leader.
-etcd batches commands together for higher throughput so this heartbeat interval is also a delay for how long it takes for commands to be committed.
-By default, etcd uses a `50ms` heartbeat interval.
-
-The second parameter is the *Election Timeout*.
-This timeout is how long a follower node will go without hearing a heartbeat before attempting to become leader itself.
-By default, etcd uses a `200ms` election timeout.
-
-Adjusting these values is a trade off.
-Lowering the heartbeat interval will cause individual commands to be committed faster but it will lower the overall throughput of etcd.
-If your etcd instances have low utilization then lowering the heartbeat interval can improve your command response time.
-
-The election timeout should be set based on the heartbeat interval and your network ping time between nodes.
-Election timeouts should be at least 10 times your ping time so it can account for variance in your network.
-For example, if the ping time between your nodes is 10ms then you should have at least a 100ms election timeout.
-
-You should also set your election timeout to at least 4 to 5 times your heartbeat interval to account for variance in leader replication.
-For a heartbeat interval of 50ms you should set your election timeout to at least 200ms - 250ms.
-
-You can override the default values on the command line:
-
-```sh
-# Command line arguments:
-$ etcd -peer-heartbeat-interval=100 -peer-election-timeout=500
-
-# Environment variables:
-$ ETCD_PEER_HEARTBEAT_INTERVAL=100 ETCD_PEER_ELECTION_TIMEOUT=500 etcd
-```
-
-Or you can set the values within the configuration file:
-
-```toml
-[peer]
-heartbeat_interval = 100
-election_timeout = 500
-```
-
-The values are specified in milliseconds.
-
-
-### Snapshots
-
-etcd appends all key changes to a log file.
-This log grows forever and is a complete linear history of every change made to the keys.
-A complete history works well for lightly used clusters but clusters that are heavily used would carry around a large log.
-
-To avoid having a huge log etcd makes periodic snapshots.
-These snapshots provide a way for etcd to compact the log by saving the current state of the system and removing old logs.
-
-### Snapshot Tuning
-
-Creating snapshots can be expensive so they're only created after a given number of changes to etcd.
-By default, snapshots will be made after every 10,000 changes.
-If etcd's memory usage and disk usage are too high, you can lower the snapshot threshold by setting the following on the command line:
-
-```sh
-# Command line arguments:
-$ etcd -snapshot-count=5000
-
-# Environment variables:
-$ ETCD_SNAPSHOT_COUNT=5000 etcd
-```
-
-Or you can change the setting in the configuration file:
-
-```toml
-snapshot_count = 5000
-```
-
-You can also disable snapshotting by adding the following to your command line:
-
-```sh
-# Command line arguments:
-$ etcd -snapshot false
-
-# Environment variables:
-$ ETCD_SNAPSHOT=false etcd
-```
-
-You can also disable snapshotting within the configuration file:
-
-```toml
-snapshot = false
-```
diff --git a/doc/upgrade.md b/doc/upgrade.md
deleted file mode 100644
index b13f4f6..0000000
--- a/doc/upgrade.md
+++ /dev/null
@@ -1,17 +0,0 @@
-# Upgrading an Existing Cluster
-
-etcd clusters can be upgraded by doing a rolling upgrade or all at once. We make every effort to test this process, but please be sure to backup your data [by etcd-dump](https://github.com/AaronO/etcd-dump), or make a copy of data directory beforehand.
-
-## Upgrade Process
-
-- Stop the old etcd processes
-- Upgrade the etcd binary
-- Restart the etcd instance using the original --name, --address, --peer-address and --data-dir.
-
-## Rolling Upgrade
-
-During an upgrade, etcd clusters are designed to continue working in a mix of old and new versions. It's recommended to converge on the new version quickly. Using new API features before the entire cluster has been upgraded is only supported as a best effort. Each instance's version can be found with `curl http://127.0.0.1:4001/version`.
-
-## All at Once
-
-If downtime is not an issue, the easiest way to upgrade your cluster is to shutdown all of the etcd instances and restart them with the new binary. The current state of the cluster is saved to disk and will be loaded into the cluster when it restarts.
