commit 1b98074897aec3c41c18ee0652b3f5c8b9dcf0c2
Author: Xiang Li <xiangli.cs@gmail.com>
Date:   Sun Apr 24 18:25:04 2016 -0700

    docs: move clustering doc

diff --git a/Documentation/clustering.md b/Documentation/clustering.md
deleted file mode 100644
index aa12905..0000000
--- a/Documentation/clustering.md
+++ /dev/null
@@ -1,434 +0,0 @@
-# Clustering Guide
-
-## Overview
-
-Starting an etcd cluster statically requires that each member knows another in the cluster. In a number of cases, you might not know the IPs of your cluster members ahead of time. In these cases, you can bootstrap an etcd cluster with the help of a discovery service.
-
-Once an etcd cluster is up and running, adding or removing members is done via [runtime reconfiguration][runtime-conf]. To better understand the design behind runtime reconfiguration, we suggest you read [the runtime configuration design document][runtime-reconf-design].
-
-This guide will cover the following mechanisms for bootstrapping an etcd cluster:
-
-* [Static](#static)
-* [etcd Discovery](#etcd-discovery)
-* [DNS Discovery](#dns-discovery)
-
-Each of the bootstrapping mechanisms will be used to create a three machine etcd cluster with the following details:
-
-|Name|Address|Hostname|
-|------|---------|------------------|
-|infra0|10.0.1.10|infra0.example.com|
-|infra1|10.0.1.11|infra1.example.com|
-|infra2|10.0.1.12|infra2.example.com|
-
-## Static
-
-As we know the cluster members, their addresses and the size of the cluster before starting, we can use an offline bootstrap configuration by setting the `initial-cluster` flag. Each machine will get either the following command line or environment variables:
-
-```
-ETCD_INITIAL_CLUSTER="infra0=http://10.0.1.10:2380,infra1=http://10.0.1.11:2380,infra2=http://10.0.1.12:2380"
-ETCD_INITIAL_CLUSTER_STATE=new
-```
-
-```
---initial-cluster infra0=http://10.0.1.10:2380,infra1=http://10.0.1.11:2380,infra2=http://10.0.1.12:2380 \
---initial-cluster-state new
-```
-
-Note that the URLs specified in `initial-cluster` are the _advertised peer URLs_, i.e. they should match the value of `initial-advertise-peer-urls` on the respective nodes.
-
-If you are spinning up multiple clusters (or creating and destroying a single cluster) with same configuration for testing purpose, it is highly recommended that you specify a unique `initial-cluster-token` for the different clusters. By doing this, etcd can generate unique cluster IDs and member IDs for the clusters even if they otherwise have the exact same configuration. This can protect you from cross-cluster-interaction, which might corrupt your clusters.
-
-etcd listens on [`listen-client-urls`][conf-listen-client] to accept client traffic. etcd member advertises the URLs specified in [`advertise-client-urls`][conf-adv-client] to other members, proxies, clients. Please make sure the `advertise-client-urls` are reachable from intended clients. A common mistake is setting `advertise-client-urls` to localhost or leave it as default when you want the remote clients to reach etcd.
-
-On each machine you would start etcd with these flags:
-
-```
-$ etcd --name infra0 --initial-advertise-peer-urls http://10.0.1.10:2380 \
-  --listen-peer-urls http://10.0.1.10:2380 \
-  --listen-client-urls http://10.0.1.10:2379,http://127.0.0.1:2379 \
-  --advertise-client-urls http://10.0.1.10:2379 \
-  --initial-cluster-token etcd-cluster-1 \
-  --initial-cluster infra0=http://10.0.1.10:2380,infra1=http://10.0.1.11:2380,infra2=http://10.0.1.12:2380 \
-  --initial-cluster-state new
-```
-```
-$ etcd --name infra1 --initial-advertise-peer-urls http://10.0.1.11:2380 \
-  --listen-peer-urls http://10.0.1.11:2380 \
-  --listen-client-urls http://10.0.1.11:2379,http://127.0.0.1:2379 \
-  --advertise-client-urls http://10.0.1.11:2379 \
-  --initial-cluster-token etcd-cluster-1 \
-  --initial-cluster infra0=http://10.0.1.10:2380,infra1=http://10.0.1.11:2380,infra2=http://10.0.1.12:2380 \
-  --initial-cluster-state new
-```
-```
-$ etcd --name infra2 --initial-advertise-peer-urls http://10.0.1.12:2380 \
-  --listen-peer-urls http://10.0.1.12:2380 \
-  --listen-client-urls http://10.0.1.12:2379,http://127.0.0.1:2379 \
-  --advertise-client-urls http://10.0.1.12:2379 \
-  --initial-cluster-token etcd-cluster-1 \
-  --initial-cluster infra0=http://10.0.1.10:2380,infra1=http://10.0.1.11:2380,infra2=http://10.0.1.12:2380 \
-  --initial-cluster-state new
-```
-
-The command line parameters starting with `--initial-cluster` will be ignored on subsequent runs of etcd. You are free to remove the environment variables or command line flags after the initial bootstrap process. If you need to make changes to the configuration later (for example, adding or removing members to/from the cluster), see the [runtime configuration][runtime-conf] guide.
-
-### Error Cases
-
-In the following example, we have not included our new host in the list of enumerated nodes. If this is a new cluster, the node _must_ be added to the list of initial cluster members.
-
-```
-$ etcd --name infra1 --initial-advertise-peer-urls http://10.0.1.11:2380 \
-  --listen-peer-urls https://10.0.1.11:2380 \
-  --listen-client-urls http://10.0.1.11:2379,http://127.0.0.1:2379 \
-  --advertise-client-urls http://10.0.1.11:2379 \
-  --initial-cluster infra0=http://10.0.1.10:2380 \
-  --initial-cluster-state new
-etcd: infra1 not listed in the initial cluster config
-exit 1
-```
-
-In this example, we are attempting to map a node (infra0) on a different address (127.0.0.1:2380) than its enumerated address in the cluster list (10.0.1.10:2380). If this node is to listen on multiple addresses, all addresses _must_ be reflected in the "initial-cluster" configuration directive.
-
-```
-$ etcd --name infra0 --initial-advertise-peer-urls http://127.0.0.1:2380 \
-  --listen-peer-urls http://10.0.1.10:2380 \
-  --listen-client-urls http://10.0.1.10:2379,http://127.0.0.1:2379 \
-  --advertise-client-urls http://10.0.1.10:2379 \
-  --initial-cluster infra0=http://10.0.1.10:2380,infra1=http://10.0.1.11:2380,infra2=http://10.0.1.12:2380 \
-  --initial-cluster-state=new
-etcd: error setting up initial cluster: infra0 has different advertised URLs in the cluster and advertised peer URLs list
-exit 1
-```
-
-If you configure a peer with a different set of configuration and attempt to join this cluster you will get a cluster ID mismatch and etcd will exit.
-
-```
-$ etcd --name infra3 --initial-advertise-peer-urls http://10.0.1.13:2380 \
-  --listen-peer-urls http://10.0.1.13:2380 \
-  --listen-client-urls http://10.0.1.13:2379,http://127.0.0.1:2379 \
-  --advertise-client-urls http://10.0.1.13:2379 \
-  --initial-cluster infra0=http://10.0.1.10:2380,infra1=http://10.0.1.11:2380,infra3=http://10.0.1.13:2380 \
-  --initial-cluster-state=new
-etcd: conflicting cluster ID to the target cluster (c6ab534d07e8fcc4 != bc25ea2a74fb18b0). Exiting.
-exit 1
-```
-
-## Discovery
-
-In a number of cases, you might not know the IPs of your cluster peers ahead of time. This is common when utilizing cloud providers or when your network uses DHCP. In these cases, rather than specifying a static configuration, you can use an existing etcd cluster to bootstrap a new one. We call this process "discovery".
-
-There two methods that can be used for discovery:
-
-* etcd discovery service
-* DNS SRV records
-
-### etcd Discovery
-
-To better understand the design about discovery service protocol, we suggest you read [this][discovery-proto].
-
-#### Lifetime of a Discovery URL
-
-A discovery URL identifies a unique etcd cluster. Instead of reusing a discovery URL, you should always create discovery URLs for new clusters.
-
-Moreover, discovery URLs should ONLY be used for the initial bootstrapping of a cluster. To change cluster membership after the cluster is already running, see the [runtime reconfiguration][runtime-conf] guide.
-
-#### Custom etcd Discovery Service
-
-Discovery uses an existing cluster to bootstrap itself. If you are using your own etcd cluster you can create a URL like so:
-
-```
-$ curl -X PUT https://myetcd.local/v2/keys/discovery/6c007a14875d53d9bf0ef5a6fc0257c817f0fb83/_config/size -d value=3
-```
-
-By setting the size key to the URL, you create a discovery URL with an expected cluster size of 3.
-
-If you bootstrap an etcd cluster using discovery service with more than the expected number of etcd members, the extra etcd processes will [fall back][fall-back] to being [proxies][proxy] by default.
-
-The URL you will use in this case will be `https://myetcd.local/v2/keys/discovery/6c007a14875d53d9bf0ef5a6fc0257c817f0fb83` and the etcd members will use the `https://myetcd.local/v2/keys/discovery/6c007a14875d53d9bf0ef5a6fc0257c817f0fb83` directory for registration as they start.
-
-**Each member must have a different name flag specified. `Hostname` or `machine-id` can be a good choice. Or discovery will fail due to duplicated name.**
-
-Now we start etcd with those relevant flags for each member:
-
-```
-$ etcd --name infra0 --initial-advertise-peer-urls http://10.0.1.10:2380 \
-  --listen-peer-urls http://10.0.1.10:2380 \
-  --listen-client-urls http://10.0.1.10:2379,http://127.0.0.1:2379 \
-  --advertise-client-urls http://10.0.1.10:2379 \
-  --discovery https://myetcd.local/v2/keys/discovery/6c007a14875d53d9bf0ef5a6fc0257c817f0fb83
-```
-```
-$ etcd --name infra1 --initial-advertise-peer-urls http://10.0.1.11:2380 \
-  --listen-peer-urls http://10.0.1.11:2380 \
-  --listen-client-urls http://10.0.1.11:2379,http://127.0.0.1:2379 \
-  --advertise-client-urls http://10.0.1.11:2379 \
-  --discovery https://myetcd.local/v2/keys/discovery/6c007a14875d53d9bf0ef5a6fc0257c817f0fb83
-```
-```
-$ etcd --name infra2 --initial-advertise-peer-urls http://10.0.1.12:2380 \
-  --listen-peer-urls http://10.0.1.12:2380 \
-  --listen-client-urls http://10.0.1.12:2379,http://127.0.0.1:2379 \
-  --advertise-client-urls http://10.0.1.12:2379 \
-  --discovery https://myetcd.local/v2/keys/discovery/6c007a14875d53d9bf0ef5a6fc0257c817f0fb83
-```
-
-This will cause each member to register itself with the custom etcd discovery service and begin the cluster once all machines have been registered.
-
-#### Public etcd Discovery Service
-
-If you do not have access to an existing cluster, you can use the public discovery service hosted at `discovery.etcd.io`.  You can create a private discovery URL using the "new" endpoint like so:
-
-```
-$ curl https://discovery.etcd.io/new?size=3
-https://discovery.etcd.io/3e86b59982e49066c5d813af1c2e2579cbf573de
-```
-
-This will create the cluster with an initial expected size of 3 members. If you do not specify a size, a default of 3 will be used.
-
-If you bootstrap an etcd cluster using discovery service with more than the expected number of etcd members, the extra etcd processes will [fall back][fall-back] to being [proxies][proxy] by default.
-
-```
-ETCD_DISCOVERY=https://discovery.etcd.io/3e86b59982e49066c5d813af1c2e2579cbf573de
-```
-
-```
---discovery https://discovery.etcd.io/3e86b59982e49066c5d813af1c2e2579cbf573de
-```
-
-**Each member must have a different name flag specified. `Hostname` or `machine-id` can be a good choice. Or discovery will fail due to duplicated name.**
-
-Now we start etcd with those relevant flags for each member:
-
-```
-$ etcd --name infra0 --initial-advertise-peer-urls http://10.0.1.10:2380 \
-  --listen-peer-urls http://10.0.1.10:2380 \
-  --listen-client-urls http://10.0.1.10:2379,http://127.0.0.1:2379 \
-  --advertise-client-urls http://10.0.1.10:2379 \
-  --discovery https://discovery.etcd.io/3e86b59982e49066c5d813af1c2e2579cbf573de
-```
-```
-$ etcd --name infra1 --initial-advertise-peer-urls http://10.0.1.11:2380 \
-  --listen-peer-urls http://10.0.1.11:2380 \
-  --listen-client-urls http://10.0.1.11:2379,http://127.0.0.1:2379 \
-  --advertise-client-urls http://10.0.1.11:2379 \
-  --discovery https://discovery.etcd.io/3e86b59982e49066c5d813af1c2e2579cbf573de
-```
-```
-$ etcd --name infra2 --initial-advertise-peer-urls http://10.0.1.12:2380 \
-  --listen-peer-urls http://10.0.1.12:2380 \
-  --listen-client-urls http://10.0.1.12:2379,http://127.0.0.1:2379 \
-  --advertise-client-urls http://10.0.1.12:2379 \
-  --discovery https://discovery.etcd.io/3e86b59982e49066c5d813af1c2e2579cbf573de
-```
-
-This will cause each member to register itself with the discovery service and begin the cluster once all members have been registered.
-
-You can use the environment variable `ETCD_DISCOVERY_PROXY` to cause etcd to use an HTTP proxy to connect to the discovery service.
-
-#### Error and Warning Cases
-
-##### Discovery Server Errors
-
-
-```
-$ etcd --name infra0 --initial-advertise-peer-urls http://10.0.1.10:2380 \
-  --listen-peer-urls http://10.0.1.10:2380 \
-  --listen-client-urls http://10.0.1.10:2379,http://127.0.0.1:2379 \
-  --advertise-client-urls http://10.0.1.10:2379 \
-  --discovery https://discovery.etcd.io/3e86b59982e49066c5d813af1c2e2579cbf573de
-etcd: error: the cluster doesnâ€™t have a size configuration value in https://discovery.etcd.io/3e86b59982e49066c5d813af1c2e2579cbf573de/_config
-exit 1
-```
-
-##### User Errors
-
-This error will occur if the discovery cluster already has the configured number of members, and `discovery-fallback` is explicitly disabled
-
-```
-$ etcd --name infra0 --initial-advertise-peer-urls http://10.0.1.10:2380 \
-  --listen-peer-urls http://10.0.1.10:2380 \
-  --listen-client-urls http://10.0.1.10:2379,http://127.0.0.1:2379 \
-  --advertise-client-urls http://10.0.1.10:2379 \
-  --discovery https://discovery.etcd.io/3e86b59982e49066c5d813af1c2e2579cbf573de \
-  --discovery-fallback exit
-etcd: discovery: cluster is full
-exit 1
-```
-
-##### Warnings
-
-This is a harmless warning notifying you that the discovery URL will be
-ignored on this machine.
-
-```
-$ etcd --name infra0 --initial-advertise-peer-urls http://10.0.1.10:2380 \
-  --listen-peer-urls http://10.0.1.10:2380 \
-  --listen-client-urls http://10.0.1.10:2379,http://127.0.0.1:2379 \
-  --advertise-client-urls http://10.0.1.10:2379 \
-  --discovery https://discovery.etcd.io/3e86b59982e49066c5d813af1c2e2579cbf573de
-etcdserver: discovery token ignored since a cluster has already been initialized. Valid log found at /var/lib/etcd
-```
-
-### DNS Discovery
-
-DNS [SRV records][rfc-srv] can be used as a discovery mechanism.
-The `-discovery-srv` flag can be used to set the DNS domain name where the discovery SRV records can be found.
-The following DNS SRV records are looked up in the listed order:
-
-* _etcd-server-ssl._tcp.example.com
-* _etcd-server._tcp.example.com
-
-If `_etcd-server-ssl._tcp.example.com` is found then etcd will attempt the bootstrapping process over SSL.
-
-To help clients discover the etcd cluster, the following DNS SRV records are looked up in the listed order:
-
-* _etcd-client._tcp.example.com
-* _etcd-client-ssl._tcp.example.com
-
-If `_etcd-client-ssl._tcp.example.com` is found, clients will attempt to communicate with the etcd cluster over SSL.
-
-#### Create DNS SRV records
-
-```
-$ dig +noall +answer SRV _etcd-server._tcp.example.com
-_etcd-server._tcp.example.com. 300 IN  SRV  0 0 2380 infra0.example.com.
-_etcd-server._tcp.example.com. 300 IN  SRV  0 0 2380 infra1.example.com.
-_etcd-server._tcp.example.com. 300 IN  SRV  0 0 2380 infra2.example.com.
-```
-
-```
-$ dig +noall +answer SRV _etcd-client._tcp.example.com
-_etcd-client._tcp.example.com. 300 IN SRV 0 0 2379 infra0.example.com.
-_etcd-client._tcp.example.com. 300 IN SRV 0 0 2379 infra1.example.com.
-_etcd-client._tcp.example.com. 300 IN SRV 0 0 2379 infra2.example.com.
-```
-
-```
-$ dig +noall +answer infra0.example.com infra1.example.com infra2.example.com
-infra0.example.com.  300  IN  A  10.0.1.10
-infra1.example.com.  300  IN  A  10.0.1.11
-infra2.example.com.  300  IN  A  10.0.1.12
-```
-#### Bootstrap the etcd cluster using DNS
-
-etcd cluster members can listen on domain names or IP address, the bootstrap process will resolve DNS A records.
-
-The resolved address in `--initial-advertise-peer-urls` *must match* one of the resolved addresses in the SRV targets. The etcd member reads the resolved address to find out if it belongs to the cluster defined in the SRV records.
-
-```
-$ etcd --name infra0 \
---discovery-srv example.com \
---initial-advertise-peer-urls http://infra0.example.com:2380 \
---initial-cluster-token etcd-cluster-1 \
---initial-cluster-state new \
---advertise-client-urls http://infra0.example.com:2379 \
---listen-client-urls http://infra0.example.com:2379 \
---listen-peer-urls http://infra0.example.com:2380
-```
-
-```
-$ etcd --name infra1 \
---discovery-srv example.com \
---initial-advertise-peer-urls http://infra1.example.com:2380 \
---initial-cluster-token etcd-cluster-1 \
---initial-cluster-state new \
---advertise-client-urls http://infra1.example.com:2379 \
---listen-client-urls http://infra1.example.com:2379 \
---listen-peer-urls http://infra1.example.com:2380
-```
-
-```
-$ etcd --name infra2 \
---discovery-srv example.com \
---initial-advertise-peer-urls http://infra2.example.com:2380 \
---initial-cluster-token etcd-cluster-1 \
---initial-cluster-state new \
---advertise-client-urls http://infra2.example.com:2379 \
---listen-client-urls http://infra2.example.com:2379 \
---listen-peer-urls http://infra2.example.com:2380
-```
-
-You can also bootstrap the cluster using IP addresses instead of domain names:
-
-```
-$ etcd --name infra0 \
---discovery-srv example.com \
---initial-advertise-peer-urls http://10.0.1.10:2380 \
---initial-cluster-token etcd-cluster-1 \
---initial-cluster-state new \
---advertise-client-urls http://10.0.1.10:2379 \
---listen-client-urls http://10.0.1.10:2379 \
---listen-peer-urls http://10.0.1.10:2380
-```
-
-```
-$ etcd --name infra1 \
---discovery-srv example.com \
---initial-advertise-peer-urls http://10.0.1.11:2380 \
---initial-cluster-token etcd-cluster-1 \
---initial-cluster-state new \
---advertise-client-urls http://10.0.1.11:2379 \
---listen-client-urls http://10.0.1.11:2379 \
---listen-peer-urls http://10.0.1.11:2380
-```
-
-```
-$ etcd --name infra2 \
---discovery-srv example.com \
---initial-advertise-peer-urls http://10.0.1.12:2380 \
---initial-cluster-token etcd-cluster-1 \
---initial-cluster-state new \
---advertise-client-urls http://10.0.1.12:2379 \
---listen-client-urls http://10.0.1.12:2379 \
---listen-peer-urls http://10.0.1.12:2380
-```
-
-#### etcd proxy configuration
-
-DNS SRV records can also be used to configure the list of peers for an etcd server running in proxy mode:
-
-```
-$ etcd --proxy on --discovery-srv example.com
-```
-
-#### etcd client configuration
-
-DNS SRV records can also be used to help clients discover the etcd cluster.
-
-The official [etcd/client][client] supports [DNS Discovery][client-discoverer].
-
-`etcdctl` also supports DNS Discovery by specifying the `--discovery-srv` option.
-
-```
-$ etcdctl --discovery-srv example.com set foo bar
-```
-
-#### Error Cases
-
-You might see an error like `cannot find local etcd $name from SRV records.`. That means the etcd member fails to find itself from the cluster defined in SRV records. The resolved address in `--initial-advertise-peer-urls` *must match* one of the resolved addresses in the SRV targets.
-
-# 0.4 to 2.0+ Migration Guide
-
-In etcd 2.0 we introduced the ability to listen on more than one address and to advertise multiple addresses. This makes using etcd easier when you have complex networking, such as private and public networks on various cloud providers.
-
-To make understanding this feature easier, we changed the naming of some flags, but we support the old flags to make the migration from the old to new version easier.
-
-|Old Flag    |New Flag    |Migration Behavior                  |
-|-----------------------|-----------------------|---------------------------------------------------------------------------------------|
-|-peer-addr    |--initial-advertise-peer-urls   |If specified, peer-addr will be used as the only peer URL. Error if both flags specified.|
-|-addr      |--advertise-client-urls  |If specified, addr will be used as the only client URL. Error if both flags specified.|
-|-peer-bind-addr  |--listen-peer-urls  |If specified, peer-bind-addr will be used as the only peer bind URL. Error if both flags specified.|
-|-bind-addr    |--listen-client-urls  |If specified, bind-addr will be used as the only client bind URL. Error if both flags specified.|
-|-peers      |none      |Deprecated. The --initial-cluster flag provides a similar concept with different semantics. Please read this guide on cluster startup.|
-|-peers-file    |none      |Deprecated. The --initial-cluster flag provides a similar concept with different semantics. Please read this guide on cluster startup.|
-
-[client]: /client
-[client-discoverer]: https://godoc.org/github.com/coreos/etcd/client#Discoverer
-[conf-adv-client]: configuration.md#-advertise-client-urls
-[conf-listen-client]: configuration.md#-listen-client-urls
-[discovery-proto]: discovery_protocol.md
-[fall-back]: proxy.md#fallback-to-proxy-mode-with-discovery-service
-[proxy]: proxy.md
-[rfc-srv]: http://www.ietf.org/rfc/rfc2052.txt
-[runtime-conf]: runtime-configuration.md
-[runtime-reconf-design]: runtime-reconf-design.md
diff --git a/Documentation/configuration.md b/Documentation/configuration.md
deleted file mode 100644
index 99817aa..0000000
--- a/Documentation/configuration.md
+++ /dev/null
@@ -1,275 +0,0 @@
-# Configuration Flags
-
-etcd is configurable through command-line flags and environment variables. Options set on the command line take precedence over those from the environment.
-
-The format of environment variable for flag `--my-flag` is `ETCD_MY_FLAG`. It applies to all  flags.
-
-The [official etcd ports][iana-ports] are 2379 for client requests, and 2380 for peer communication. Some legacy code and documentation still references ports 4001 and 7001, but all new etcd use and discussion should adopt the assigned ports.
-
-To start etcd automatically using custom settings at startup in Linux, using a [systemd][systemd-intro] unit is highly recommended.
-
-[systemd-intro]: http://freedesktop.org/wiki/Software/systemd/
-
-## Member Flags
-
-### --name
-+ Human-readable name for this member.
-+ default: "default"
-+ env variable: ETCD_NAME
-+ This value is referenced as this node's own entries listed in the `--initial-cluster` flag (Ex: `default=http://localhost:2380` or `default=http://localhost:2380,default=http://localhost:7001`). This needs to match the key used in the flag if you're using [static bootstrapping][build-cluster]. When using discovery, each member must have a unique name. `Hostname` or `machine-id` can be a good choice.
-
-### --data-dir
-+ Path to the data directory.
-+ default: "${name}.etcd"
-+ env variable: ETCD_DATA_DIR
-
-### --wal-dir
-+ Path to the dedicated wal directory. If this flag is set, etcd will write the WAL files to the walDir rather than the dataDir. This allows a dedicated disk to be used, and helps avoid io competition between logging and other IO operations.
-+ default: ""
-+ env variable: ETCD_WAL_DIR
-
-### --snapshot-count
-+ Number of committed transactions to trigger a snapshot to disk.
-+ default: "10000"
-+ env variable: ETCD_SNAPSHOT_COUNT
-
-### --heartbeat-interval
-+ Time (in milliseconds) of a heartbeat interval.
-+ default: "100"
-+ env variable: ETCD_HEARTBEAT_INTERVAL
-
-### --election-timeout
-+ Time (in milliseconds) for an election to timeout. See [Documentation/tuning.md](tuning.md#time-parameters) for details.
-+ default: "1000"
-+ env variable: ETCD_ELECTION_TIMEOUT
-
-### --listen-peer-urls
-+ List of URLs to listen on for peer traffic. This flag tells the etcd to accept incoming requests from its peers on the specified scheme://IP:port combinations. Scheme can be either http or https.If 0.0.0.0 is specified as the IP, etcd listens to the given port on all interfaces. If an IP address is given as well as a port, etcd will listen on the given port and interface. Multiple URLs may be used to specify a number of addresses and ports to listen on. The etcd will respond to requests from any of the listed addresses and ports.
-+ default: "http://localhost:2380,http://localhost:7001"
-+ env variable: ETCD_LISTEN_PEER_URLS
-+ example: "http://10.0.0.1:2380"
-+ invalid example: "http://example.com:2380" (domain name is invalid for binding)
-
-### --listen-client-urls
-+ List of URLs to listen on for client traffic. This flag tells the etcd to accept incoming requests from the clients on the specified scheme://IP:port combinations. Scheme can be either http or https. If 0.0.0.0 is specified as the IP, etcd listens to the given port on all interfaces. If an IP address is given as well as a port, etcd will listen on the given port and interface. Multiple URLs may be used to specify a number of addresses and ports to listen on. The etcd will respond to requests from any of the listed addresses and ports.
-+ default: "http://localhost:2379,http://localhost:4001"
-+ env variable: ETCD_LISTEN_CLIENT_URLS
-+ example: "http://10.0.0.1:2379"
-+ invalid example: "http://example.com:2379" (domain name is invalid for binding)
-
-### --max-snapshots
-+ Maximum number of snapshot files to retain (0 is unlimited)
-+ default: 5
-+ env variable: ETCD_MAX_SNAPSHOTS
-+ The default for users on Windows is unlimited, and manual purging down to 5 (or your preference for safety) is recommended.
-
-### --max-wals
-+ Maximum number of wal files to retain (0 is unlimited)
-+ default: 5
-+ env variable: ETCD_MAX_WALS
-+ The default for users on Windows is unlimited, and manual purging down to 5 (or your preference for safety) is recommended.
-
-### --cors
-+ Comma-separated white list of origins for CORS (cross-origin resource sharing).
-+ default: none
-+ env variable: ETCD_CORS
-
-## Clustering Flags
-
-`--initial` prefix flags are used in bootstrapping ([static bootstrap][build-cluster], [discovery-service bootstrap][discovery] or [runtime reconfiguration][reconfig]) a new member, and ignored when restarting an existing member.
-
-`--discovery` prefix flags need to be set when using [discovery service][discovery].
-
-### --initial-advertise-peer-urls
-
-+ List of this member's peer URLs to advertise to the rest of the cluster. These addresses are used for communicating etcd data around the cluster. At least one must be routable to all cluster members. These URLs can contain domain names.
-+ default: "http://localhost:2380,http://localhost:7001"
-+ env variable: ETCD_INITIAL_ADVERTISE_PEER_URLS
-+ example: "http://example.com:2380, http://10.0.0.1:2380"
-
-### --initial-cluster
-+ Initial cluster configuration for bootstrapping.
-+ default: "default=http://localhost:2380,default=http://localhost:7001"
-+ env variable: ETCD_INITIAL_CLUSTER
-+ The key is the value of the `--name` flag for each node provided. The default uses `default` for the key because this is the default for the `--name` flag.
-
-### --initial-cluster-state
-+ Initial cluster state ("new" or "existing"). Set to `new` for all members present during initial static or DNS bootstrapping. If this option is set to `existing`, etcd will attempt to join the existing cluster. If the wrong value is set, etcd will attempt to start but fail safely.
-+ default: "new"
-+ env variable: ETCD_INITIAL_CLUSTER_STATE
-
-[static bootstrap]: clustering.md#static
-
-### --initial-cluster-token
-+ Initial cluster token for the etcd cluster during bootstrap.
-+ default: "etcd-cluster"
-+ env variable: ETCD_INITIAL_CLUSTER_TOKEN
-
-### --advertise-client-urls
-+ List of this member's client URLs to advertise to the rest of the cluster. These URLs can contain domain names.
-+ default: "http://localhost:2379,http://localhost:4001"
-+ env variable: ETCD_ADVERTISE_CLIENT_URLS
-+ example: "http://example.com:2379, http://10.0.0.1:2379"
-+ Be careful if you are advertising URLs such as http://localhost:2379 from a cluster member and are using the proxy feature of etcd. This will cause loops, because the proxy will be forwarding requests to itself until its resources (memory, file descriptors) are eventually depleted.
-
-### --discovery
-+ Discovery URL used to bootstrap the cluster.
-+ default: none
-+ env variable: ETCD_DISCOVERY
-
-### --discovery-srv
-+ DNS srv domain used to bootstrap the cluster.
-+ default: none
-+ env variable: ETCD_DISCOVERY_SRV
-
-### --discovery-fallback
-+ Expected behavior ("exit" or "proxy") when discovery services fails.
-+ default: "proxy"
-+ env variable: ETCD_DISCOVERY_FALLBACK
-
-### --discovery-proxy
-+ HTTP proxy to use for traffic to discovery service.
-+ default: none
-+ env variable: ETCD_DISCOVERY_PROXY
-
-### --strict-reconfig-check
-+ Reject reconfiguration requests that would cause quorum loss.
-+ default: false
-+ env variable: ETCD_STRICT_RECONFIG_CHECK
-
-## Proxy Flags
-
-`--proxy` prefix flags configures etcd to run in [proxy mode][proxy].
-
-### --proxy
-+ Proxy mode setting ("off", "readonly" or "on").
-+ default: "off"
-+ env variable: ETCD_PROXY
-
-### --proxy-failure-wait
-+ Time (in milliseconds) an endpoint will be held in a failed state before being reconsidered for proxied requests.
-+ default: 5000
-+ env variable: ETCD_PROXY_FAILURE_WAIT
-
-### --proxy-refresh-interval
-+ Time (in milliseconds) of the endpoints refresh interval.
-+ default: 30000
-+ env variable: ETCD_PROXY_REFRESH_INTERVAL
-
-### --proxy-dial-timeout
-+ Time (in milliseconds) for a dial to timeout or 0 to disable the timeout
-+ default: 1000
-+ env variable: ETCD_PROXY_DIAL_TIMEOUT
-
-### --proxy-write-timeout
-+ Time (in milliseconds) for a write to timeout or 0 to disable the timeout.
-+ default: 5000
-+ env variable: ETCD_PROXY_WRITE_TIMEOUT
-
-### --proxy-read-timeout
-+ Time (in milliseconds) for a read to timeout or 0 to disable the timeout.
-+ Don't change this value if you use watches because they are using long polling requests.
-+ default: 0
-+ env variable: ETCD_PROXY_READ_TIMEOUT
-
-## Security Flags
-
-The security flags help to [build a secure etcd cluster][security].
-
-### --ca-file [DEPRECATED]
-+ Path to the client server TLS CA file. `--ca-file ca.crt` could be replaced by `--trusted-ca-file ca.crt --client-cert-auth` and etcd will perform the same.
-+ default: none
-+ env variable: ETCD_CA_FILE
-
-### --cert-file
-+ Path to the client server TLS cert file.
-+ default: none
-+ env variable: ETCD_CERT_FILE
-
-### --key-file
-+ Path to the client server TLS key file.
-+ default: none
-+ env variable: ETCD_KEY_FILE
-
-### --client-cert-auth
-+ Enable client cert authentication.
-+ default: false
-+ env variable: ETCD_CLIENT_CERT_AUTH
-
-### --trusted-ca-file
-+ Path to the client server TLS trusted CA key file.
-+ default: none
-+ env variable: ETCD_TRUSTED_CA_FILE
-
-### --peer-ca-file [DEPRECATED]
-+ Path to the peer server TLS CA file. `--peer-ca-file ca.crt` could be replaced by `--peer-trusted-ca-file ca.crt --peer-client-cert-auth` and etcd will perform the same.
-+ default: none
-+ env variable: ETCD_PEER_CA_FILE
-
-### --peer-cert-file
-+ Path to the peer server TLS cert file.
-+ default: none
-+ env variable: ETCD_PEER_CERT_FILE
-
-### --peer-key-file
-+ Path to the peer server TLS key file.
-+ default: none
-+ env variable: ETCD_PEER_KEY_FILE
-
-### --peer-client-cert-auth
-+ Enable peer client cert authentication.
-+ default: false
-+ env variable: ETCD_PEER_CLIENT_CERT_AUTH
-
-### --peer-trusted-ca-file
-+ Path to the peer server TLS trusted CA file.
-+ default: none
-+ env variable: ETCD_PEER_TRUSTED_CA_FILE
-
-## Logging Flags
-
-### --debug
-+ Drop the default log level to DEBUG for all subpackages.
-+ default: false (INFO for all packages)
-+ env variable: ETCD_DEBUG
-
-### --log-package-levels
-+ Set individual etcd subpackages to specific log levels. An example being `etcdserver=WARNING,security=DEBUG` 
-+ default: none (INFO for all packages)
-+ env variable: ETCD_LOG_PACKAGE_LEVELS
-
-
-## Unsafe Flags
-
-Please be CAUTIOUS when using unsafe flags because it will break the guarantees given by the consensus protocol.
-For example, it may panic if other members in the cluster are still alive.
-Follow the instructions when using these flags.
-
-### --force-new-cluster
-+ Force to create a new one-member cluster. It commits configuration changes forcing to remove all existing members in the cluster and add itself. It needs to be set to [restore a backup][restore].
-+ default: false
-+ env variable: ETCD_FORCE_NEW_CLUSTER
-
-## Miscellaneous Flags
-
-### --version
-+ Print the version and exit.
-+ default: false
-
-## Profiling flags
-
-### --enable-pprof
-+ Enable runtime profiling data via HTTP server. Address is at client URL + "/debug/pprof"
-+ default: false
-
-[build-cluster]: clustering.md#static
-[reconfig]: runtime-configuration.md
-[discovery]: clustering.md#discovery
-[iana-ports]: https://www.iana.org/assignments/service-names-port-numbers/service-names-port-numbers.xhtml?search=etcd
-[proxy]: proxy.md
-[reconfig]: runtime-configuration.md
-[restore]: admin_guide.md#restoring-a-backup
-[rfc-v3]: rfc/v3api.md
-[security]: security.md
-[systemd-intro]: http://freedesktop.org/wiki/Software/systemd/
-[tuning]: tuning.md#time-parameters
diff --git a/Documentation/dev-internal/discovery_protocol.md b/Documentation/dev-internal/discovery_protocol.md
new file mode 100644
index 0000000..c78a4c6
--- /dev/null
+++ b/Documentation/dev-internal/discovery_protocol.md
@@ -0,0 +1,114 @@
+# Discovery Service Protocol
+
+Discovery service protocol helps new etcd member to discover all other members in cluster bootstrap phase using a shared discovery URL.
+
+Discovery service protocol is _only_ used in cluster bootstrap phase, and cannot be used for runtime reconfiguration or cluster monitoring.
+
+The protocol uses a new discovery token to bootstrap one _unique_ etcd cluster. Remember that one discovery token can represent only one etcd cluster. As long as discovery protocol on this token starts, even if it fails halfway, it must not be used to bootstrap another etcd cluster.
+
+The rest of this article will walk through the discovery process with examples that correspond to a self-hosted discovery cluster. The public discovery service, discovery.etcd.io, functions the same way, but with a layer of polish to abstract away ugly URLs, generate UUIDs automatically, and provide some protections against excessive requests. At its core, the public discovery service still uses an etcd cluster as the data store as described in this document.
+
+## The Protocol Workflow
+
+The idea of discovery protocol is to use an internal etcd cluster to coordinate bootstrap of a new cluster. First, all new members interact with discovery service and help to generate the expected member list. Then each new member bootstraps its server using this list, which performs the same functionality as -initial-cluster flag.
+
+In the following example workflow, we will list each step of protocol in curl format for ease of understanding.
+
+By convention the etcd discovery protocol uses the key prefix `_etcd/registry`. If `http://example.com` hosts an etcd cluster for discovery service, a full URL to discovery keyspace will be `http://example.com/v2/keys/_etcd/registry`. We will use this as the URL prefix in the example.
+
+### Creating a New Discovery Token
+
+Generate a unique token that will identify the new cluster. This will be used as a unique prefix in discovery keyspace in the following steps. An easy way to do this is to use `uuidgen`:
+
+```
+UUID=$(uuidgen)
+```
+
+### Specifying the Expected Cluster Size
+
+You need to specify the expected cluster size for this discovery token. The size is used by the discovery service to know when it has found all members that will initially form the cluster.
+
+```
+curl -X PUT http://example.com/v2/keys/_etcd/registry/${UUID}/_config/size -d value=${cluster_size}
+```
+
+Usually the cluster size is 3, 5 or 7. Check [optimal cluster size][cluster-size] for more details.
+
+### Bringing up etcd Processes
+
+Now that you have your discovery URL, you can use it as `-discovery` flag and bring up etcd processes. Every etcd process will follow this next few steps internally if given a `-discovery` flag.
+
+### Registering itself
+
+The first thing for etcd process is to register itself into the discovery URL as a member. This is done by creating member ID as a key in the discovery URL.
+
+```
+curl -X PUT http://example.com/v2/keys/_etcd/registry/${UUID}/${member_id}?prevExist=false -d value="${member_name}=${member_peer_url_1}&${member_name}=${member_peer_url_2}"
+```
+
+### Checking the Status
+
+It checks the expected cluster size and registration status in discovery URL, and decides what the next action is.
+
+```
+curl -X GET http://example.com/v2/keys/_etcd/registry/${UUID}/_config/size
+curl -X GET http://example.com/v2/keys/_etcd/registry/${UUID}
+```
+
+If registered members are still not enough, it will wait for left members to appear.
+
+If the number of registered members is bigger than the expected size N, it treats the first N registered members as the member list for the cluster. If the member itself is in the member list, the discovery procedure succeeds and it fetches all peers through the member list. If it is not in the member list, the discovery procedure finishes with the failure that the cluster has been full.
+
+In etcd implementation, the member may check the cluster status even before registering itself. So it could fail quickly if the cluster has been full.
+
+### Waiting for All Members
+
+
+The wait process is described in detail in the [etcd API documentation][api].
+
+```
+curl -X GET http://example.com/v2/keys/_etcd/registry/${UUID}?wait=true&waitIndex=${current_etcd_index}
+```
+
+It keeps waiting until finding all members.
+
+## Public Discovery Service
+
+CoreOS Inc. hosts a public discovery service at https://discovery.etcd.io/ , which provides some nice features for ease of use.
+
+### Mask Key Prefix
+
+Public discovery service will redirect `https://discovery.etcd.io/${UUID}` to etcd cluster behind for the key at `/v2/keys/_etcd/registry`. It masks register key prefix for short and readable discovery url.
+
+### Get new token
+
+```
+GET /new
+
+Sent query:
+	size=${cluster_size}
+Possible status codes:
+	200 OK
+	400 Bad Request
+200 Body:
+	generated discovery url
+```
+
+The generation process in the service follows the steps from [Creating a New Discovery Token][new-discovery-token] to [Specifying the Expected Cluster Size][expected-cluster-size].
+
+### Check Discovery Status
+
+```
+GET /${UUID}
+```
+
+You can check the status for this discovery token, including the machines that have been registered, by requesting the value of the UUID.
+
+### Open-source repository
+
+The repository is located at https://github.com/coreos/discovery.etcd.io. You could use it to build your own public discovery service.
+
+[api]: api.md#waiting-for-a-change
+[cluster-size]: admin_guide.md#optimal-cluster-size
+[expected-cluster-size]: #specifying-the-expected-cluster-size
+[new-discovery-token]: #creating-a-new-discovery-token
diff --git a/Documentation/discovery_protocol.md b/Documentation/discovery_protocol.md
deleted file mode 100644
index c78a4c6..0000000
--- a/Documentation/discovery_protocol.md
+++ /dev/null
@@ -1,114 +0,0 @@
-# Discovery Service Protocol
-
-Discovery service protocol helps new etcd member to discover all other members in cluster bootstrap phase using a shared discovery URL.
-
-Discovery service protocol is _only_ used in cluster bootstrap phase, and cannot be used for runtime reconfiguration or cluster monitoring.
-
-The protocol uses a new discovery token to bootstrap one _unique_ etcd cluster. Remember that one discovery token can represent only one etcd cluster. As long as discovery protocol on this token starts, even if it fails halfway, it must not be used to bootstrap another etcd cluster.
-
-The rest of this article will walk through the discovery process with examples that correspond to a self-hosted discovery cluster. The public discovery service, discovery.etcd.io, functions the same way, but with a layer of polish to abstract away ugly URLs, generate UUIDs automatically, and provide some protections against excessive requests. At its core, the public discovery service still uses an etcd cluster as the data store as described in this document.
-
-## The Protocol Workflow
-
-The idea of discovery protocol is to use an internal etcd cluster to coordinate bootstrap of a new cluster. First, all new members interact with discovery service and help to generate the expected member list. Then each new member bootstraps its server using this list, which performs the same functionality as -initial-cluster flag.
-
-In the following example workflow, we will list each step of protocol in curl format for ease of understanding.
-
-By convention the etcd discovery protocol uses the key prefix `_etcd/registry`. If `http://example.com` hosts an etcd cluster for discovery service, a full URL to discovery keyspace will be `http://example.com/v2/keys/_etcd/registry`. We will use this as the URL prefix in the example.
-
-### Creating a New Discovery Token
-
-Generate a unique token that will identify the new cluster. This will be used as a unique prefix in discovery keyspace in the following steps. An easy way to do this is to use `uuidgen`:
-
-```
-UUID=$(uuidgen)
-```
-
-### Specifying the Expected Cluster Size
-
-You need to specify the expected cluster size for this discovery token. The size is used by the discovery service to know when it has found all members that will initially form the cluster.
-
-```
-curl -X PUT http://example.com/v2/keys/_etcd/registry/${UUID}/_config/size -d value=${cluster_size}
-```
-
-Usually the cluster size is 3, 5 or 7. Check [optimal cluster size][cluster-size] for more details.
-
-### Bringing up etcd Processes
-
-Now that you have your discovery URL, you can use it as `-discovery` flag and bring up etcd processes. Every etcd process will follow this next few steps internally if given a `-discovery` flag.
-
-### Registering itself
-
-The first thing for etcd process is to register itself into the discovery URL as a member. This is done by creating member ID as a key in the discovery URL.
-
-```
-curl -X PUT http://example.com/v2/keys/_etcd/registry/${UUID}/${member_id}?prevExist=false -d value="${member_name}=${member_peer_url_1}&${member_name}=${member_peer_url_2}"
-```
-
-### Checking the Status
-
-It checks the expected cluster size and registration status in discovery URL, and decides what the next action is.
-
-```
-curl -X GET http://example.com/v2/keys/_etcd/registry/${UUID}/_config/size
-curl -X GET http://example.com/v2/keys/_etcd/registry/${UUID}
-```
-
-If registered members are still not enough, it will wait for left members to appear.
-
-If the number of registered members is bigger than the expected size N, it treats the first N registered members as the member list for the cluster. If the member itself is in the member list, the discovery procedure succeeds and it fetches all peers through the member list. If it is not in the member list, the discovery procedure finishes with the failure that the cluster has been full.
-
-In etcd implementation, the member may check the cluster status even before registering itself. So it could fail quickly if the cluster has been full.
-
-### Waiting for All Members
-
-
-The wait process is described in detail in the [etcd API documentation][api].
-
-```
-curl -X GET http://example.com/v2/keys/_etcd/registry/${UUID}?wait=true&waitIndex=${current_etcd_index}
-```
-
-It keeps waiting until finding all members.
-
-## Public Discovery Service
-
-CoreOS Inc. hosts a public discovery service at https://discovery.etcd.io/ , which provides some nice features for ease of use.
-
-### Mask Key Prefix
-
-Public discovery service will redirect `https://discovery.etcd.io/${UUID}` to etcd cluster behind for the key at `/v2/keys/_etcd/registry`. It masks register key prefix for short and readable discovery url.
-
-### Get new token
-
-```
-GET /new
-
-Sent query:
-	size=${cluster_size}
-Possible status codes:
-	200 OK
-	400 Bad Request
-200 Body:
-	generated discovery url
-```
-
-The generation process in the service follows the steps from [Creating a New Discovery Token][new-discovery-token] to [Specifying the Expected Cluster Size][expected-cluster-size].
-
-### Check Discovery Status
-
-```
-GET /${UUID}
-```
-
-You can check the status for this discovery token, including the machines that have been registered, by requesting the value of the UUID.
-
-### Open-source repository
-
-The repository is located at https://github.com/coreos/discovery.etcd.io. You could use it to build your own public discovery service.
-
-[api]: api.md#waiting-for-a-change
-[cluster-size]: admin_guide.md#optimal-cluster-size
-[expected-cluster-size]: #specifying-the-expected-cluster-size
-[new-discovery-token]: #creating-a-new-discovery-token
diff --git a/Documentation/docs.md b/Documentation/docs.md
index 0a0950c..2871c4c 100644
--- a/Documentation/docs.md
+++ b/Documentation/docs.md
@@ -16,10 +16,10 @@ Want to develop applications with etcd? Start by [setting up a local cluster][lo
 
 ## Operating etcd clusters
 
-Want to operate etcd clusters? Start by setting up a cluster on multiple machines!
+Want to operate etcd clusters? Start by [setting up a cluster on multiple machines][clustering]!
 
- - Setting up clusters
- - Configuration
+ - [Setting up clusters][clustering]
+ - [Configuration][conf]
  - Security
  - Monitoring
  - [Maintenance][maintenance]
@@ -40,9 +40,11 @@ Want to learn more about the concepts and internals behind etcd? Read the follow
 ## Troubleshooting
 
 [api_ref]: dev-guide/api_reference_v3.md
+[clustering]: op-guide/clustering.md
+[conf]: op-guide/configuration.md
 [download_build]: dl_build.md
 [glossary]: learning/glossary.md
 [interacting]: dev-guide/interacting_v3.md
 [local_cluster]: dev-guide/local_cluster.md
-[maintenance]: op_guide/maintenance.md
-[recovery]: op_guide/recovery.md
\ No newline at end of file
+[recovery]: op-guide/recovery.md
+[maintenance]: op-guide/maintenance.md
diff --git a/Documentation/op-guide/clustering.md b/Documentation/op-guide/clustering.md
new file mode 100644
index 0000000..9f5a34f
--- /dev/null
+++ b/Documentation/op-guide/clustering.md
@@ -0,0 +1,380 @@
+# Clustering Guide
+
+## Overview
+
+Starting an etcd cluster statically requires that each member knows another in the cluster. In a number of cases, you might not know the IPs of your cluster members ahead of time. In these cases, you can bootstrap an etcd cluster with the help of a discovery service.
+
+Once an etcd cluster is up and running, adding or removing members is done via [runtime reconfiguration][runtime-conf]. To better understand the design behind runtime reconfiguration, we suggest you read [the runtime configuration design document][runtime-reconf-design].
+
+This guide will cover the following mechanisms for bootstrapping an etcd cluster:
+
+* [Static](#static)
+* [etcd Discovery](#etcd-discovery)
+* [DNS Discovery](#dns-discovery)
+
+Each of the bootstrapping mechanisms will be used to create a three machine etcd cluster with the following details:
+
+|Name|Address|Hostname|
+|------|---------|------------------|
+|infra0|10.0.1.10|infra0.example.com|
+|infra1|10.0.1.11|infra1.example.com|
+|infra2|10.0.1.12|infra2.example.com|
+
+## Static
+
+As we know the cluster members, their addresses and the size of the cluster before starting, we can use an offline bootstrap configuration by setting the `initial-cluster` flag. Each machine will get either the following command line or environment variables:
+
+```
+ETCD_INITIAL_CLUSTER="infra0=http://10.0.1.10:2380,infra1=http://10.0.1.11:2380,infra2=http://10.0.1.12:2380"
+ETCD_INITIAL_CLUSTER_STATE=new
+```
+
+```
+--initial-cluster infra0=http://10.0.1.10:2380,infra1=http://10.0.1.11:2380,infra2=http://10.0.1.12:2380 \
+--initial-cluster-state new
+```
+
+Note that the URLs specified in `initial-cluster` are the _advertised peer URLs_, i.e. they should match the value of `initial-advertise-peer-urls` on the respective nodes.
+
+If you are spinning up multiple clusters (or creating and destroying a single cluster) with same configuration for testing purpose, it is highly recommended that you specify a unique `initial-cluster-token` for the different clusters. By doing this, etcd can generate unique cluster IDs and member IDs for the clusters even if they otherwise have the exact same configuration. This can protect you from cross-cluster-interaction, which might corrupt your clusters.
+
+etcd listens on [`listen-client-urls`][conf-listen-client] to accept client traffic. etcd member advertises the URLs specified in [`advertise-client-urls`][conf-adv-client] to other members, proxies, clients. Please make sure the `advertise-client-urls` are reachable from intended clients. A common mistake is setting `advertise-client-urls` to localhost or leave it as default when you want the remote clients to reach etcd.
+
+On each machine you would start etcd with these flags:
+
+```
+$ etcd --name infra0 --initial-advertise-peer-urls http://10.0.1.10:2380 \
+  --listen-peer-urls http://10.0.1.10:2380 \
+  --listen-client-urls http://10.0.1.10:2379,http://127.0.0.1:2379 \
+  --advertise-client-urls http://10.0.1.10:2379 \
+  --initial-cluster-token etcd-cluster-1 \
+  --initial-cluster infra0=http://10.0.1.10:2380,infra1=http://10.0.1.11:2380,infra2=http://10.0.1.12:2380 \
+  --initial-cluster-state new
+```
+```
+$ etcd --name infra1 --initial-advertise-peer-urls http://10.0.1.11:2380 \
+  --listen-peer-urls http://10.0.1.11:2380 \
+  --listen-client-urls http://10.0.1.11:2379,http://127.0.0.1:2379 \
+  --advertise-client-urls http://10.0.1.11:2379 \
+  --initial-cluster-token etcd-cluster-1 \
+  --initial-cluster infra0=http://10.0.1.10:2380,infra1=http://10.0.1.11:2380,infra2=http://10.0.1.12:2380 \
+  --initial-cluster-state new
+```
+```
+$ etcd --name infra2 --initial-advertise-peer-urls http://10.0.1.12:2380 \
+  --listen-peer-urls http://10.0.1.12:2380 \
+  --listen-client-urls http://10.0.1.12:2379,http://127.0.0.1:2379 \
+  --advertise-client-urls http://10.0.1.12:2379 \
+  --initial-cluster-token etcd-cluster-1 \
+  --initial-cluster infra0=http://10.0.1.10:2380,infra1=http://10.0.1.11:2380,infra2=http://10.0.1.12:2380 \
+  --initial-cluster-state new
+```
+
+The command line parameters starting with `--initial-cluster` will be ignored on subsequent runs of etcd. You are free to remove the environment variables or command line flags after the initial bootstrap process. If you need to make changes to the configuration later (for example, adding or removing members to/from the cluster), see the [runtime configuration][runtime-conf] guide.
+
+### Error Cases
+
+In the following example, we have not included our new host in the list of enumerated nodes. If this is a new cluster, the node _must_ be added to the list of initial cluster members.
+
+```
+$ etcd --name infra1 --initial-advertise-peer-urls http://10.0.1.11:2380 \
+  --listen-peer-urls https://10.0.1.11:2380 \
+  --listen-client-urls http://10.0.1.11:2379,http://127.0.0.1:2379 \
+  --advertise-client-urls http://10.0.1.11:2379 \
+  --initial-cluster infra0=http://10.0.1.10:2380 \
+  --initial-cluster-state new
+etcd: infra1 not listed in the initial cluster config
+exit 1
+```
+
+In this example, we are attempting to map a node (infra0) on a different address (127.0.0.1:2380) than its enumerated address in the cluster list (10.0.1.10:2380). If this node is to listen on multiple addresses, all addresses _must_ be reflected in the "initial-cluster" configuration directive.
+
+```
+$ etcd --name infra0 --initial-advertise-peer-urls http://127.0.0.1:2380 \
+  --listen-peer-urls http://10.0.1.10:2380 \
+  --listen-client-urls http://10.0.1.10:2379,http://127.0.0.1:2379 \
+  --advertise-client-urls http://10.0.1.10:2379 \
+  --initial-cluster infra0=http://10.0.1.10:2380,infra1=http://10.0.1.11:2380,infra2=http://10.0.1.12:2380 \
+  --initial-cluster-state=new
+etcd: error setting up initial cluster: infra0 has different advertised URLs in the cluster and advertised peer URLs list
+exit 1
+```
+
+If you configure a peer with a different set of configuration and attempt to join this cluster you will get a cluster ID mismatch and etcd will exit.
+
+```
+$ etcd --name infra3 --initial-advertise-peer-urls http://10.0.1.13:2380 \
+  --listen-peer-urls http://10.0.1.13:2380 \
+  --listen-client-urls http://10.0.1.13:2379,http://127.0.0.1:2379 \
+  --advertise-client-urls http://10.0.1.13:2379 \
+  --initial-cluster infra0=http://10.0.1.10:2380,infra1=http://10.0.1.11:2380,infra3=http://10.0.1.13:2380 \
+  --initial-cluster-state=new
+etcd: conflicting cluster ID to the target cluster (c6ab534d07e8fcc4 != bc25ea2a74fb18b0). Exiting.
+exit 1
+```
+
+## Discovery
+
+In a number of cases, you might not know the IPs of your cluster peers ahead of time. This is common when utilizing cloud providers or when your network uses DHCP. In these cases, rather than specifying a static configuration, you can use an existing etcd cluster to bootstrap a new one. We call this process "discovery".
+
+There two methods that can be used for discovery:
+
+* etcd discovery service
+* DNS SRV records
+
+### etcd Discovery
+
+To better understand the design about discovery service protocol, we suggest you read [this][discovery-proto].
+
+#### Lifetime of a Discovery URL
+
+A discovery URL identifies a unique etcd cluster. Instead of reusing a discovery URL, you should always create discovery URLs for new clusters.
+
+Moreover, discovery URLs should ONLY be used for the initial bootstrapping of a cluster. To change cluster membership after the cluster is already running, see the [runtime reconfiguration][runtime-conf] guide.
+
+#### Custom etcd Discovery Service
+
+Discovery uses an existing cluster to bootstrap itself. If you are using your own etcd cluster you can create a URL like so:
+
+```
+$ curl -X PUT https://myetcd.local/v2/keys/discovery/6c007a14875d53d9bf0ef5a6fc0257c817f0fb83/_config/size -d value=3
+```
+
+By setting the size key to the URL, you create a discovery URL with an expected cluster size of 3.
+
+The URL you will use in this case will be `https://myetcd.local/v2/keys/discovery/6c007a14875d53d9bf0ef5a6fc0257c817f0fb83` and the etcd members will use the `https://myetcd.local/v2/keys/discovery/6c007a14875d53d9bf0ef5a6fc0257c817f0fb83` directory for registration as they start.
+
+**Each member must have a different name flag specified. `Hostname` or `machine-id` can be a good choice. Or discovery will fail due to duplicated name.**
+
+Now we start etcd with those relevant flags for each member:
+
+```
+$ etcd --name infra0 --initial-advertise-peer-urls http://10.0.1.10:2380 \
+  --listen-peer-urls http://10.0.1.10:2380 \
+  --listen-client-urls http://10.0.1.10:2379,http://127.0.0.1:2379 \
+  --advertise-client-urls http://10.0.1.10:2379 \
+  --discovery https://myetcd.local/v2/keys/discovery/6c007a14875d53d9bf0ef5a6fc0257c817f0fb83
+```
+```
+$ etcd --name infra1 --initial-advertise-peer-urls http://10.0.1.11:2380 \
+  --listen-peer-urls http://10.0.1.11:2380 \
+  --listen-client-urls http://10.0.1.11:2379,http://127.0.0.1:2379 \
+  --advertise-client-urls http://10.0.1.11:2379 \
+  --discovery https://myetcd.local/v2/keys/discovery/6c007a14875d53d9bf0ef5a6fc0257c817f0fb83
+```
+```
+$ etcd --name infra2 --initial-advertise-peer-urls http://10.0.1.12:2380 \
+  --listen-peer-urls http://10.0.1.12:2380 \
+  --listen-client-urls http://10.0.1.12:2379,http://127.0.0.1:2379 \
+  --advertise-client-urls http://10.0.1.12:2379 \
+  --discovery https://myetcd.local/v2/keys/discovery/6c007a14875d53d9bf0ef5a6fc0257c817f0fb83
+```
+
+This will cause each member to register itself with the custom etcd discovery service and begin the cluster once all machines have been registered.
+
+#### Public etcd Discovery Service
+
+If you do not have access to an existing cluster, you can use the public discovery service hosted at `discovery.etcd.io`.  You can create a private discovery URL using the "new" endpoint like so:
+
+```
+$ curl https://discovery.etcd.io/new?size=3
+https://discovery.etcd.io/3e86b59982e49066c5d813af1c2e2579cbf573de
+```
+
+This will create the cluster with an initial expected size of 3 members. If you do not specify a size, a default of 3 will be used.
+
+```
+ETCD_DISCOVERY=https://discovery.etcd.io/3e86b59982e49066c5d813af1c2e2579cbf573de
+```
+
+```
+--discovery https://discovery.etcd.io/3e86b59982e49066c5d813af1c2e2579cbf573de
+```
+
+**Each member must have a different name flag specified. `Hostname` or `machine-id` can be a good choice. Or discovery will fail due to duplicated name.**
+
+Now we start etcd with those relevant flags for each member:
+
+```
+$ etcd --name infra0 --initial-advertise-peer-urls http://10.0.1.10:2380 \
+  --listen-peer-urls http://10.0.1.10:2380 \
+  --listen-client-urls http://10.0.1.10:2379,http://127.0.0.1:2379 \
+  --advertise-client-urls http://10.0.1.10:2379 \
+  --discovery https://discovery.etcd.io/3e86b59982e49066c5d813af1c2e2579cbf573de
+```
+```
+$ etcd --name infra1 --initial-advertise-peer-urls http://10.0.1.11:2380 \
+  --listen-peer-urls http://10.0.1.11:2380 \
+  --listen-client-urls http://10.0.1.11:2379,http://127.0.0.1:2379 \
+  --advertise-client-urls http://10.0.1.11:2379 \
+  --discovery https://discovery.etcd.io/3e86b59982e49066c5d813af1c2e2579cbf573de
+```
+```
+$ etcd --name infra2 --initial-advertise-peer-urls http://10.0.1.12:2380 \
+  --listen-peer-urls http://10.0.1.12:2380 \
+  --listen-client-urls http://10.0.1.12:2379,http://127.0.0.1:2379 \
+  --advertise-client-urls http://10.0.1.12:2379 \
+  --discovery https://discovery.etcd.io/3e86b59982e49066c5d813af1c2e2579cbf573de
+```
+
+This will cause each member to register itself with the discovery service and begin the cluster once all members have been registered.
+
+You can use the environment variable `ETCD_DISCOVERY_PROXY` to cause etcd to use an HTTP proxy to connect to the discovery service.
+
+#### Error and Warning Cases
+
+##### Discovery Server Errors
+
+
+```
+$ etcd --name infra0 --initial-advertise-peer-urls http://10.0.1.10:2380 \
+  --listen-peer-urls http://10.0.1.10:2380 \
+  --listen-client-urls http://10.0.1.10:2379,http://127.0.0.1:2379 \
+  --advertise-client-urls http://10.0.1.10:2379 \
+  --discovery https://discovery.etcd.io/3e86b59982e49066c5d813af1c2e2579cbf573de
+etcd: error: the cluster doesnâ€™t have a size configuration value in https://discovery.etcd.io/3e86b59982e49066c5d813af1c2e2579cbf573de/_config
+exit 1
+```
+
+##### Warnings
+
+This is a harmless warning notifying you that the discovery URL will be
+ignored on this machine.
+
+```
+$ etcd --name infra0 --initial-advertise-peer-urls http://10.0.1.10:2380 \
+  --listen-peer-urls http://10.0.1.10:2380 \
+  --listen-client-urls http://10.0.1.10:2379,http://127.0.0.1:2379 \
+  --advertise-client-urls http://10.0.1.10:2379 \
+  --discovery https://discovery.etcd.io/3e86b59982e49066c5d813af1c2e2579cbf573de
+etcdserver: discovery token ignored since a cluster has already been initialized. Valid log found at /var/lib/etcd
+```
+
+### DNS Discovery
+
+DNS [SRV records][rfc-srv] can be used as a discovery mechanism.
+The `-discovery-srv` flag can be used to set the DNS domain name where the discovery SRV records can be found.
+The following DNS SRV records are looked up in the listed order:
+
+* _etcd-server-ssl._tcp.example.com
+* _etcd-server._tcp.example.com
+
+If `_etcd-server-ssl._tcp.example.com` is found then etcd will attempt the bootstrapping process over SSL.
+
+To help clients discover the etcd cluster, the following DNS SRV records are looked up in the listed order:
+
+* _etcd-client._tcp.example.com
+* _etcd-client-ssl._tcp.example.com
+
+If `_etcd-client-ssl._tcp.example.com` is found, clients will attempt to communicate with the etcd cluster over SSL.
+
+#### Create DNS SRV records
+
+```
+$ dig +noall +answer SRV _etcd-server._tcp.example.com
+_etcd-server._tcp.example.com. 300 IN  SRV  0 0 2380 infra0.example.com.
+_etcd-server._tcp.example.com. 300 IN  SRV  0 0 2380 infra1.example.com.
+_etcd-server._tcp.example.com. 300 IN  SRV  0 0 2380 infra2.example.com.
+```
+
+```
+$ dig +noall +answer SRV _etcd-client._tcp.example.com
+_etcd-client._tcp.example.com. 300 IN SRV 0 0 2379 infra0.example.com.
+_etcd-client._tcp.example.com. 300 IN SRV 0 0 2379 infra1.example.com.
+_etcd-client._tcp.example.com. 300 IN SRV 0 0 2379 infra2.example.com.
+```
+
+```
+$ dig +noall +answer infra0.example.com infra1.example.com infra2.example.com
+infra0.example.com.  300  IN  A  10.0.1.10
+infra1.example.com.  300  IN  A  10.0.1.11
+infra2.example.com.  300  IN  A  10.0.1.12
+```
+#### Bootstrap the etcd cluster using DNS
+
+etcd cluster members can listen on domain names or IP address, the bootstrap process will resolve DNS A records.
+
+The resolved address in `--initial-advertise-peer-urls` *must match* one of the resolved addresses in the SRV targets. The etcd member reads the resolved address to find out if it belongs to the cluster defined in the SRV records.
+
+```
+$ etcd --name infra0 \
+--discovery-srv example.com \
+--initial-advertise-peer-urls http://infra0.example.com:2380 \
+--initial-cluster-token etcd-cluster-1 \
+--initial-cluster-state new \
+--advertise-client-urls http://infra0.example.com:2379 \
+--listen-client-urls http://infra0.example.com:2379 \
+--listen-peer-urls http://infra0.example.com:2380
+```
+
+```
+$ etcd --name infra1 \
+--discovery-srv example.com \
+--initial-advertise-peer-urls http://infra1.example.com:2380 \
+--initial-cluster-token etcd-cluster-1 \
+--initial-cluster-state new \
+--advertise-client-urls http://infra1.example.com:2379 \
+--listen-client-urls http://infra1.example.com:2379 \
+--listen-peer-urls http://infra1.example.com:2380
+```
+
+```
+$ etcd --name infra2 \
+--discovery-srv example.com \
+--initial-advertise-peer-urls http://infra2.example.com:2380 \
+--initial-cluster-token etcd-cluster-1 \
+--initial-cluster-state new \
+--advertise-client-urls http://infra2.example.com:2379 \
+--listen-client-urls http://infra2.example.com:2379 \
+--listen-peer-urls http://infra2.example.com:2380
+```
+
+You can also bootstrap the cluster using IP addresses instead of domain names:
+
+```
+$ etcd --name infra0 \
+--discovery-srv example.com \
+--initial-advertise-peer-urls http://10.0.1.10:2380 \
+--initial-cluster-token etcd-cluster-1 \
+--initial-cluster-state new \
+--advertise-client-urls http://10.0.1.10:2379 \
+--listen-client-urls http://10.0.1.10:2379 \
+--listen-peer-urls http://10.0.1.10:2380
+```
+
+```
+$ etcd --name infra1 \
+--discovery-srv example.com \
+--initial-advertise-peer-urls http://10.0.1.11:2380 \
+--initial-cluster-token etcd-cluster-1 \
+--initial-cluster-state new \
+--advertise-client-urls http://10.0.1.11:2379 \
+--listen-client-urls http://10.0.1.11:2379 \
+--listen-peer-urls http://10.0.1.11:2380
+```
+
+```
+$ etcd --name infra2 \
+--discovery-srv example.com \
+--initial-advertise-peer-urls http://10.0.1.12:2380 \
+--initial-cluster-token etcd-cluster-1 \
+--initial-cluster-state new \
+--advertise-client-urls http://10.0.1.12:2379 \
+--listen-client-urls http://10.0.1.12:2379 \
+--listen-peer-urls http://10.0.1.12:2380
+```
+
+### Proxy
+
+When the `--proxy` flag is set, etcd runs in [proxy mode][proxy]. This proxy mode only supports the etcd v2 API; there are no plans to support the v3 API. Instead, for v3 API support, there will be a new proxy with enhanced features following the etcd 3.0 release.
+
+To setup an etcd cluster with proxies of v2 API, please read the the [clustering doc in etcd 2.3 release][clustering_etcd2].
+
+[conf-adv-client]: configuration.md#-advertise-client-urls
+[conf-listen-client]: configuration.md#-listen-client-urls
+[discovery-proto]: dev-internal/discovery_protocol.md
+[rfc-srv]: http://www.ietf.org/rfc/rfc2052.txt
+[runtime-conf]: runtime-configuration.md
+[runtime-reconf-design]: runtime-reconf-design.md
+[proxy]: https://github.com/coreos/etcd/blob/release-2.3/Documentation/proxy.md
+[clustering_etcd2]: https://github.com/coreos/etcd/blob/release-2.3/Documentation/clustering.md
diff --git a/Documentation/op-guide/configuration.md b/Documentation/op-guide/configuration.md
new file mode 100644
index 0000000..99817aa
--- /dev/null
+++ b/Documentation/op-guide/configuration.md
@@ -0,0 +1,275 @@
+# Configuration Flags
+
+etcd is configurable through command-line flags and environment variables. Options set on the command line take precedence over those from the environment.
+
+The format of environment variable for flag `--my-flag` is `ETCD_MY_FLAG`. It applies to all  flags.
+
+The [official etcd ports][iana-ports] are 2379 for client requests, and 2380 for peer communication. Some legacy code and documentation still references ports 4001 and 7001, but all new etcd use and discussion should adopt the assigned ports.
+
+To start etcd automatically using custom settings at startup in Linux, using a [systemd][systemd-intro] unit is highly recommended.
+
+[systemd-intro]: http://freedesktop.org/wiki/Software/systemd/
+
+## Member Flags
+
+### --name
++ Human-readable name for this member.
++ default: "default"
++ env variable: ETCD_NAME
++ This value is referenced as this node's own entries listed in the `--initial-cluster` flag (Ex: `default=http://localhost:2380` or `default=http://localhost:2380,default=http://localhost:7001`). This needs to match the key used in the flag if you're using [static bootstrapping][build-cluster]. When using discovery, each member must have a unique name. `Hostname` or `machine-id` can be a good choice.
+
+### --data-dir
++ Path to the data directory.
++ default: "${name}.etcd"
++ env variable: ETCD_DATA_DIR
+
+### --wal-dir
++ Path to the dedicated wal directory. If this flag is set, etcd will write the WAL files to the walDir rather than the dataDir. This allows a dedicated disk to be used, and helps avoid io competition between logging and other IO operations.
++ default: ""
++ env variable: ETCD_WAL_DIR
+
+### --snapshot-count
++ Number of committed transactions to trigger a snapshot to disk.
++ default: "10000"
++ env variable: ETCD_SNAPSHOT_COUNT
+
+### --heartbeat-interval
++ Time (in milliseconds) of a heartbeat interval.
++ default: "100"
++ env variable: ETCD_HEARTBEAT_INTERVAL
+
+### --election-timeout
++ Time (in milliseconds) for an election to timeout. See [Documentation/tuning.md](tuning.md#time-parameters) for details.
++ default: "1000"
++ env variable: ETCD_ELECTION_TIMEOUT
+
+### --listen-peer-urls
++ List of URLs to listen on for peer traffic. This flag tells the etcd to accept incoming requests from its peers on the specified scheme://IP:port combinations. Scheme can be either http or https.If 0.0.0.0 is specified as the IP, etcd listens to the given port on all interfaces. If an IP address is given as well as a port, etcd will listen on the given port and interface. Multiple URLs may be used to specify a number of addresses and ports to listen on. The etcd will respond to requests from any of the listed addresses and ports.
++ default: "http://localhost:2380,http://localhost:7001"
++ env variable: ETCD_LISTEN_PEER_URLS
++ example: "http://10.0.0.1:2380"
++ invalid example: "http://example.com:2380" (domain name is invalid for binding)
+
+### --listen-client-urls
++ List of URLs to listen on for client traffic. This flag tells the etcd to accept incoming requests from the clients on the specified scheme://IP:port combinations. Scheme can be either http or https. If 0.0.0.0 is specified as the IP, etcd listens to the given port on all interfaces. If an IP address is given as well as a port, etcd will listen on the given port and interface. Multiple URLs may be used to specify a number of addresses and ports to listen on. The etcd will respond to requests from any of the listed addresses and ports.
++ default: "http://localhost:2379,http://localhost:4001"
++ env variable: ETCD_LISTEN_CLIENT_URLS
++ example: "http://10.0.0.1:2379"
++ invalid example: "http://example.com:2379" (domain name is invalid for binding)
+
+### --max-snapshots
++ Maximum number of snapshot files to retain (0 is unlimited)
++ default: 5
++ env variable: ETCD_MAX_SNAPSHOTS
++ The default for users on Windows is unlimited, and manual purging down to 5 (or your preference for safety) is recommended.
+
+### --max-wals
++ Maximum number of wal files to retain (0 is unlimited)
++ default: 5
++ env variable: ETCD_MAX_WALS
++ The default for users on Windows is unlimited, and manual purging down to 5 (or your preference for safety) is recommended.
+
+### --cors
++ Comma-separated white list of origins for CORS (cross-origin resource sharing).
++ default: none
++ env variable: ETCD_CORS
+
+## Clustering Flags
+
+`--initial` prefix flags are used in bootstrapping ([static bootstrap][build-cluster], [discovery-service bootstrap][discovery] or [runtime reconfiguration][reconfig]) a new member, and ignored when restarting an existing member.
+
+`--discovery` prefix flags need to be set when using [discovery service][discovery].
+
+### --initial-advertise-peer-urls
+
++ List of this member's peer URLs to advertise to the rest of the cluster. These addresses are used for communicating etcd data around the cluster. At least one must be routable to all cluster members. These URLs can contain domain names.
++ default: "http://localhost:2380,http://localhost:7001"
++ env variable: ETCD_INITIAL_ADVERTISE_PEER_URLS
++ example: "http://example.com:2380, http://10.0.0.1:2380"
+
+### --initial-cluster
++ Initial cluster configuration for bootstrapping.
++ default: "default=http://localhost:2380,default=http://localhost:7001"
++ env variable: ETCD_INITIAL_CLUSTER
++ The key is the value of the `--name` flag for each node provided. The default uses `default` for the key because this is the default for the `--name` flag.
+
+### --initial-cluster-state
++ Initial cluster state ("new" or "existing"). Set to `new` for all members present during initial static or DNS bootstrapping. If this option is set to `existing`, etcd will attempt to join the existing cluster. If the wrong value is set, etcd will attempt to start but fail safely.
++ default: "new"
++ env variable: ETCD_INITIAL_CLUSTER_STATE
+
+[static bootstrap]: clustering.md#static
+
+### --initial-cluster-token
++ Initial cluster token for the etcd cluster during bootstrap.
++ default: "etcd-cluster"
++ env variable: ETCD_INITIAL_CLUSTER_TOKEN
+
+### --advertise-client-urls
++ List of this member's client URLs to advertise to the rest of the cluster. These URLs can contain domain names.
++ default: "http://localhost:2379,http://localhost:4001"
++ env variable: ETCD_ADVERTISE_CLIENT_URLS
++ example: "http://example.com:2379, http://10.0.0.1:2379"
++ Be careful if you are advertising URLs such as http://localhost:2379 from a cluster member and are using the proxy feature of etcd. This will cause loops, because the proxy will be forwarding requests to itself until its resources (memory, file descriptors) are eventually depleted.
+
+### --discovery
++ Discovery URL used to bootstrap the cluster.
++ default: none
++ env variable: ETCD_DISCOVERY
+
+### --discovery-srv
++ DNS srv domain used to bootstrap the cluster.
++ default: none
++ env variable: ETCD_DISCOVERY_SRV
+
+### --discovery-fallback
++ Expected behavior ("exit" or "proxy") when discovery services fails.
++ default: "proxy"
++ env variable: ETCD_DISCOVERY_FALLBACK
+
+### --discovery-proxy
++ HTTP proxy to use for traffic to discovery service.
++ default: none
++ env variable: ETCD_DISCOVERY_PROXY
+
+### --strict-reconfig-check
++ Reject reconfiguration requests that would cause quorum loss.
++ default: false
++ env variable: ETCD_STRICT_RECONFIG_CHECK
+
+## Proxy Flags
+
+`--proxy` prefix flags configures etcd to run in [proxy mode][proxy].
+
+### --proxy
++ Proxy mode setting ("off", "readonly" or "on").
++ default: "off"
++ env variable: ETCD_PROXY
+
+### --proxy-failure-wait
++ Time (in milliseconds) an endpoint will be held in a failed state before being reconsidered for proxied requests.
++ default: 5000
++ env variable: ETCD_PROXY_FAILURE_WAIT
+
+### --proxy-refresh-interval
++ Time (in milliseconds) of the endpoints refresh interval.
++ default: 30000
++ env variable: ETCD_PROXY_REFRESH_INTERVAL
+
+### --proxy-dial-timeout
++ Time (in milliseconds) for a dial to timeout or 0 to disable the timeout
++ default: 1000
++ env variable: ETCD_PROXY_DIAL_TIMEOUT
+
+### --proxy-write-timeout
++ Time (in milliseconds) for a write to timeout or 0 to disable the timeout.
++ default: 5000
++ env variable: ETCD_PROXY_WRITE_TIMEOUT
+
+### --proxy-read-timeout
++ Time (in milliseconds) for a read to timeout or 0 to disable the timeout.
++ Don't change this value if you use watches because they are using long polling requests.
++ default: 0
++ env variable: ETCD_PROXY_READ_TIMEOUT
+
+## Security Flags
+
+The security flags help to [build a secure etcd cluster][security].
+
+### --ca-file [DEPRECATED]
++ Path to the client server TLS CA file. `--ca-file ca.crt` could be replaced by `--trusted-ca-file ca.crt --client-cert-auth` and etcd will perform the same.
++ default: none
++ env variable: ETCD_CA_FILE
+
+### --cert-file
++ Path to the client server TLS cert file.
++ default: none
++ env variable: ETCD_CERT_FILE
+
+### --key-file
++ Path to the client server TLS key file.
++ default: none
++ env variable: ETCD_KEY_FILE
+
+### --client-cert-auth
++ Enable client cert authentication.
++ default: false
++ env variable: ETCD_CLIENT_CERT_AUTH
+
+### --trusted-ca-file
++ Path to the client server TLS trusted CA key file.
++ default: none
++ env variable: ETCD_TRUSTED_CA_FILE
+
+### --peer-ca-file [DEPRECATED]
++ Path to the peer server TLS CA file. `--peer-ca-file ca.crt` could be replaced by `--peer-trusted-ca-file ca.crt --peer-client-cert-auth` and etcd will perform the same.
++ default: none
++ env variable: ETCD_PEER_CA_FILE
+
+### --peer-cert-file
++ Path to the peer server TLS cert file.
++ default: none
++ env variable: ETCD_PEER_CERT_FILE
+
+### --peer-key-file
++ Path to the peer server TLS key file.
++ default: none
++ env variable: ETCD_PEER_KEY_FILE
+
+### --peer-client-cert-auth
++ Enable peer client cert authentication.
++ default: false
++ env variable: ETCD_PEER_CLIENT_CERT_AUTH
+
+### --peer-trusted-ca-file
++ Path to the peer server TLS trusted CA file.
++ default: none
++ env variable: ETCD_PEER_TRUSTED_CA_FILE
+
+## Logging Flags
+
+### --debug
++ Drop the default log level to DEBUG for all subpackages.
++ default: false (INFO for all packages)
++ env variable: ETCD_DEBUG
+
+### --log-package-levels
++ Set individual etcd subpackages to specific log levels. An example being `etcdserver=WARNING,security=DEBUG` 
++ default: none (INFO for all packages)
++ env variable: ETCD_LOG_PACKAGE_LEVELS
+
+
+## Unsafe Flags
+
+Please be CAUTIOUS when using unsafe flags because it will break the guarantees given by the consensus protocol.
+For example, it may panic if other members in the cluster are still alive.
+Follow the instructions when using these flags.
+
+### --force-new-cluster
++ Force to create a new one-member cluster. It commits configuration changes forcing to remove all existing members in the cluster and add itself. It needs to be set to [restore a backup][restore].
++ default: false
++ env variable: ETCD_FORCE_NEW_CLUSTER
+
+## Miscellaneous Flags
+
+### --version
++ Print the version and exit.
++ default: false
+
+## Profiling flags
+
+### --enable-pprof
++ Enable runtime profiling data via HTTP server. Address is at client URL + "/debug/pprof"
++ default: false
+
+[build-cluster]: clustering.md#static
+[reconfig]: runtime-configuration.md
+[discovery]: clustering.md#discovery
+[iana-ports]: https://www.iana.org/assignments/service-names-port-numbers/service-names-port-numbers.xhtml?search=etcd
+[proxy]: proxy.md
+[reconfig]: runtime-configuration.md
+[restore]: admin_guide.md#restoring-a-backup
+[rfc-v3]: rfc/v3api.md
+[security]: security.md
+[systemd-intro]: http://freedesktop.org/wiki/Software/systemd/
+[tuning]: tuning.md#time-parameters
diff --git a/Documentation/op-guide/maintenance.md b/Documentation/op-guide/maintenance.md
new file mode 100644
index 0000000..e416ff0
--- /dev/null
+++ b/Documentation/op-guide/maintenance.md
@@ -0,0 +1,115 @@
+# Maintenance
+
+## Overview
+
+An etcd cluster needs periodic maintenance to remain reliable. Depending on an etcd application's needs, this maintenance can usually be automated and performed without downtime or significantly degraded performance.
+
+All etcd maintenance manages storage resources consumed by the etcd keyspace. Failure to adequately control the keyspace size is guarded by storage space quotas; if an etcd member runs low on space, a quota will trigger cluster-wide alarms which will put the system into a limited-operation maintenance mode. To avoid running out of space for writes to the keyspace, the etcd keyspace history must be compacted. Storage space itself may be reclaimed by defragmenting etcd members. Finally, periodic snapshot backups of etcd member state makes it possible to recover any unintended logical data loss or corruption caused by operational error.
+
+## History compaction
+
+Since etcd keeps an exact history of its keyspace, this history should be periodically compacted to avoid performance degradation and eventual storage space exhaustion. Compacting the keyspace history drops all information about keys superseded prior to a given keyspace revision. The space used by these keys then becomes available for additional writes to the keyspace.
+
+The keyspace can be compacted automatically with `etcd`'s time windowed history retention policy, or manually with `etcdctl`. The `etcdctl` method provides fine-grained control over the compacting process whereas automatic compacting fits applications that only need key history for some length of time.
+
+`etcd` can be set to automatically compact the keyspace with the `--experimental-auto-compaction` option with a period of hours:
+
+```sh
+# keep one hour of history
+$ etcd --experimental-auto-compaction-retention=1
+```
+
+An `etcdctl` initiated compaction works as follows:
+
+```sh
+# compact up to revision 3
+$ etcdctl compact 3
+
+```
+
+Revisions prior to the compaction revision become inaccessible:
+
+```sh
+$ etcdctl get --rev=2 somekey
+Error:  rpc error: code = 11 desc = etcdserver: storage: required revision has been compacted
+```
+
+## Defragmentation
+
+After compacting the keyspace, the backend database may exhibit internal fragmentation. Any internal fragmentation is space that is free to use by the backend but still consumes storage space. The process of defragmentation releases this storage space back to the file system. Defragmentation is issued on a per-member so that cluster-wide latency spikes may be avoided.
+
+Compacting old revisions internally fragments `etcd` by leaving gaps in backend database. Fragmented space is available for use by `etcd` but unavailable to the host filesystem.
+
+To defragment an etcd member, use the `etcdctl defrag` command:
+
+```sh
+$ etcdctl defrag
+Finished defragmenting etcd member[127.0.0.1:2379]
+```
+
+## Space quota
+
+The space quota in `etcd` ensures the cluster operates in a reliable fashion. Without a space quota, `etcd` may suffer from poor performance if the keyspace grows excessively large, or it may simply run out of storage space, leading to unpredictable cluster behavior. If the keyspace's backend database for any member exceeds the space quota, `etcd` raises a cluster-wide alarm that puts the cluster into a maintenance mode which only accepts key reads and deletes. After freeing enough space in the keyspace, the alarm can be disarmed and the cluster will resume normal operation.
+
+By default, `etcd` sets a conservative space quota suitable for most applications, but it may be configured on the command line, in bytes:
+
+```sh
+# set a very small 16MB quota
+$ etcd --quota-backend-bytes=16777216
+```
+
+The space quota can be triggered with a loop:
+
+```sh
+# fill keyspace
+$ while [ 1 ]; do dd if=/dev/urandom bs=1024 count=1024  | etcdctl put key  || break; done
+...
+Error:  rpc error: code = 8 desc = etcdserver: storage: database space exceeded
+# confirm quota space is exceeded
+$ etcdctl endpoint status
++----------------+------------------+-----------+---------+-----------+-----------+------------+
+|    ENDPOINT    |        ID        |  VERSION  | DB SIZE | IS LEADER | RAFT TERM | RAFT INDEX |
++----------------+------------------+-----------+---------+-----------+-----------+------------+
+| 127.0.0.1:2379 | bf9071f4639c75cc | 2.3.0+git | 18 MB   | true      |         2 |       3332 |
++----------------+------------------+-----------+---------+-----------+-----------+------------+
+# confirm alarm is raised
+$ etcdctl alarm list
+memberID:13803658152347727308 alarm:NOSPACE 
+```
+
+Removing excessive keyspace data will put the cluster back within the quota limits so the alarm can be disarmed:
+
+```sh
+# get current revision
+$ etcdctl --endpoints=:2379 endpoint status
+[{"Endpoint":"127.0.0.1:2379","Status":{"header":{"cluster_id":8925027824743593106,"member_id":13803658152347727308,"revision":1516,"raft_term":2},"version":"2.3.0+git","dbSize":17973248,"leader":13803658152347727308,"raftIndex":6359,"raftTerm":2}}]
+# compact away all old revisions
+$ etdctl compact 1516
+compacted revision 1516
+# defragment away excessive space
+$ etcdctl defrag
+Finished defragmenting etcd member[127.0.0.1:2379]
+# disarm alarm
+$ etcdctl alarm disarm
+memberID:13803658152347727308 alarm:NOSPACE 
+# test puts are allowed again
+$ etdctl put newkey 123
+OK
+```
+
+## Snapshot backup
+
+Snapshotting the `etcd` cluster on a regular basis serves as a durable backup for an etcd keyspace. By taking periodic snapshots of an etcd member's backend database, an `etcd` cluster can be recovered to a point in time with a known good state.
+
+A snapshot is taken with `etcdctl`:
+
+```sh
+$ etcdctl snapshot save backup.db
+$ etcdctl snapshot status backup.db
++----------+----------+------------+------------+
+|   HASH   | REVISION | TOTAL KEYS | TOTAL SIZE |
++----------+----------+------------+------------+
+| fe01cf57 |       10 |          7 | 2.1 MB     |
++----------+----------+------------+------------+
+
+```
diff --git a/Documentation/op-guide/recovery.md b/Documentation/op-guide/recovery.md
new file mode 100644
index 0000000..2788630
--- /dev/null
+++ b/Documentation/op-guide/recovery.md
@@ -0,0 +1,61 @@
+## Disaster recovery
+
+etcd is designed to withstand machine failures. An etcd cluster automatically recovers from temporary failures (e.g., machine reboots) and tolerates up to *(N-1)/2* permanent failures for a cluster of N members. When a member permanently fails, whether due to hardware failure or disk corruption, it loses access to the cluster. If the cluster permanently loses more than *(N-1)/2* members then it disastrously fails, irrevocably losing quorum. Once quorum is lost, the cluster cannot reach consensus and therefore cannot continue accepting updates.
+
+To recover from disastrous failure, etcd provides snapshot and restore facilities to recreate the cluster without data loss.
+
+TODO(xiangli): add note to clarify this only recovers for the kv store of etcd3.
+
+### Snapshotting the keyspace
+
+Recovering a cluster first needs a snapshot of the keyspace from an etcd member. A snapshot may either be taken from a live member with the `etcdctl snapshot save` command or by copying the `member/snap/db` file from an etcd data directory. For example, the following command snapshots the keyspace served by `$ENDPOINT` to the file `snapshot.db`:
+
+```sh
+$ etcdctl --endpoints $ENDPOINT snapshot save snapshot.db
+```
+
+### Restoring a cluster
+
+To restore a cluster, all that is needed is a single snapshot "db" file. A cluster restore with `etcdctl snapshot restore` creates new etcd data directories; all members should restore using the same snapshot. Restoring overwrites some snapshot metadata (specifically, the member ID and cluster ID); the member loses its former identity. This metadata overwrite prevents the new member from inadvertently joining an existing cluster. Therefore in order to start a cluster from a snapshot, the restore must start a new logical cluster.
+
+A restore initializes a new member of a new cluster, with a fresh cluster configuration using `etcd`'s cluster configuration flags, but preserves the contents of the etcd keyspace. Continuing from the previous example, the following creates new etcd data directories (`m1.etcd`, `m2.etcd`, `m3.etcd`) for a three member cluster:
+
+```sh
+$ etcdctl snapshot restore snapshot.db \
+  --name m1 \
+  --initial-cluster m1=http:/host1:2380,m2=http://host2:2380,m3=http://host3:2380 \
+  --initial-cluster-token etcd-cluster-1 \
+  --initial-advertise-peer-urls http://host1:2380
+$ etcdctl snapshot restore snapshot.db \
+  --name m2 \
+  --initial-cluster m1=http:/host1:2380,m2=http://host2:2380,m3=http://host3:2380 \
+  --initial-cluster-token etcd-cluster-1 \
+  --initial-advertise-peer-urls http://host2:2380
+$ etcdctl snapshot restore snapshot.db \
+  --name m3 \
+  --initial-cluster m1=http:/host1:2380,m2=http://host2:2380,m3=http://host3:2380 \
+  --initial-cluster-token etcd-cluster-1 \
+  --initial-advertise-peer-urls http://host3:2380
+```
+
+Next, start `etcd` with the new data directories:
+
+```sh
+$ etcd \
+  --name m1 \
+  --listen-client-urls http://host1:2379 \
+  --advertise-client-urls http://host1:2379 \
+  --listen-peer-urls http://host1:2380 &
+$ etcd \
+  --name m2 \
+  --listen-client-urls http://host2:2379 \
+  --advertise-client-urls http://host2:2379 \
+  --listen-peer-urls http://host2:2380 &
+$ etcd \
+  --name m3 \
+  --listen-client-urls http://host3:2379 \
+  --advertise-client-urls http://host3:2379 \
+  --listen-peer-urls http://host3:2380 &
+```
+
+Now the restored etcd cluster should be available and serving the keyspace given by the snapshot.
diff --git a/Documentation/op-guide/runtime-configuration.md b/Documentation/op-guide/runtime-configuration.md
new file mode 100644
index 0000000..ca5ce29
--- /dev/null
+++ b/Documentation/op-guide/runtime-configuration.md
@@ -0,0 +1,184 @@
+# Runtime Reconfiguration
+
+etcd comes with support for incremental runtime reconfiguration, which allows users to update the membership of the cluster at run time.
+
+Reconfiguration requests can only be processed when the majority of the cluster members are functioning. It is **highly recommended** to always have a cluster size greater than two in production. It is unsafe to remove a member from a two member cluster. The majority of a two member cluster is also two. If there is a failure during the removal process, the cluster might not able to make progress and need to [restart from majority failure][majority failure].
+
+To better understand the design behind runtime reconfiguration, we suggest you read [the runtime reconfiguration document][runtime-reconf].
+
+## Reconfiguration Use Cases
+
+Let's walk through some common reasons for reconfiguring a cluster. Most of these just involve combinations of adding or removing a member, which are explained below under [Cluster Reconfiguration Operations][cluster-reconf].
+
+### Cycle or Upgrade Multiple Machines
+
+If you need to move multiple members of your cluster due to planned maintenance (hardware upgrades, network downtime, etc.), it is recommended to modify members one at a time.
+
+It is safe to remove the leader, however there is a brief period of downtime while the election process takes place. If your cluster holds more than 50MB, it is recommended to [migrate the member's data directory][member migration].
+
+### Change the Cluster Size
+
+Increasing the cluster size can enhance [failure tolerance][fault tolerance table] and provide better read performance. Since clients can read from any member, increasing the number of members increases the overall read throughput.
+
+Decreasing the cluster size can improve the write performance of a cluster, with a trade-off of decreased resilience. Writes into the cluster are replicated to a majority of members of the cluster before considered committed. Decreasing the cluster size lowers the majority, and each write is committed more quickly.
+
+### Replace A Failed Machine
+
+If a machine fails due to hardware failure, data directory corruption, or some other fatal situation, it should be replaced as soon as possible. Machines that have failed but haven't been removed adversely affect your quorum and reduce the tolerance for an additional failure.
+
+To replace the machine, follow the instructions for [removing the member][remove member] from the cluster, and then [add a new member][add member] in its place. If your cluster holds more than 50MB, it is recommended to [migrate the failed member's data directory][member migration] if you can still access it.
+
+### Restart Cluster from Majority Failure
+
+If the majority of your cluster is lost or all of your nodes have changed IP addresses, then you need to take manual action in order to recover safely.
+The basic steps in the recovery process include [creating a new cluster using the old data][disaster recovery], forcing a single member to act as the leader, and finally using runtime configuration to [add new members][add member] to this new cluster one at a time.
+
+## Cluster Reconfiguration Operations
+
+Now that we have the use cases in mind, let us lay out the operations involved in each.
+
+Before making any change, the simple majority (quorum) of etcd members must be available.
+This is essentially the same requirement as for any other write to etcd.
+
+All changes to the cluster are done one at a time:
+
+* To update a single member peerURLs you will make an update operation
+* To replace a single member you will make an add then a remove operation
+* To increase from 3 to 5 members you will make two add operations
+* To decrease from 5 to 3 you will make two remove operations
+
+All of these examples will use the `etcdctl` command line tool that ships with etcd.
+If you want to use the members API directly you can find the documentation [here][member-api].
+
+### Update a Member
+
+#### Update advertise client URLs
+
+If you would like to update the advertise client URLs of a member, you can simply restart
+that member with updated client urls flag (`--advertise-client-urls`) or environment variable
+(`ETCD_ADVERTISE_CLIENT_URLS`). The restarted member will self publish the updated URLs.
+A wrongly updated client URL will not affect the health of the etcd cluster.
+
+#### Update advertise peer URLs
+
+If you would like to update the advertise peer URLs of a member, you have to first update 
+it explicitly via member command and then restart the member. The additional action is required
+since updating peer URLs changes the cluster wide configuration and can affect the health of the etcd cluster. 
+
+To update the peer URLs, first, we need to find the target member's ID. You can list all members with `etcdctl`:
+
+```sh
+$ etcdctl member list
+6e3bd23ae5f1eae0: name=node2 peerURLs=http://localhost:23802 clientURLs=http://127.0.0.1:23792
+924e2e83e93f2560: name=node3 peerURLs=http://localhost:23803 clientURLs=http://127.0.0.1:23793
+a8266ecf031671f3: name=node1 peerURLs=http://localhost:23801 clientURLs=http://127.0.0.1:23791
+```
+
+In this example let's `update` a8266ecf031671f3 member ID and change its peerURLs value to http://10.0.1.10:2380
+
+```sh
+$ etcdctl member update a8266ecf031671f3 http://10.0.1.10:2380
+Updated member with ID a8266ecf031671f3 in cluster
+```
+
+### Remove a Member
+
+Let us say the member ID we want to remove is a8266ecf031671f3.
+We then use the `remove` command to perform the removal:
+
+```sh
+$ etcdctl member remove a8266ecf031671f3
+Removed member a8266ecf031671f3 from cluster
+```
+
+The target member will stop itself at this point and print out the removal in the log:
+
+```
+etcd: this member has been permanently removed from the cluster. Exiting.
+```
+
+It is safe to remove the leader, however the cluster will be inactive while a new leader is elected. This duration is normally the period of election timeout plus the voting process.
+
+### Add a New Member
+
+Adding a member is a two step process:
+
+ * Add the new member to the cluster via the [members API][member-api] or the `etcdctl member add` command.
+ * Start the new member with the new cluster configuration, including a list of the updated members (existing members + the new member).
+
+Using `etcdctl` let's add the new member to the cluster by specifying its [name][conf-name] and [advertised peer URLs][conf-adv-peer]:
+
+```sh
+$ etcdctl member add infra3 http://10.0.1.13:2380
+added member 9bf1b35fc7761a23 to cluster
+
+ETCD_NAME="infra3"
+ETCD_INITIAL_CLUSTER="infra0=http://10.0.1.10:2380,infra1=http://10.0.1.11:2380,infra2=http://10.0.1.12:2380,infra3=http://10.0.1.13:2380"
+ETCD_INITIAL_CLUSTER_STATE=existing
+```
+
+`etcdctl` has informed the cluster about the new member and printed out the environment variables needed to successfully start it.
+Now start the new etcd process with the relevant flags for the new member:
+
+```sh
+$ export ETCD_NAME="infra3"
+$ export ETCD_INITIAL_CLUSTER="infra0=http://10.0.1.10:2380,infra1=http://10.0.1.11:2380,infra2=http://10.0.1.12:2380,infra3=http://10.0.1.13:2380"
+$ export ETCD_INITIAL_CLUSTER_STATE=existing
+$ etcd --listen-client-urls http://10.0.1.13:2379 --advertise-client-urls http://10.0.1.13:2379 --listen-peer-urls http://10.0.1.13:2380 --initial-advertise-peer-urls http://10.0.1.13:2380 --data-dir %data_dir%
+```
+
+The new member will run as a part of the cluster and immediately begin catching up with the rest of the cluster.
+
+If you are adding multiple members the best practice is to configure a single member at a time and verify it starts correctly before adding more new members.
+If you add a new member to a 1-node cluster, the cluster cannot make progress before the new member starts because it needs two members as majority to agree on the consensus. You will only see this behavior between the time `etcdctl member add` informs the cluster about the new member and the new member successfully establishing a connection to the existing one.
+
+#### Error Cases When Adding Members
+
+In the following case we have not included our new host in the list of enumerated nodes.
+If this is a new cluster, the node must be added to the list of initial cluster members.
+
+```sh
+$ etcd --name infra3 \
+  --initial-cluster infra0=http://10.0.1.10:2380,infra1=http://10.0.1.11:2380,infra2=http://10.0.1.12:2380 \
+  --initial-cluster-state existing
+etcdserver: assign ids error: the member count is unequal
+exit 1
+```
+
+In this case we give a different address (10.0.1.14:2380) to the one that we used to join the cluster (10.0.1.13:2380).
+
+```sh
+$ etcd --name infra4 \
+  --initial-cluster infra0=http://10.0.1.10:2380,infra1=http://10.0.1.11:2380,infra2=http://10.0.1.12:2380,infra4=http://10.0.1.14:2380 \
+  --initial-cluster-state existing
+etcdserver: assign ids error: unmatched member while checking PeerURLs
+exit 1
+```
+
+When we start etcd using the data directory of a removed member, etcd will exit automatically if it connects to any active member in the cluster:
+
+```sh
+$ etcd
+etcd: this member has been permanently removed from the cluster. Exiting.
+exit 1
+```
+
+### Strict Reconfiguration Check Mode (`-strict-reconfig-check`)
+
+As described in the above, the best practice of adding new members is to configure a single member at a time and verify it starts correctly before adding more new members. This step by step approach is very important because if newly added members is not configured correctly (for example the peer URLs are incorrect), the cluster can lose quorum. The quorum loss happens since the newly added member are counted in the quorum even if that member is not reachable from other existing members. Also quorum loss might happen if there is a connectivity issue or there are operational issues.
+
+For avoiding this problem, etcd provides an option `-strict-reconfig-check`. If this option is passed to etcd, etcd rejects reconfiguration requests if the number of started members will be less than a quorum of the reconfigured cluster.
+
+It is recommended to enable this option. However, it is disabled by default because of keeping compatibility.
+
+[add member]: #add-a-new-member
+[cluster-reconf]: #cluster-reconfiguration-operations
+[conf-adv-peer]: configuration.md#-initial-advertise-peer-urls
+[conf-name]: configuration.md#-name
+[disaster recovery]: admin_guide.md#disaster-recovery
+[fault tolerance table]: admin_guide.md#fault-tolerance-table
+[majority failure]: #restart-cluster-from-majority-failure
+[member-api]: members_api.md
+[member migration]: admin_guide.md#member-migration
+[remove member]: #remove-a-member
+[runtime-reconf]: runtime-reconf-design.md
diff --git a/Documentation/op-guide/runtime-reconf-design.md b/Documentation/op-guide/runtime-reconf-design.md
new file mode 100644
index 0000000..d577278
--- /dev/null
+++ b/Documentation/op-guide/runtime-reconf-design.md
@@ -0,0 +1,50 @@
+# Design of Runtime Reconfiguration
+
+Runtime reconfiguration is one of the hardest and most error prone features in a distributed system, especially in a consensus based system like etcd.
+
+Read on to learn about the design of etcd's runtime reconfiguration commands and how we tackled these problems.
+
+## Two Phase Config Changes Keep you Safe
+
+In etcd, every runtime reconfiguration has to go through [two phases][add-member] for safety reasons. For example, to add a member you need to first inform cluster of new configuration and then start the new member.
+
+Phase 1 - Inform cluster of new configuration
+
+To add a member into etcd cluster, you need to make an API call to request a new member to be added to the cluster. And this is only way that you can add a new member into an existing cluster. The API call returns when the cluster agrees on the configuration change.
+
+Phase 2 - Start new member
+
+To join the etcd member into the existing cluster, you need to specify the correct `initial-cluster` and set `initial-cluster-state` to `existing`. When the member starts, it will contact the existing cluster first and verify the current cluster configuration matches the expected one specified in `initial-cluster`. When the new member successfully starts, you know your cluster reached the expected configuration.
+
+By splitting the process into two discrete phases users are forced to be explicit regarding cluster membership changes. This actually gives users more flexibility and makes things easier to reason about. For example, if there is an attempt to add a new member with the same ID as an existing member in an etcd cluster, the action will fail immediately during phase one without impacting the running cluster. Similar protection is provided to prevent adding new members by mistake. If a new etcd member attempts to join the cluster before the cluster has accepted the configuration change,, it will not be accepted by the cluster.
+
+Without the explicit workflow around cluster membership etcd would be vulnerable to unexpected cluster membership changes. For example, if etcd is running under an init system such as systemd, etcd would be restarted after being removed via the membership API, and attempt to rejoin the cluster on startup. This cycle would continue every time a member is removed via the API and systemd is set to restart etcd after failing, which is unexpected.
+
+We think runtime reconfiguration should be a low frequent operation. We made the decision to keep it explicit and user-driven to ensure configuration safety and keep your cluster always running smoothly under your control.
+
+## Permanent Loss of Quorum Requires New Cluster
+
+If a cluster permanently loses a majority of its members, a new cluster will need to be started from an old data directory to recover the previous state.
+
+It is entirely possible to force removing the failed members from the existing cluster to recover. However, we decided not to support this method since it bypasses the normal consensus committing phase, which is unsafe. If the member to remove is not actually dead or you force to remove different members through different members in the same cluster, you will end up with diverged cluster with same clusterID. This is very dangerous and hard to debug/fix afterwards. 
+
+If you have a correct deployment, the possibility of permanent majority lose is very low. But it is a severe enough problem that worth special care. We strongly suggest you to read the [disaster recovery documentation][disaster-recovery] and prepare for permanent majority lose before you put etcd into production.
+
+## Do Not Use Public Discovery Service For Runtime Reconfiguration
+
+The public discovery service should only be used for bootstrapping a cluster. To join member into an existing cluster, you should use runtime reconfiguration API. 
+
+Discovery service is designed for bootstrapping an etcd cluster in the cloud environment, when you do not know the IP addresses of all the members beforehand. After you successfully bootstrap a cluster, the IP addresses of all the members are known. Technically, you should not need the discovery service any more.
+
+It seems that using public discovery service is a convenient way to do runtime reconfiguration, after all discovery service already has all the cluster configuration information. However relying on public discovery service brings troubles: 
+
+1. it introduces external dependencies for the entire life-cycle of your cluster, not just bootstrap time. If there is a network issue between your cluster and public discovery service, your cluster will suffer from it.
+ 
+2. public discovery service must reflect correct runtime configuration of your cluster during it life-cycle. It has to provide security mechanism to avoid bad actions, and it is hard. 
+
+3. public discovery service has to keep tens of thousands of cluster configurations. Our public discovery service backend is not ready for that workload.
+
+If you want to have a discovery service that supports runtime reconfiguration, the best choice is to build your private one.
+
+[add-member]: runtime-configuration.md#add-a-new-member
+[disaster-recovery]: admin_guide.md#disaster-recovery
diff --git a/Documentation/op_guide/maintenance.md b/Documentation/op_guide/maintenance.md
deleted file mode 100644
index e416ff0..0000000
--- a/Documentation/op_guide/maintenance.md
+++ /dev/null
@@ -1,115 +0,0 @@
-# Maintenance
-
-## Overview
-
-An etcd cluster needs periodic maintenance to remain reliable. Depending on an etcd application's needs, this maintenance can usually be automated and performed without downtime or significantly degraded performance.
-
-All etcd maintenance manages storage resources consumed by the etcd keyspace. Failure to adequately control the keyspace size is guarded by storage space quotas; if an etcd member runs low on space, a quota will trigger cluster-wide alarms which will put the system into a limited-operation maintenance mode. To avoid running out of space for writes to the keyspace, the etcd keyspace history must be compacted. Storage space itself may be reclaimed by defragmenting etcd members. Finally, periodic snapshot backups of etcd member state makes it possible to recover any unintended logical data loss or corruption caused by operational error.
-
-## History compaction
-
-Since etcd keeps an exact history of its keyspace, this history should be periodically compacted to avoid performance degradation and eventual storage space exhaustion. Compacting the keyspace history drops all information about keys superseded prior to a given keyspace revision. The space used by these keys then becomes available for additional writes to the keyspace.
-
-The keyspace can be compacted automatically with `etcd`'s time windowed history retention policy, or manually with `etcdctl`. The `etcdctl` method provides fine-grained control over the compacting process whereas automatic compacting fits applications that only need key history for some length of time.
-
-`etcd` can be set to automatically compact the keyspace with the `--experimental-auto-compaction` option with a period of hours:
-
-```sh
-# keep one hour of history
-$ etcd --experimental-auto-compaction-retention=1
-```
-
-An `etcdctl` initiated compaction works as follows:
-
-```sh
-# compact up to revision 3
-$ etcdctl compact 3
-
-```
-
-Revisions prior to the compaction revision become inaccessible:
-
-```sh
-$ etcdctl get --rev=2 somekey
-Error:  rpc error: code = 11 desc = etcdserver: storage: required revision has been compacted
-```
-
-## Defragmentation
-
-After compacting the keyspace, the backend database may exhibit internal fragmentation. Any internal fragmentation is space that is free to use by the backend but still consumes storage space. The process of defragmentation releases this storage space back to the file system. Defragmentation is issued on a per-member so that cluster-wide latency spikes may be avoided.
-
-Compacting old revisions internally fragments `etcd` by leaving gaps in backend database. Fragmented space is available for use by `etcd` but unavailable to the host filesystem.
-
-To defragment an etcd member, use the `etcdctl defrag` command:
-
-```sh
-$ etcdctl defrag
-Finished defragmenting etcd member[127.0.0.1:2379]
-```
-
-## Space quota
-
-The space quota in `etcd` ensures the cluster operates in a reliable fashion. Without a space quota, `etcd` may suffer from poor performance if the keyspace grows excessively large, or it may simply run out of storage space, leading to unpredictable cluster behavior. If the keyspace's backend database for any member exceeds the space quota, `etcd` raises a cluster-wide alarm that puts the cluster into a maintenance mode which only accepts key reads and deletes. After freeing enough space in the keyspace, the alarm can be disarmed and the cluster will resume normal operation.
-
-By default, `etcd` sets a conservative space quota suitable for most applications, but it may be configured on the command line, in bytes:
-
-```sh
-# set a very small 16MB quota
-$ etcd --quota-backend-bytes=16777216
-```
-
-The space quota can be triggered with a loop:
-
-```sh
-# fill keyspace
-$ while [ 1 ]; do dd if=/dev/urandom bs=1024 count=1024  | etcdctl put key  || break; done
-...
-Error:  rpc error: code = 8 desc = etcdserver: storage: database space exceeded
-# confirm quota space is exceeded
-$ etcdctl endpoint status
-+----------------+------------------+-----------+---------+-----------+-----------+------------+
-|    ENDPOINT    |        ID        |  VERSION  | DB SIZE | IS LEADER | RAFT TERM | RAFT INDEX |
-+----------------+------------------+-----------+---------+-----------+-----------+------------+
-| 127.0.0.1:2379 | bf9071f4639c75cc | 2.3.0+git | 18 MB   | true      |         2 |       3332 |
-+----------------+------------------+-----------+---------+-----------+-----------+------------+
-# confirm alarm is raised
-$ etcdctl alarm list
-memberID:13803658152347727308 alarm:NOSPACE 
-```
-
-Removing excessive keyspace data will put the cluster back within the quota limits so the alarm can be disarmed:
-
-```sh
-# get current revision
-$ etcdctl --endpoints=:2379 endpoint status
-[{"Endpoint":"127.0.0.1:2379","Status":{"header":{"cluster_id":8925027824743593106,"member_id":13803658152347727308,"revision":1516,"raft_term":2},"version":"2.3.0+git","dbSize":17973248,"leader":13803658152347727308,"raftIndex":6359,"raftTerm":2}}]
-# compact away all old revisions
-$ etdctl compact 1516
-compacted revision 1516
-# defragment away excessive space
-$ etcdctl defrag
-Finished defragmenting etcd member[127.0.0.1:2379]
-# disarm alarm
-$ etcdctl alarm disarm
-memberID:13803658152347727308 alarm:NOSPACE 
-# test puts are allowed again
-$ etdctl put newkey 123
-OK
-```
-
-## Snapshot backup
-
-Snapshotting the `etcd` cluster on a regular basis serves as a durable backup for an etcd keyspace. By taking periodic snapshots of an etcd member's backend database, an `etcd` cluster can be recovered to a point in time with a known good state.
-
-A snapshot is taken with `etcdctl`:
-
-```sh
-$ etcdctl snapshot save backup.db
-$ etcdctl snapshot status backup.db
-+----------+----------+------------+------------+
-|   HASH   | REVISION | TOTAL KEYS | TOTAL SIZE |
-+----------+----------+------------+------------+
-| fe01cf57 |       10 |          7 | 2.1 MB     |
-+----------+----------+------------+------------+
-
-```
diff --git a/Documentation/op_guide/recovery.md b/Documentation/op_guide/recovery.md
deleted file mode 100644
index 2788630..0000000
--- a/Documentation/op_guide/recovery.md
+++ /dev/null
@@ -1,61 +0,0 @@
-## Disaster recovery
-
-etcd is designed to withstand machine failures. An etcd cluster automatically recovers from temporary failures (e.g., machine reboots) and tolerates up to *(N-1)/2* permanent failures for a cluster of N members. When a member permanently fails, whether due to hardware failure or disk corruption, it loses access to the cluster. If the cluster permanently loses more than *(N-1)/2* members then it disastrously fails, irrevocably losing quorum. Once quorum is lost, the cluster cannot reach consensus and therefore cannot continue accepting updates.
-
-To recover from disastrous failure, etcd provides snapshot and restore facilities to recreate the cluster without data loss.
-
-TODO(xiangli): add note to clarify this only recovers for the kv store of etcd3.
-
-### Snapshotting the keyspace
-
-Recovering a cluster first needs a snapshot of the keyspace from an etcd member. A snapshot may either be taken from a live member with the `etcdctl snapshot save` command or by copying the `member/snap/db` file from an etcd data directory. For example, the following command snapshots the keyspace served by `$ENDPOINT` to the file `snapshot.db`:
-
-```sh
-$ etcdctl --endpoints $ENDPOINT snapshot save snapshot.db
-```
-
-### Restoring a cluster
-
-To restore a cluster, all that is needed is a single snapshot "db" file. A cluster restore with `etcdctl snapshot restore` creates new etcd data directories; all members should restore using the same snapshot. Restoring overwrites some snapshot metadata (specifically, the member ID and cluster ID); the member loses its former identity. This metadata overwrite prevents the new member from inadvertently joining an existing cluster. Therefore in order to start a cluster from a snapshot, the restore must start a new logical cluster.
-
-A restore initializes a new member of a new cluster, with a fresh cluster configuration using `etcd`'s cluster configuration flags, but preserves the contents of the etcd keyspace. Continuing from the previous example, the following creates new etcd data directories (`m1.etcd`, `m2.etcd`, `m3.etcd`) for a three member cluster:
-
-```sh
-$ etcdctl snapshot restore snapshot.db \
-  --name m1 \
-  --initial-cluster m1=http:/host1:2380,m2=http://host2:2380,m3=http://host3:2380 \
-  --initial-cluster-token etcd-cluster-1 \
-  --initial-advertise-peer-urls http://host1:2380
-$ etcdctl snapshot restore snapshot.db \
-  --name m2 \
-  --initial-cluster m1=http:/host1:2380,m2=http://host2:2380,m3=http://host3:2380 \
-  --initial-cluster-token etcd-cluster-1 \
-  --initial-advertise-peer-urls http://host2:2380
-$ etcdctl snapshot restore snapshot.db \
-  --name m3 \
-  --initial-cluster m1=http:/host1:2380,m2=http://host2:2380,m3=http://host3:2380 \
-  --initial-cluster-token etcd-cluster-1 \
-  --initial-advertise-peer-urls http://host3:2380
-```
-
-Next, start `etcd` with the new data directories:
-
-```sh
-$ etcd \
-  --name m1 \
-  --listen-client-urls http://host1:2379 \
-  --advertise-client-urls http://host1:2379 \
-  --listen-peer-urls http://host1:2380 &
-$ etcd \
-  --name m2 \
-  --listen-client-urls http://host2:2379 \
-  --advertise-client-urls http://host2:2379 \
-  --listen-peer-urls http://host2:2380 &
-$ etcd \
-  --name m3 \
-  --listen-client-urls http://host3:2379 \
-  --advertise-client-urls http://host3:2379 \
-  --listen-peer-urls http://host3:2380 &
-```
-
-Now the restored etcd cluster should be available and serving the keyspace given by the snapshot.
diff --git a/Documentation/runtime-configuration.md b/Documentation/runtime-configuration.md
deleted file mode 100644
index ca5ce29..0000000
--- a/Documentation/runtime-configuration.md
+++ /dev/null
@@ -1,184 +0,0 @@
-# Runtime Reconfiguration
-
-etcd comes with support for incremental runtime reconfiguration, which allows users to update the membership of the cluster at run time.
-
-Reconfiguration requests can only be processed when the majority of the cluster members are functioning. It is **highly recommended** to always have a cluster size greater than two in production. It is unsafe to remove a member from a two member cluster. The majority of a two member cluster is also two. If there is a failure during the removal process, the cluster might not able to make progress and need to [restart from majority failure][majority failure].
-
-To better understand the design behind runtime reconfiguration, we suggest you read [the runtime reconfiguration document][runtime-reconf].
-
-## Reconfiguration Use Cases
-
-Let's walk through some common reasons for reconfiguring a cluster. Most of these just involve combinations of adding or removing a member, which are explained below under [Cluster Reconfiguration Operations][cluster-reconf].
-
-### Cycle or Upgrade Multiple Machines
-
-If you need to move multiple members of your cluster due to planned maintenance (hardware upgrades, network downtime, etc.), it is recommended to modify members one at a time.
-
-It is safe to remove the leader, however there is a brief period of downtime while the election process takes place. If your cluster holds more than 50MB, it is recommended to [migrate the member's data directory][member migration].
-
-### Change the Cluster Size
-
-Increasing the cluster size can enhance [failure tolerance][fault tolerance table] and provide better read performance. Since clients can read from any member, increasing the number of members increases the overall read throughput.
-
-Decreasing the cluster size can improve the write performance of a cluster, with a trade-off of decreased resilience. Writes into the cluster are replicated to a majority of members of the cluster before considered committed. Decreasing the cluster size lowers the majority, and each write is committed more quickly.
-
-### Replace A Failed Machine
-
-If a machine fails due to hardware failure, data directory corruption, or some other fatal situation, it should be replaced as soon as possible. Machines that have failed but haven't been removed adversely affect your quorum and reduce the tolerance for an additional failure.
-
-To replace the machine, follow the instructions for [removing the member][remove member] from the cluster, and then [add a new member][add member] in its place. If your cluster holds more than 50MB, it is recommended to [migrate the failed member's data directory][member migration] if you can still access it.
-
-### Restart Cluster from Majority Failure
-
-If the majority of your cluster is lost or all of your nodes have changed IP addresses, then you need to take manual action in order to recover safely.
-The basic steps in the recovery process include [creating a new cluster using the old data][disaster recovery], forcing a single member to act as the leader, and finally using runtime configuration to [add new members][add member] to this new cluster one at a time.
-
-## Cluster Reconfiguration Operations
-
-Now that we have the use cases in mind, let us lay out the operations involved in each.
-
-Before making any change, the simple majority (quorum) of etcd members must be available.
-This is essentially the same requirement as for any other write to etcd.
-
-All changes to the cluster are done one at a time:
-
-* To update a single member peerURLs you will make an update operation
-* To replace a single member you will make an add then a remove operation
-* To increase from 3 to 5 members you will make two add operations
-* To decrease from 5 to 3 you will make two remove operations
-
-All of these examples will use the `etcdctl` command line tool that ships with etcd.
-If you want to use the members API directly you can find the documentation [here][member-api].
-
-### Update a Member
-
-#### Update advertise client URLs
-
-If you would like to update the advertise client URLs of a member, you can simply restart
-that member with updated client urls flag (`--advertise-client-urls`) or environment variable
-(`ETCD_ADVERTISE_CLIENT_URLS`). The restarted member will self publish the updated URLs.
-A wrongly updated client URL will not affect the health of the etcd cluster.
-
-#### Update advertise peer URLs
-
-If you would like to update the advertise peer URLs of a member, you have to first update 
-it explicitly via member command and then restart the member. The additional action is required
-since updating peer URLs changes the cluster wide configuration and can affect the health of the etcd cluster. 
-
-To update the peer URLs, first, we need to find the target member's ID. You can list all members with `etcdctl`:
-
-```sh
-$ etcdctl member list
-6e3bd23ae5f1eae0: name=node2 peerURLs=http://localhost:23802 clientURLs=http://127.0.0.1:23792
-924e2e83e93f2560: name=node3 peerURLs=http://localhost:23803 clientURLs=http://127.0.0.1:23793
-a8266ecf031671f3: name=node1 peerURLs=http://localhost:23801 clientURLs=http://127.0.0.1:23791
-```
-
-In this example let's `update` a8266ecf031671f3 member ID and change its peerURLs value to http://10.0.1.10:2380
-
-```sh
-$ etcdctl member update a8266ecf031671f3 http://10.0.1.10:2380
-Updated member with ID a8266ecf031671f3 in cluster
-```
-
-### Remove a Member
-
-Let us say the member ID we want to remove is a8266ecf031671f3.
-We then use the `remove` command to perform the removal:
-
-```sh
-$ etcdctl member remove a8266ecf031671f3
-Removed member a8266ecf031671f3 from cluster
-```
-
-The target member will stop itself at this point and print out the removal in the log:
-
-```
-etcd: this member has been permanently removed from the cluster. Exiting.
-```
-
-It is safe to remove the leader, however the cluster will be inactive while a new leader is elected. This duration is normally the period of election timeout plus the voting process.
-
-### Add a New Member
-
-Adding a member is a two step process:
-
- * Add the new member to the cluster via the [members API][member-api] or the `etcdctl member add` command.
- * Start the new member with the new cluster configuration, including a list of the updated members (existing members + the new member).
-
-Using `etcdctl` let's add the new member to the cluster by specifying its [name][conf-name] and [advertised peer URLs][conf-adv-peer]:
-
-```sh
-$ etcdctl member add infra3 http://10.0.1.13:2380
-added member 9bf1b35fc7761a23 to cluster
-
-ETCD_NAME="infra3"
-ETCD_INITIAL_CLUSTER="infra0=http://10.0.1.10:2380,infra1=http://10.0.1.11:2380,infra2=http://10.0.1.12:2380,infra3=http://10.0.1.13:2380"
-ETCD_INITIAL_CLUSTER_STATE=existing
-```
-
-`etcdctl` has informed the cluster about the new member and printed out the environment variables needed to successfully start it.
-Now start the new etcd process with the relevant flags for the new member:
-
-```sh
-$ export ETCD_NAME="infra3"
-$ export ETCD_INITIAL_CLUSTER="infra0=http://10.0.1.10:2380,infra1=http://10.0.1.11:2380,infra2=http://10.0.1.12:2380,infra3=http://10.0.1.13:2380"
-$ export ETCD_INITIAL_CLUSTER_STATE=existing
-$ etcd --listen-client-urls http://10.0.1.13:2379 --advertise-client-urls http://10.0.1.13:2379 --listen-peer-urls http://10.0.1.13:2380 --initial-advertise-peer-urls http://10.0.1.13:2380 --data-dir %data_dir%
-```
-
-The new member will run as a part of the cluster and immediately begin catching up with the rest of the cluster.
-
-If you are adding multiple members the best practice is to configure a single member at a time and verify it starts correctly before adding more new members.
-If you add a new member to a 1-node cluster, the cluster cannot make progress before the new member starts because it needs two members as majority to agree on the consensus. You will only see this behavior between the time `etcdctl member add` informs the cluster about the new member and the new member successfully establishing a connection to the existing one.
-
-#### Error Cases When Adding Members
-
-In the following case we have not included our new host in the list of enumerated nodes.
-If this is a new cluster, the node must be added to the list of initial cluster members.
-
-```sh
-$ etcd --name infra3 \
-  --initial-cluster infra0=http://10.0.1.10:2380,infra1=http://10.0.1.11:2380,infra2=http://10.0.1.12:2380 \
-  --initial-cluster-state existing
-etcdserver: assign ids error: the member count is unequal
-exit 1
-```
-
-In this case we give a different address (10.0.1.14:2380) to the one that we used to join the cluster (10.0.1.13:2380).
-
-```sh
-$ etcd --name infra4 \
-  --initial-cluster infra0=http://10.0.1.10:2380,infra1=http://10.0.1.11:2380,infra2=http://10.0.1.12:2380,infra4=http://10.0.1.14:2380 \
-  --initial-cluster-state existing
-etcdserver: assign ids error: unmatched member while checking PeerURLs
-exit 1
-```
-
-When we start etcd using the data directory of a removed member, etcd will exit automatically if it connects to any active member in the cluster:
-
-```sh
-$ etcd
-etcd: this member has been permanently removed from the cluster. Exiting.
-exit 1
-```
-
-### Strict Reconfiguration Check Mode (`-strict-reconfig-check`)
-
-As described in the above, the best practice of adding new members is to configure a single member at a time and verify it starts correctly before adding more new members. This step by step approach is very important because if newly added members is not configured correctly (for example the peer URLs are incorrect), the cluster can lose quorum. The quorum loss happens since the newly added member are counted in the quorum even if that member is not reachable from other existing members. Also quorum loss might happen if there is a connectivity issue or there are operational issues.
-
-For avoiding this problem, etcd provides an option `-strict-reconfig-check`. If this option is passed to etcd, etcd rejects reconfiguration requests if the number of started members will be less than a quorum of the reconfigured cluster.
-
-It is recommended to enable this option. However, it is disabled by default because of keeping compatibility.
-
-[add member]: #add-a-new-member
-[cluster-reconf]: #cluster-reconfiguration-operations
-[conf-adv-peer]: configuration.md#-initial-advertise-peer-urls
-[conf-name]: configuration.md#-name
-[disaster recovery]: admin_guide.md#disaster-recovery
-[fault tolerance table]: admin_guide.md#fault-tolerance-table
-[majority failure]: #restart-cluster-from-majority-failure
-[member-api]: members_api.md
-[member migration]: admin_guide.md#member-migration
-[remove member]: #remove-a-member
-[runtime-reconf]: runtime-reconf-design.md
diff --git a/Documentation/runtime-reconf-design.md b/Documentation/runtime-reconf-design.md
deleted file mode 100644
index d577278..0000000
--- a/Documentation/runtime-reconf-design.md
+++ /dev/null
@@ -1,50 +0,0 @@
-# Design of Runtime Reconfiguration
-
-Runtime reconfiguration is one of the hardest and most error prone features in a distributed system, especially in a consensus based system like etcd.
-
-Read on to learn about the design of etcd's runtime reconfiguration commands and how we tackled these problems.
-
-## Two Phase Config Changes Keep you Safe
-
-In etcd, every runtime reconfiguration has to go through [two phases][add-member] for safety reasons. For example, to add a member you need to first inform cluster of new configuration and then start the new member.
-
-Phase 1 - Inform cluster of new configuration
-
-To add a member into etcd cluster, you need to make an API call to request a new member to be added to the cluster. And this is only way that you can add a new member into an existing cluster. The API call returns when the cluster agrees on the configuration change.
-
-Phase 2 - Start new member
-
-To join the etcd member into the existing cluster, you need to specify the correct `initial-cluster` and set `initial-cluster-state` to `existing`. When the member starts, it will contact the existing cluster first and verify the current cluster configuration matches the expected one specified in `initial-cluster`. When the new member successfully starts, you know your cluster reached the expected configuration.
-
-By splitting the process into two discrete phases users are forced to be explicit regarding cluster membership changes. This actually gives users more flexibility and makes things easier to reason about. For example, if there is an attempt to add a new member with the same ID as an existing member in an etcd cluster, the action will fail immediately during phase one without impacting the running cluster. Similar protection is provided to prevent adding new members by mistake. If a new etcd member attempts to join the cluster before the cluster has accepted the configuration change,, it will not be accepted by the cluster.
-
-Without the explicit workflow around cluster membership etcd would be vulnerable to unexpected cluster membership changes. For example, if etcd is running under an init system such as systemd, etcd would be restarted after being removed via the membership API, and attempt to rejoin the cluster on startup. This cycle would continue every time a member is removed via the API and systemd is set to restart etcd after failing, which is unexpected.
-
-We think runtime reconfiguration should be a low frequent operation. We made the decision to keep it explicit and user-driven to ensure configuration safety and keep your cluster always running smoothly under your control.
-
-## Permanent Loss of Quorum Requires New Cluster
-
-If a cluster permanently loses a majority of its members, a new cluster will need to be started from an old data directory to recover the previous state.
-
-It is entirely possible to force removing the failed members from the existing cluster to recover. However, we decided not to support this method since it bypasses the normal consensus committing phase, which is unsafe. If the member to remove is not actually dead or you force to remove different members through different members in the same cluster, you will end up with diverged cluster with same clusterID. This is very dangerous and hard to debug/fix afterwards. 
-
-If you have a correct deployment, the possibility of permanent majority lose is very low. But it is a severe enough problem that worth special care. We strongly suggest you to read the [disaster recovery documentation][disaster-recovery] and prepare for permanent majority lose before you put etcd into production.
-
-## Do Not Use Public Discovery Service For Runtime Reconfiguration
-
-The public discovery service should only be used for bootstrapping a cluster. To join member into an existing cluster, you should use runtime reconfiguration API. 
-
-Discovery service is designed for bootstrapping an etcd cluster in the cloud environment, when you do not know the IP addresses of all the members beforehand. After you successfully bootstrap a cluster, the IP addresses of all the members are known. Technically, you should not need the discovery service any more.
-
-It seems that using public discovery service is a convenient way to do runtime reconfiguration, after all discovery service already has all the cluster configuration information. However relying on public discovery service brings troubles: 
-
-1. it introduces external dependencies for the entire life-cycle of your cluster, not just bootstrap time. If there is a network issue between your cluster and public discovery service, your cluster will suffer from it.
- 
-2. public discovery service must reflect correct runtime configuration of your cluster during it life-cycle. It has to provide security mechanism to avoid bad actions, and it is hard. 
-
-3. public discovery service has to keep tens of thousands of cluster configurations. Our public discovery service backend is not ready for that workload.
-
-If you want to have a discovery service that supports runtime reconfiguration, the best choice is to build your private one.
-
-[add-member]: runtime-configuration.md#add-a-new-member
-[disaster-recovery]: admin_guide.md#disaster-recovery
